@book{sutton2018,
  title     = {Reinforcement Learning: An Introduction},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  isbn      = {978-0-262-03924-6},
  series    = {Adaptive Computation and Machine Learning series},
  url       = {https://mitpress.mit.edu/9780262039246/reinforcement-learning/},
  year      = {2018},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  edition   = {second edition},
  month     = {November}
}

@misc{wang2020gamblersproblem,
  title         = {The Gambler's Problem and Beyond},
  author        = {Baoxiang Wang and Shuai Li and Jiajin Li and Siu On Chan},
  year          = {2020},
  eprint        = {2001.00102},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/2001.00102}
}

@misc{randomservices,
  author       = {Kyle Siegrist},
  title        = {Probability, Mathematical Statistics, Stochastic Processes},
  year         = {2023},
  howpublished = {\url{https://www.randomservices.org/random/index.html}},
  note         = {Accessed: 2025-06-16}
}

@book{oprea2000,
  title     = {The Mathematics of Soap Films: Explorations with Maple: Explorations with Maple},
  author    = {Oprea, John},
  isbn      = {9780821821183},
  lccn      = {00041614},
  series    = {Fields Institute Communications},
  year      = {2000},
  publisher = {American Mathematical Society}
}

@article{agapiou2017Importance,
  author    = {Sergios Agapiou and Omiros Papaspiliopoulos and Daniel Sanz-Alonso and Andrew M. Stuart},
  title     = {Importance Sampling: Intrinsic Dimension and Computational Cost},
  journal   = {Statistical Science},
  volume    = {32},
  number    = {3},
  pages     = {405--431},
  year      = {2017},
  publisher = {Institute of Mathematical Statistics},
  doi       = {10.1214/17-STS611},
  url       = {https://doi.org/10.1214/17-STS611},
  arxiv     = {1511.06196}
}

@article{eyal2006ActionElimination,
  author     = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
  title      = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
  year       = {2006},
  issue_date = {12/1/2006},
  publisher  = {JMLR.org},
  volume     = {7},
  issn       = {1532-4435},
  abstract   = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/ε2)log(1/δ)) times to find an ε-optimal arm with probability of at least 1-δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning.},
  journal    = {J. Mach. Learn. Res.},
  month      = dec,
  pages      = {1079–1105},
  numpages   = {27}
}
