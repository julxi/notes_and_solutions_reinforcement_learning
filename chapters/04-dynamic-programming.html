<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-06-17">

<title>4&nbsp; Dynamic Programming ‚Äì Notes on Sutton &amp; Barto</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/05-monte-carlo-methods.html" rel="next">
<link href="../chapters/03-finite-markov-decision-processes.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-6fc64a0c1cda8d1c841de64652c337fd.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-6fc64a0c1cda8d1c841de64652c337fd.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fb02f3c877dd64dca3cf6a6e57462b95.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-72e3104a5210017f14ac4024f5014c88.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/04-dynamic-programming.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dynamic Programming</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Notes on Sutton &amp; Barto</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/julxi/notes_and_solutions_reinforcement_learning" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-multi-armed-bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-finite-markov-decision-processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-dynamic-programming.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dynamic Programming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-monte-carlo-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-temporal-difference-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Temporal-Difference Learning (Still in Progress üî®)</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#policy-evaluation" id="toc-policy-evaluation" class="nav-link active" data-scroll-target="#policy-evaluation"><span class="header-section-number">4.1</span> Policy Evaluation</a></li>
  <li><a href="#sec-policy-improvement" id="toc-sec-policy-improvement" class="nav-link" data-scroll-target="#sec-policy-improvement"><span class="header-section-number">4.2</span> Policy Improvement</a></li>
  <li><a href="#sec-policy-iteration" id="toc-sec-policy-iteration" class="nav-link" data-scroll-target="#sec-policy-iteration"><span class="header-section-number">4.3</span> Policy Iteration</a></li>
  <li><a href="#value-iteration" id="toc-value-iteration" class="nav-link" data-scroll-target="#value-iteration"><span class="header-section-number">4.4</span> Value Iteration</a>
  <ul class="collapse">
  <li><a href="#gamblers-problem" id="toc-gamblers-problem" class="nav-link" data-scroll-target="#gamblers-problem"><span class="header-section-number">4.4.1</span> gambler‚Äôs problem</a></li>
  </ul></li>
  <li><a href="#asynchronous-dynamic-programming" id="toc-asynchronous-dynamic-programming" class="nav-link" data-scroll-target="#asynchronous-dynamic-programming"><span class="header-section-number">4.5</span> Asynchronous Dynamic Programming</a></li>
  <li><a href="#generalized-policy-iteration" id="toc-generalized-policy-iteration" class="nav-link" data-scroll-target="#generalized-policy-iteration"><span class="header-section-number">4.6</span> Generalized Policy Iteration</a></li>
  <li><a href="#efficiency-of-dynamic-programming" id="toc-efficiency-of-dynamic-programming" class="nav-link" data-scroll-target="#efficiency-of-dynamic-programming"><span class="header-section-number">4.7</span> Efficiency of Dynamic Programming</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">4.8</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dynamic Programming</span></h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 17, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="policy-evaluation" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="policy-evaluation"><span class="header-section-number">4.1</span> Policy Evaluation</h2>
<p>The Bellman equations for state-value <a href="03-finite-markov-decision-processes.html#eq-bellman-state-value" class="quarto-xref">Equation&nbsp;<span>3.12</span></a> and for action-value <a href="03-finite-markov-decision-processes.html#eq-bellman-action-value" class="quarto-xref">Equation&nbsp;<span>3.13</span></a> can be used as update rules to approximate <span class="math inline">\(v_\pi\)</span> and <span class="math inline">\(q_\pi\)</span>: <span id="eq-bellman-update-state-value"><span class="math display">\[
v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_{k}(s')]
\tag{4.1}\]</span></span> <span id="eq-bellman-update-action-value"><span class="math display">\[
q_{k+1}(s,a) = \sum_{s',r}p(s',r|s,a) [r + \gamma \sum_{a'}\pi(a'|s')q_k(s',a')]
\tag{4.2}\]</span></span></p>
<p>These equations form the basis for iterative policy evaluation. The algorithm below demonstrates how to approximate <span class="math inline">\(v_\pi\)</span>, where updates are performed in ‚Äúsweeps‚Äù rather than ‚Äúchunk updates‚Äù. This constitutes the policy evaluations step,<span class="math inline">\(\pi \overset{\mathrm{Eval}}{\to} v_{\pi}\)</span>, in the policy iteration algorithm (<a href="#sec-policy-iteration" class="quarto-xref"><span>Section 4.3</span></a>).</p>
<div id="lst-policy-evaluation" class="listing-block listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-policy-evaluation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4.1: Iterative Policy Evaluation, for estimating <span class="math inline">\(V \approx v_\pi\)</span>
</figcaption>
<div aria-describedby="lst-policy-evaluation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="line-block"><strong>Input</strong>: <span class="math inline">\(\pi\)</span>, the policy to be evaluated<br>
<strong>Parameters</strong>: <span class="math inline">\(\theta &gt; 0\)</span>, determining accuracy of estimation<br>
<strong>Initialisation</strong>: <span class="math inline">\(V(s)\)</span>, for all <span class="math inline">\(s \in \mathcal{S}\)</span> arbitrarily, and <span class="math inline">\(V(\mathrm{terminal}) = 0\)</span><br>
Loop:<br>
&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets 0\)</span><br>
&nbsp;&nbsp;&nbsp;Loop for each <span class="math inline">\(s \in \mathcal{S}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(v \gets V(s)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(V(s) \gets \sum_a \pi(a|s) \sum_{s',r}p(s',r|s,a)[r + \gamma V(s')]\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets \max(\Delta, |v - V(s))\)</span><br>
until <span class="math inline">\(\Delta &lt; \theta\)</span></div>
</div>
</figure>
</div>
<div id="exm-4.1" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1</strong></span> Here we explore Example 4.1 from <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span></p>
<p>Here is the quick summary:</p>
<ul>
<li>states: non-terminal states are numbered 1 through 14. The two gray cells are treated as a single terminal state.</li>
<li>actions: Four deterministic actions available in each state: up, down, left, right. Moving ‚Äúoff the grid‚Äù results in no state change.</li>
<li>rewards: A reward of -1 is given for every transition until the terminal state is reached</li>
<li>return: undiscounted</li>
</ul>
<div class="cell" data-fig-width="4" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="384" height="480" viewbox="0.00 0.00 128.00 128.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 124)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-124 124,-124 124,4 -4,4"></polygon>
<!-- grid -->
<g id="node1" class="node">
<title>grid</title>
<polygon fill="lightgray" stroke="transparent" points="0,-90 0,-120 30,-120 30,-90 0,-90"></polygon>
<polygon fill="none" stroke="black" points="0,-90 0,-120 30,-120 30,-90 0,-90"></polygon>
<polygon fill="none" stroke="black" points="30,-90 30,-120 60,-120 60,-90 30,-90"></polygon>
<text text-anchor="start" x="41.5" y="-100.8" font-family="Times,serif" font-size="14.00">1</text>
<polygon fill="none" stroke="black" points="60,-90 60,-120 90,-120 90,-90 60,-90"></polygon>
<text text-anchor="start" x="71.5" y="-100.8" font-family="Times,serif" font-size="14.00">2</text>
<polygon fill="none" stroke="black" points="90,-90 90,-120 120,-120 120,-90 90,-90"></polygon>
<text text-anchor="start" x="101.5" y="-100.8" font-family="Times,serif" font-size="14.00">3</text>
<polygon fill="none" stroke="black" points="0,-60 0,-90 30,-90 30,-60 0,-60"></polygon>
<text text-anchor="start" x="11.5" y="-70.8" font-family="Times,serif" font-size="14.00">4</text>
<polygon fill="none" stroke="black" points="30,-60 30,-90 60,-90 60,-60 30,-60"></polygon>
<text text-anchor="start" x="41.5" y="-70.8" font-family="Times,serif" font-size="14.00">5</text>
<polygon fill="none" stroke="black" points="60,-60 60,-90 90,-90 90,-60 60,-60"></polygon>
<text text-anchor="start" x="71.5" y="-70.8" font-family="Times,serif" font-size="14.00">6</text>
<polygon fill="none" stroke="black" points="90,-60 90,-90 120,-90 120,-60 90,-60"></polygon>
<text text-anchor="start" x="101.5" y="-70.8" font-family="Times,serif" font-size="14.00">7</text>
<polygon fill="none" stroke="black" points="0,-30 0,-60 30,-60 30,-30 0,-30"></polygon>
<text text-anchor="start" x="11.5" y="-40.8" font-family="Times,serif" font-size="14.00">8</text>
<polygon fill="none" stroke="black" points="30,-30 30,-60 60,-60 60,-30 30,-30"></polygon>
<text text-anchor="start" x="41.5" y="-40.8" font-family="Times,serif" font-size="14.00">9</text>
<polygon fill="none" stroke="black" points="60,-30 60,-60 90,-60 90,-30 60,-30"></polygon>
<text text-anchor="start" x="68" y="-40.8" font-family="Times,serif" font-size="14.00">10</text>
<polygon fill="none" stroke="black" points="90,-30 90,-60 120,-60 120,-30 90,-30"></polygon>
<text text-anchor="start" x="98" y="-40.8" font-family="Times,serif" font-size="14.00">11</text>
<polygon fill="none" stroke="black" points="0,0 0,-30 30,-30 30,0 0,0"></polygon>
<text text-anchor="start" x="8" y="-10.8" font-family="Times,serif" font-size="14.00">12</text>
<polygon fill="none" stroke="black" points="30,0 30,-30 60,-30 60,0 30,0"></polygon>
<text text-anchor="start" x="38" y="-10.8" font-family="Times,serif" font-size="14.00">13</text>
<polygon fill="none" stroke="black" points="60,0 60,-30 90,-30 90,0 60,0"></polygon>
<text text-anchor="start" x="68" y="-10.8" font-family="Times,serif" font-size="14.00">14</text>
<polygon fill="lightgray" stroke="transparent" points="90,0 90,-30 120,-30 120,0 90,0"></polygon>
<polygon fill="none" stroke="black" points="90,0 90,-30 120,-30 120,0 90,0"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>And here are the state-values for the random policy:</p>
<div class="cell" data-fig-width="4" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="384" height="480" viewbox="0.00 0.00 128.00 128.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 124)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-124 124,-124 124,4 -4,4"></polygon>
<!-- grid -->
<g id="node1" class="node">
<title>grid</title>
<polygon fill="lightgray" stroke="transparent" points="0,-90 0,-120 30,-120 30,-90 0,-90"></polygon>
<polygon fill="none" stroke="black" points="0,-90 0,-120 30,-120 30,-90 0,-90"></polygon>
<text text-anchor="start" x="11.5" y="-100.8" font-family="Times,serif" font-size="14.00">0</text>
<polygon fill="none" stroke="black" points="30,-90 30,-120 60,-120 60,-90 30,-90"></polygon>
<text text-anchor="start" x="35.67" y="-100.8" font-family="Times,serif" font-size="14.00">-14</text>
<polygon fill="none" stroke="black" points="60,-90 60,-120 90,-120 90,-90 60,-90"></polygon>
<text text-anchor="start" x="65.67" y="-100.8" font-family="Times,serif" font-size="14.00">-20</text>
<polygon fill="none" stroke="black" points="90,-90 90,-120 120,-120 120,-90 90,-90"></polygon>
<text text-anchor="start" x="95.67" y="-100.8" font-family="Times,serif" font-size="14.00">-22</text>
<polygon fill="none" stroke="black" points="0,-60 0,-90 30,-90 30,-60 0,-60"></polygon>
<text text-anchor="start" x="5.67" y="-70.8" font-family="Times,serif" font-size="14.00">-14</text>
<polygon fill="none" stroke="black" points="30,-60 30,-90 60,-90 60,-60 30,-60"></polygon>
<text text-anchor="start" x="35.67" y="-70.8" font-family="Times,serif" font-size="14.00">-18</text>
<polygon fill="none" stroke="black" points="60,-60 60,-90 90,-90 90,-60 60,-60"></polygon>
<text text-anchor="start" x="65.67" y="-70.8" font-family="Times,serif" font-size="14.00">-20</text>
<polygon fill="none" stroke="black" points="90,-60 90,-90 120,-90 120,-60 90,-60"></polygon>
<text text-anchor="start" x="95.67" y="-70.8" font-family="Times,serif" font-size="14.00">-20</text>
<polygon fill="none" stroke="black" points="0,-30 0,-60 30,-60 30,-30 0,-30"></polygon>
<text text-anchor="start" x="5.67" y="-40.8" font-family="Times,serif" font-size="14.00">-20</text>
<polygon fill="none" stroke="black" points="30,-30 30,-60 60,-60 60,-30 30,-30"></polygon>
<text text-anchor="start" x="35.67" y="-40.8" font-family="Times,serif" font-size="14.00">-20</text>
<polygon fill="none" stroke="black" points="60,-30 60,-60 90,-60 90,-30 60,-30"></polygon>
<text text-anchor="start" x="65.67" y="-40.8" font-family="Times,serif" font-size="14.00">-18</text>
<polygon fill="none" stroke="black" points="90,-30 90,-60 120,-60 120,-30 90,-30"></polygon>
<text text-anchor="start" x="95.67" y="-40.8" font-family="Times,serif" font-size="14.00">-14</text>
<polygon fill="none" stroke="black" points="0,0 0,-30 30,-30 30,0 0,0"></polygon>
<text text-anchor="start" x="5.67" y="-10.8" font-family="Times,serif" font-size="14.00">-22</text>
<polygon fill="none" stroke="black" points="30,0 30,-30 60,-30 60,0 30,0"></polygon>
<text text-anchor="start" x="35.67" y="-10.8" font-family="Times,serif" font-size="14.00">-20</text>
<polygon fill="none" stroke="black" points="60,0 60,-30 90,-30 90,0 60,0"></polygon>
<text text-anchor="start" x="65.67" y="-10.8" font-family="Times,serif" font-size="14.00">-14</text>
<polygon fill="lightgray" stroke="transparent" points="90,0 90,-30 120,-30 120,0 90,0"></polygon>
<polygon fill="none" stroke="black" points="90,0 90,-30 120,-30 120,0 90,0"></polygon>
<text text-anchor="start" x="101.5" y="-10.8" font-family="Times,serif" font-size="14.00">0</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Note that these values are (luckily) exact, which will be useful for the next exercises</p>
</div>
<div id="exr-4.1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.1</strong></span> In <a href="#exm-4.1" class="quarto-xref">Example&nbsp;<span>4.1</span></a>, if <span class="math inline">\(\pi\)</span> is the equiprobable random policy, what is <span class="math inline">\(q_\pi(11, \mathrm{down})\)</span>? What is <span class="math inline">\(q_\pi(7, \mathrm{down})\)</span>?</p>
</div>
<div id="sol-4.1" class="proof solution">
<p><span class="proof-title"><em>Solution 4.1</em>. </span>We can use the state-value function given in the example: <span class="math display">\[
\begin{split}
q_\pi(11, \mathrm{down}) &amp;= -1 + 0 = -1\\
q_\pi(7, \mathrm{down}) &amp;= -1 + v_\pi(11) = -1 + (-14) = -15
\end{split}
\]</span></p>
</div>
<div id="exr-4.2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.2</strong></span> In <a href="#exm-4.1" class="quarto-xref">Example&nbsp;<span>4.1</span></a>, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is <span class="math inline">\(v_\pi(15)\)</span> for the equiprobable random policy?</p>
<p>Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is <span class="math inline">\(v_\pi(15)\)</span> for the equiprobable random policy in this case?</p>
</div>
<div id="sol-4.2" class="proof solution">
<p><span class="proof-title"><em>Solution 4.2</em>. </span>Since the MDP is deterministic and all transitions give the same reward, the undiscounted Bellman equation <a href="03-finite-markov-decision-processes.html#eq-bellman-state-value" class="quarto-xref">Equation&nbsp;<span>3.12</span></a> simplifies to: <span class="math display">\[
v_{\pi}(s) = r + \sum_{a}\pi(a|s') [v_{\pi}(s')],
\]</span></p>
<p>where <span class="math inline">\(r = -1\)</span>.</p>
<p>The first case is quite easy to compute. The transitions for all original states remain unchanged, so their values also remain unchanged. For the new state 15, we can write: <span class="math display">\[
v_\pi(15) = -1 + \frac{1}{4}(v_\pi(12) + v_\pi(13) + v_\pi(14) + v_\pi(15))
\]</span> which gives <span class="math inline">\(v_\pi(15) = -20\)</span>.</p>
<p>Now in the second case we might be up for a lot of work, as state 13 has a new transition: taking action ‚Äúdown‚Äù leads to state 15. This changes the dynamics of the MDP, so in principle the values might change. However, luckily the existing state-value function still satisfies the Bellman equation for state 13: <span class="math display">\[
v_\pi(13) = -1 + \frac{1}{4}(v_\pi(12) + v_\pi(9) + v_\pi(13) + v_\pi(15))
\]</span></p>
<p>Substitute the known values we see that the equation holds <span class="math display">\[
v_\pi(13) = -20 = -1 + \frac{1}{4}(-22 - 20 - 14 - 20)
\]</span></p>
<p>So <span class="math inline">\(v_\pi(13)\)</span> remains consistent with the new dynamics. Since all Bellman equations continue to hold with the same values, the state-value function does not change. So, <span class="math inline">\(v_\pi(15)=-20\)</span> also in this case.</p>
</div>
<div id="exr-4.3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.3</strong></span> What are the equations analogous to <a href="03-finite-markov-decision-processes.html#eq-state-value-one-step-look-ahead" class="quarto-xref">Equation&nbsp;<span>3.10</span></a>, <a href="03-finite-markov-decision-processes.html#eq-bellman-state-value" class="quarto-xref">Equation&nbsp;<span>3.12</span></a>, and <a href="#eq-bellman-update-state-value" class="quarto-xref">Equation&nbsp;<span>4.1</span></a> for the action-value function <span class="math inline">\(q_\pi\)</span> and its successive approximation by a sequence of functions <span class="math inline">\(q_0, q_1, \dots\)</span>?</p>
</div>
<div id="sol-4-3" class="proof solution">
<p><span class="proof-title"><em>Solution 4.3</em>. </span>We have already stated these equations in tandem as <a href="03-finite-markov-decision-processes.html#eq-action-value-one-step-look-ahead" class="quarto-xref">Equation&nbsp;<span>3.11</span></a>, <a href="03-finite-markov-decision-processes.html#eq-bellman-action-value" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>, and <a href="#eq-bellman-update-action-value" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>.</p>
</div>
</section>
<section id="sec-policy-improvement" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-policy-improvement"><span class="header-section-number">4.2</span> Policy Improvement</h2>
<p>An optimal policy can always be chosen to be deterministic. This is quite intuitive: why would introducing randomness in action selection be beneficial if all you care about is maximising expected return? More rigorously, if you are choosing between two actions, <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span>, and you know their values <span class="math inline">\(q_\pi(s,a_1)\)</span> and <span class="math inline">\(q_\pi(s,a_2)\)</span>, then it is clearly best to take the one with the higher value. A key tool for this kind of reasoning is the policy improvement theorem.</p>
<div id="thm-policy-improvement-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1</strong></span> Let <span class="math inline">\(\pi\)</span> be any policy and <span class="math inline">\(\pi'\)</span> a deterministic policy. Then <span class="math inline">\(\pi \leq \pi'\)</span> if <span class="math display">\[
v_\pi(s) \leq q_\pi(s,\pi'(s)),
\]</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the assumption, we have: <span class="math display">\[
\begin{split}
v_\pi(s) &amp;\leq q_\pi(s, \pi'(s)) \\
&amp;= \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = \pi'(s)] \\
&amp;= \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \\
\end{split}
\]</span> (if you wonder about the indices in the expectation: the first expectation is completely determined by the MDP, in the second one we stipulate action selection according to <span class="math inline">\(\pi'\)</span>.)</p>
<p>Now, we can unroll this expression recursively: <span class="math display">\[
\begin{align}
v_\pi(s) &amp;\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \\
&amp;\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R'_{t+2} + \gamma v_\pi(S'_{t+2}) \mid S'_{t+1} = S_{t+1}] \mid S_t = s] \\
&amp;= \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(S_{t+2}) \mid S_t = s]
\end{align}
\]</span></p>
<p>and so on. The last equality should be justified formally by the law of total expectation and the law of the unconscious statistician (<a href="02-multi-armed-bandits.html#thm-law-of-total-expectation" class="quarto-xref">Theorem&nbsp;<span>2.4</span></a> and <a href="02-multi-armed-bandits.html#thm-law-of-the-unconscious-statistician" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a>).</p>
<p>Iterating this process a couple of times we get <span class="math display">\[
v_\pi(s) \leq \mathbb{E}_{\pi'}\bigg[\sum_{i=0}^{N} \gamma^i R_{t+1+i} + \gamma^{N+1} v_\pi(S_{t+1+N}) \;\bigg|\; S_t = s \bigg]
\]</span></p>
<p>and in the limit (everything is bounded so this should be kosher) <span class="math display">\[
v_\pi(s) \leq \mathbb{E}_{\pi'}\bigg[\sum_{i=0}^{\infty} \gamma^i R_{t+1+i} \;\bigg| \; S_t = s \bigg] = v_{\pi'}(s).
\]</span></p>
</div>
<p>This result allows us to show that every finite MDP has an optimal deterministic policy.</p>
<p>Let <span class="math inline">\(\pi\)</span> be any policy. Define a new deterministic policy <span class="math inline">\(\pi'\)</span> by <span class="math display">\[
\pi'(s)= \underset{a \in \mathcal{A}}{\mathrm{argmax}} q_{\pi}(s,a)
\]</span></p>
<p>By the policy improvement theorem, we have <span class="math inline">\(\pi \leq \pi'\)</span>. Now consider two deterministic policies, <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span>, and define their meet (pointwise maximum policy) as <span class="math display">\[
(\pi_1 \vee \pi_2)(s) =
\begin{cases}\pi_1(s) &amp;\text{if } v_{\pi_1}(s) \geq v_{\pi_2}(s) \\
\pi_2(s) &amp;\text{else}
\end{cases}
\]</span></p>
<p>Then, again by the policy improvement theorem, we have <span class="math inline">\(\pi_1, \pi_2 \leq \pi_1 \vee \pi_2\)</span>.</p>
<p>Now, since the number of deterministic policies is finite (as both <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> are finite), we can take the meet over all deterministic policies and obtain an optimal deterministic policy.</p>
<p>This leads directly to a characterisation of optimality in terms of greedy action selection.</p>
<div id="thm-optimal-policy-equation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2</strong></span> A policy <span class="math inline">\(\pi\)</span> is optimal, if and only if, <span id="eq-optimal-policy"><span class="math display">\[
v_\pi(s) = \max_{a \in \mathcal{A}(s)} q_{\pi}(s,a),
\tag{4.3}\]</span></span></p>
<p>for all <span class="math inline">\(s \in \mathcal{S}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(\pi\)</span> is optimal then <span class="math inline">\(v_\pi(s) &lt; \max_{a} q_\pi(s,a)\)</span> would lead to a contradiction using the policy improvement theorem.</p>
<p>For the converse we do an argument very similar to the proof of <a href="#thm-policy-improvement-theorem" class="quarto-xref">Theorem&nbsp;<span>4.1</span></a>. So similar in fact that I‚Äôm afraid that were doing the same work twice. Let <span class="math inline">\(\pi\)</span> satisfy <a href="#eq-optimal-policy" class="quarto-xref">Equation&nbsp;<span>4.3</span></a>. We show that <span class="math inline">\(\pi\)</span> is optimal by showing that <span class="math display">\[
\Delta(s) = v_{\pi_*}(s) - v_{\pi}(s)
\]</span> is <span class="math inline">\(0\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>, where <span class="math inline">\(\pi_*\)</span> is any deterministic, optimal policy.</p>
<p>We can bound <span class="math inline">\(\Delta(s)\)</span> like so <span class="math display">\[
\begin{split}
\Delta(s) &amp;= q_{\pi_*}(s,\pi_*(s)) - \max_a q_{\pi}(s,a) \\
&amp;\leq q_{\pi_*}(s,\pi_*(s)) - q_\pi(s,\pi_*(s)) \\
&amp;= \mathbb{E}_{\pi_*}[ R_{t+1} + \gamma v_{\pi_*}(S_{t+1}) - (R_{t+1} + \gamma v_{\pi}(S_{t+1})) | S_{t} = s] \\
&amp;= \mathbb{E}_{\pi_*}[\gamma \Delta(S_{t+1}) | S_t = s]
\end{split}
\]</span></p>
<p>Iterating this and taking the limit gives <span class="math display">\[
\Delta(s) \leq \lim_{k \to \infty} \mathbb{E}_{\pi_*}[\gamma^k \Delta(S_{t+k}) \mid S_t = s] = 0.
\]</span></p>
</div>
<p>For a policy <span class="math inline">\(\pi\)</span>, if we define <span class="math inline">\(\pi'(s) := \underset{a}{\mathrm{argmax}}\;q_\pi(s,a)\)</span>, we get an improved policy, unless <span class="math inline">\(\pi\)</span> was already optimal. This constitutes the policy improvement step,<span class="math inline">\(v_\pi \overset{\mathrm{Imp}}{\to} \pi'\)</span>, in the policy iteration algorithm.</p>
</section>
<section id="sec-policy-iteration" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-policy-iteration"><span class="header-section-number">4.3</span> Policy Iteration</h2>
<p>The policy iteration algorithm chains evaluation and improvement and converges to an optimal policy, for any initial policy <span class="math inline">\(\pi_0\)</span>: <span class="math display">\[
\pi_0 \overset{\mathrm{Eval}}{\to} v_{\pi_0} \overset{\mathrm{Imp}}{\to}
\pi_1 \overset{\mathrm{Eval}}{\to} v_{\pi_1} \overset{\mathrm{Imp}}{\to}
\pi_2 \overset{\mathrm{Eval}}{\to} v_{\pi_2} \overset{\mathrm{Imp}}{\to}
\dots
\overset{\mathrm{Imp}}{\to} \pi_* \overset{\mathrm{Eval}}{\to} v_{*}
\]</span></p>
<p>And here is the pseudo code.</p>
<div id="lst-policy-iteration" class="listing-block listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-policy-iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4.2: Policy Iteration (using iterative policy evaluation) for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</figcaption>
<div aria-describedby="lst-policy-iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="line-block"><strong>Parameter</strong>:<br>
<span class="math inline">\(\theta\)</span>: a small positive number determining the accuracy of estimation<br>
<br>
<strong>1. Initialisation</strong>:<br>
<span class="math inline">\(V(s) \in \mathbb{R}\)</span>, <span class="math inline">\(\pi(s) \in \mathcal{A}(s)\)</span> arbitrarily, <span class="math inline">\(V(\mathrm{terminal}) = 0\)</span><br>
<br>
<strong>2. Policy Evaluation</strong><br>
Loop:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets 0\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;Loop for each <span class="math inline">\(s \in \mathcal{S}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(v \gets V(s)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(V(s) \gets  \sum_{s',r}p(s',r|s,\pi(s))[r + \gamma V(s')]\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets \max(\Delta, |v - V(s)|)\)</span><br>
until <span class="math inline">\(\Delta &lt; \theta\)</span><br>
<br>
<strong>3. Policy Improvement</strong><br>
<span class="math inline">\(\text{policy-stable} \gets \mathrm{true}\)</span><br>
For each <span class="math inline">\(s \in \mathcal{S}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{old-action} \gets \pi(s)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\pi(s) \gets \underset{a}{\mathrm{argmax}} \sum_{s',r} p(s', r |s,a)[r + \gamma V(s')]\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;If <span class="math inline">\(\text{old-action} \neq \pi(s)\)</span>, then <span class="math inline">\(\text{policy-stable} \gets \text{false}\)</span><br>
If <span class="math inline">\(\text{policy-stable}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;return <span class="math inline">\(V \approx v_*\)</span> and <span class="math inline">\(\pi \approx \pi_*\)</span><br>
else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;go to <strong>Policy Evaluation</strong></div>
</div>
</figure>
</div>
<p>Note that the final policy improvement step does not change the policy and is basically just checking that the current policy is optimal. So this is basically an extra step to see that this chain is done: <span class="math inline">\(\pi_0 \overset{\mathrm{Eval}}{\to} v_{\pi_0} \overset{\mathrm{Imp}}{\to}
\dots
\overset{\mathrm{Imp}}{\to} \pi_* \overset{\mathrm{Eval}}{\to} v_{*} \overset{\mathrm{Imp}}{\to} \text{Finished}\)</span></p>
<div id="exm-4.2" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2</strong></span> Here we explore Example 4.2 from <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span> - Jack‚Äôs car rental.</p>
<p>Here‚Äôs a quick summary:</p>
<ul>
<li>two locations, each with a maximum of 20 cars (more cars added to a location magically vanish into thin air)</li>
<li>during the day, a random amount of customers rent cars and then another random amount of customers return cars</li>
<li>at the end of a day, up to 5 cars can be moved between the locations</li>
<li>each car rented rewards 10</li>
<li>each move car costs 2</li>
<li>rentals and returns are Poisson distributed:<br>
<span class="math inline">\(\lambda_{1,\text{rent}} =3\)</span>, <span class="math inline">\(\lambda_{1,\text{return}} =3\)</span>, <span class="math inline">\(\lambda_{2,\text{rent}} =4\)</span>, <span class="math inline">\(\lambda_{2,\text{return}} =2\)</span></li>
</ul>
<p>We solve Jack‚Äôs car rental using policy iteration. The core computation in policy iteration is getting a state-action value from the state values, the one-step lookahead, which is used both in policy evaluation and policy improvement. This is the main performance bottleneck: <span class="math display">\[
Q(s,a) = \sum_{s',r} p(s',r|s,a) [ r + \gamma V(s')]
\]</span></p>
<p>This expression is good for theorizing about the algorithm but its direct implementation feels awkward and inefficient. In practice, it‚Äôs better to split the four-argument <span class="math inline">\(p(s',r|s,a)\)</span> into expected immediate reward <span class="math inline">\(r(s,a)\)</span> and the transition probability <span class="math inline">\(p(s'|s,a)\)</span>.</p>
<p><span class="math display">\[
\begin{split}
Q(s,a) &amp;= \sum_{s',r}p(s',r|s,a)r + \gamma\sum_{s',r}p(s',r|s,a)V(s') \\
&amp;= r(s,a) + \gamma \sum_{s'}p(s'|s, a) V(s')
\end{split}
\]</span></p>
<p>Furthermore, we can make the algorithm more natural for this problem by introducing afterstates <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018, sec. 6.8</a>)</span>. Although we don‚Äôt use them to their full potential (we don‚Äôt learn an afterstate value function).</p>
<p>An afterstate is the environment state immediately after the agent‚Äôs action, but before the environment‚Äôs stochastic dynamics. This formulation is particularly effective when actions have deterministic effects. In Jack‚Äôs car rental, moving cars deterministically leads to an afterstate <span class="math inline">\(s \oplus a\)</span>, while the stochastic dynamics - rentals and returns - then determine the next state <span class="math inline">\(s'\)</span>.</p>
<p>The one-step lookahead using afterstates becomes: <span id="eq-one-step-lookahead-with-afterstates"><span class="math display">\[
Q(s,a) = c(a) + r(s \oplus a) + \gamma \sum_{s'} p(s'|s\oplus a)V(s'),
\tag{4.4}\]</span></span></p>
<p>where <span class="math inline">\(c(a)\)</span> is the cost of the, <span class="math inline">\(r(s \oplus a)\)</span> is the expected immediate reward for the afterstate.</p>
<p>The following code is a nearly verbatim implementation of the policy iteration pseudocode, using <a href="#eq-one-step-lookahead-with-afterstates" class="quarto-xref">Equation&nbsp;<span>4.4</span></a> for evaluation. (Select annotations to see inline explanations.)</p>
<div id="2a6bb6af" class="cell" data-code-annotations="hover" data-execution_count="2">
<div class="sourceCode cell-code" id="annotated-cell-7"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-7-1"><a href="#annotated-cell-7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === policy iteration for Jack's car rental ===</span></span>
<span id="annotated-cell-7-2"><a href="#annotated-cell-7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="annotated-cell-7-3"><a href="#annotated-cell-7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="1">1</button><span id="annotated-cell-7-4" class="code-annotation-target"><a href="#annotated-cell-7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scripts.jacks_car_rental.jacks_car_rental <span class="im">import</span> (</span>
<span id="annotated-cell-7-5"><a href="#annotated-cell-7-5" aria-hidden="true" tabindex="-1"></a>    JacksCarRental,</span>
<span id="annotated-cell-7-6"><a href="#annotated-cell-7-6" aria-hidden="true" tabindex="-1"></a>    State,</span>
<span id="annotated-cell-7-7"><a href="#annotated-cell-7-7" aria-hidden="true" tabindex="-1"></a>    Action,</span>
<span id="annotated-cell-7-8"><a href="#annotated-cell-7-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="annotated-cell-7-9"><a href="#annotated-cell-7-9" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="2">2</button><span id="annotated-cell-7-10" class="code-annotation-target"><a href="#annotated-cell-7-10" aria-hidden="true" tabindex="-1"></a>Policy <span class="op">=</span> Dict[State, Action]</span>
<span id="annotated-cell-7-11"><a href="#annotated-cell-7-11" aria-hidden="true" tabindex="-1"></a>ValueFn <span class="op">=</span> Dict[State, <span class="bu">float</span>]</span>
<span id="annotated-cell-7-12"><a href="#annotated-cell-7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-13"><a href="#annotated-cell-7-13" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="3">3</button><span id="annotated-cell-7-14" class="code-annotation-target"><a href="#annotated-cell-7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_state_action_value(</span>
<span id="annotated-cell-7-15"><a href="#annotated-cell-7-15" aria-hidden="true" tabindex="-1"></a>    env: JacksCarRental, state: State, action: Action, value: ValueFn, Œ≥: <span class="bu">float</span></span>
<span id="annotated-cell-7-16"><a href="#annotated-cell-7-16" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="annotated-cell-7-17"><a href="#annotated-cell-7-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="annotated-cell-7-18"><a href="#annotated-cell-7-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the expected one‚Äêstep return</span></span>
<span id="annotated-cell-7-19"><a href="#annotated-cell-7-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="annotated-cell-7-20"><a href="#annotated-cell-7-20" aria-hidden="true" tabindex="-1"></a>    after_state, cost <span class="op">=</span> env.move(state, action)</span>
<span id="annotated-cell-7-21"><a href="#annotated-cell-7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-22"><a href="#annotated-cell-7-22" aria-hidden="true" tabindex="-1"></a>    future_return <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="annotated-cell-7-23"><a href="#annotated-cell-7-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> state_new <span class="kw">in</span> env.state_space:</span>
<span id="annotated-cell-7-24"><a href="#annotated-cell-7-24" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> env.get_transition_probability(after_state, state_new)</span>
<span id="annotated-cell-7-25"><a href="#annotated-cell-7-25" aria-hidden="true" tabindex="-1"></a>        future_return <span class="op">+=</span> p <span class="op">*</span> value[state_new]</span>
<span id="annotated-cell-7-26"><a href="#annotated-cell-7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-27"><a href="#annotated-cell-7-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost <span class="op">+</span> env.get_expected_revenue(after_state) <span class="op">+</span> Œ≥ <span class="op">*</span> future_return</span>
<span id="annotated-cell-7-28"><a href="#annotated-cell-7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-29"><a href="#annotated-cell-7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-30"><a href="#annotated-cell-7-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_evaluation(</span>
<span id="annotated-cell-7-31"><a href="#annotated-cell-7-31" aria-hidden="true" tabindex="-1"></a>    env: JacksCarRental, œÄ: Policy, value: ValueFn, Œ∏: <span class="bu">float</span>, Œ≥: <span class="bu">float</span></span>
<span id="annotated-cell-7-32"><a href="#annotated-cell-7-32" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="annotated-cell-7-33"><a href="#annotated-cell-7-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="annotated-cell-7-34"><a href="#annotated-cell-7-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Approximates the ValueFn from a deterministic policy</span></span>
<span id="annotated-cell-7-35"><a href="#annotated-cell-7-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="annotated-cell-7-36"><a href="#annotated-cell-7-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="annotated-cell-7-37"><a href="#annotated-cell-7-37" aria-hidden="true" tabindex="-1"></a>        Œî <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="annotated-cell-7-38"><a href="#annotated-cell-7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-39"><a href="#annotated-cell-7-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> env.state_space:</span>
<span id="annotated-cell-7-40"><a href="#annotated-cell-7-40" aria-hidden="true" tabindex="-1"></a>            v_old <span class="op">=</span> value[s]</span>
<span id="annotated-cell-7-41"><a href="#annotated-cell-7-41" aria-hidden="true" tabindex="-1"></a>            v_new <span class="op">=</span> compute_state_action_value(env, s, œÄ[s], value, Œ≥)</span>
<span id="annotated-cell-7-42"><a href="#annotated-cell-7-42" aria-hidden="true" tabindex="-1"></a>            value[s] <span class="op">=</span> v_new</span>
<span id="annotated-cell-7-43"><a href="#annotated-cell-7-43" aria-hidden="true" tabindex="-1"></a>            Œî <span class="op">=</span> <span class="bu">max</span>(Œî, <span class="bu">abs</span>(v_old <span class="op">-</span> v_new))</span>
<span id="annotated-cell-7-44"><a href="#annotated-cell-7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-45"><a href="#annotated-cell-7-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> Œî <span class="op">&lt;</span> Œ∏:</span>
<span id="annotated-cell-7-46"><a href="#annotated-cell-7-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="annotated-cell-7-47"><a href="#annotated-cell-7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-48"><a href="#annotated-cell-7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-49"><a href="#annotated-cell-7-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_improvement(</span>
<span id="annotated-cell-7-50"><a href="#annotated-cell-7-50" aria-hidden="true" tabindex="-1"></a>    env: JacksCarRental, œÄ: Policy, value: ValueFn, Œ≥: <span class="bu">float</span></span>
<span id="annotated-cell-7-51"><a href="#annotated-cell-7-51" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="annotated-cell-7-52"><a href="#annotated-cell-7-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="annotated-cell-7-53"><a href="#annotated-cell-7-53" aria-hidden="true" tabindex="-1"></a><span class="co">    Improve a policy according to the provided value‚Äêfunction</span></span>
<span id="annotated-cell-7-54"><a href="#annotated-cell-7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-55"><a href="#annotated-cell-7-55" aria-hidden="true" tabindex="-1"></a><span class="co">    If no state's action changes, return True (policy is stable). Otherwise return False.</span></span>
<span id="annotated-cell-7-56"><a href="#annotated-cell-7-56" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="annotated-cell-7-57"><a href="#annotated-cell-7-57" aria-hidden="true" tabindex="-1"></a>    stable <span class="op">=</span> <span class="va">True</span></span>
<span id="annotated-cell-7-58"><a href="#annotated-cell-7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-59"><a href="#annotated-cell-7-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> env.state_space:</span>
<span id="annotated-cell-7-60"><a href="#annotated-cell-7-60" aria-hidden="true" tabindex="-1"></a>        old_action <span class="op">=</span> œÄ[s]</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="4">4</button><span id="annotated-cell-7-61" class="code-annotation-target"><a href="#annotated-cell-7-61" aria-hidden="true" tabindex="-1"></a>        best_action <span class="op">=</span> <span class="bu">max</span>(</span>
<span id="annotated-cell-7-62"><a href="#annotated-cell-7-62" aria-hidden="true" tabindex="-1"></a>            env.action_space,</span>
<span id="annotated-cell-7-63"><a href="#annotated-cell-7-63" aria-hidden="true" tabindex="-1"></a>            key<span class="op">=</span><span class="kw">lambda</span> a: compute_state_action_value(env, s, a, value, Œ≥),</span>
<span id="annotated-cell-7-64"><a href="#annotated-cell-7-64" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="annotated-cell-7-65"><a href="#annotated-cell-7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-66"><a href="#annotated-cell-7-66" aria-hidden="true" tabindex="-1"></a>        œÄ[s] <span class="op">=</span> best_action</span>
<span id="annotated-cell-7-67"><a href="#annotated-cell-7-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> best_action <span class="op">!=</span> old_action:</span>
<span id="annotated-cell-7-68"><a href="#annotated-cell-7-68" aria-hidden="true" tabindex="-1"></a>            stable <span class="op">=</span> <span class="va">False</span></span>
<span id="annotated-cell-7-69"><a href="#annotated-cell-7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-70"><a href="#annotated-cell-7-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stable</span>
<span id="annotated-cell-7-71"><a href="#annotated-cell-7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-72"><a href="#annotated-cell-7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-73"><a href="#annotated-cell-7-73" aria-hidden="true" tabindex="-1"></a>PolicyIterationStep <span class="op">=</span> namedtuple(<span class="st">"PolicyIterationStep"</span>, [<span class="st">"policy"</span>, <span class="st">"values"</span>])</span>
<span id="annotated-cell-7-74"><a href="#annotated-cell-7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-75"><a href="#annotated-cell-7-75" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="5">5</button><span id="annotated-cell-7-76" class="code-annotation-target"><a href="#annotated-cell-7-76" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_iteration(env, Œ∏, Œ≥) <span class="op">-&gt;</span> List[PolicyIterationStep]:</span>
<span id="annotated-cell-7-77"><a href="#annotated-cell-7-77" aria-hidden="true" tabindex="-1"></a>    optimal <span class="op">=</span> <span class="va">False</span></span>
<span id="annotated-cell-7-78"><a href="#annotated-cell-7-78" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> []</span>
<span id="annotated-cell-7-79"><a href="#annotated-cell-7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-80"><a href="#annotated-cell-7-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init policy and value-function</span></span>
<span id="annotated-cell-7-81"><a href="#annotated-cell-7-81" aria-hidden="true" tabindex="-1"></a>    œÄ: Policy <span class="op">=</span> {s: <span class="dv">0</span> <span class="cf">for</span> s <span class="kw">in</span> env.state_space}</span>
<span id="annotated-cell-7-82"><a href="#annotated-cell-7-82" aria-hidden="true" tabindex="-1"></a>    value: ValueFn <span class="op">=</span> {s: <span class="fl">0.0</span> <span class="cf">for</span> s <span class="kw">in</span> env.state_space}</span>
<span id="annotated-cell-7-83"><a href="#annotated-cell-7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-84"><a href="#annotated-cell-7-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> optimal:</span>
<span id="annotated-cell-7-85"><a href="#annotated-cell-7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-86"><a href="#annotated-cell-7-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># evaluation and save</span></span>
<span id="annotated-cell-7-87"><a href="#annotated-cell-7-87" aria-hidden="true" tabindex="-1"></a>        policy_evaluation(env, œÄ, value, Œ∏, Œ≥)</span>
<span id="annotated-cell-7-88"><a href="#annotated-cell-7-88" aria-hidden="true" tabindex="-1"></a>        history.append(PolicyIterationStep(œÄ.copy(), value.copy()))</span>
<span id="annotated-cell-7-89"><a href="#annotated-cell-7-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-90"><a href="#annotated-cell-7-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># find better policy</span></span>
<span id="annotated-cell-7-91"><a href="#annotated-cell-7-91" aria-hidden="true" tabindex="-1"></a>        optimal <span class="op">=</span> policy_improvement(env, œÄ, value, Œ≥)</span>
<span id="annotated-cell-7-92"><a href="#annotated-cell-7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-93"><a href="#annotated-cell-7-93" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> history</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-7" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="4,5,6,7,8" data-code-annotation="1">Import the environment class and type aliases (State, Action) from the Jack‚Äôs car‚Äêrental module.</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="10" data-code-annotation="2">Since we are dealing with deterministic policies we can define Policy as a mapping.</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="14,15,16" data-code-annotation="3">Implements the one‚Äêstep lookahead with afterstates (see <a href="#eq-one-step-lookahead-with-afterstates" class="quarto-xref">Equation&nbsp;<span>4.4</span></a>).</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="61,63,64" data-code-annotation="4">Choose the action <span class="math inline">\(a\in \mathcal{A}(s)\)</span> that maximises the one‚Äêstep lookahead; this is a concise way to do ‚Äú<span class="math inline">\(\mathrm{argmax}_a \dots\)</span>‚Äù‚Äù in Python</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="76" data-code-annotation="5">The only slight change to the pseudocode: instead of just returning the optimal policy and its state-value function we return the history of policies.</span>
</dd>
</dl>
</div>
</div>
<p>Here is some more code just to be able to visualize our solution for Jack‚Äôs car rental problem‚Ä¶</p>
<div id="a938021a" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> BoundaryNorm</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scripts.jacks_car_rental.jacks_car_rental <span class="im">import</span> (</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    JacksCarRentalConfig,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_policy(title: <span class="bu">str</span>, config: JacksCarRentalConfig, œÄ: <span class="bu">dict</span>):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    max_cars <span class="op">=</span> config.max_cars</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    max_move <span class="op">=</span> config.max_move</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build a (max_cars+1)√ó(max_cars+1) integer grid of ‚Äúaction‚Äù values</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    policy_grid <span class="op">=</span> np.zeros((max_cars <span class="op">+</span> <span class="dv">1</span>, max_cars <span class="op">+</span> <span class="dv">1</span>), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (cars1, cars2), action <span class="kw">in</span> œÄ.items():</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        policy_grid[cars1, cars2] <span class="op">=</span> action</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X/Y coordinates for pcolormesh:</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.arange(max_cars <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.arange(max_cars <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discrete actions range</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    actions <span class="op">=</span> np.arange(<span class="op">-</span>max_move, max_move <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    n_colors <span class="op">=</span> <span class="bu">len</span>(actions)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a ‚Äúcoolwarm‚Äù colormap with exactly n_colors bins</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    cmap <span class="op">=</span> plt.get_cmap(<span class="st">"coolwarm"</span>, n_colors)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For a discrete colormap, we want boundaries at x.5, so that integer values</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get mapped to their own color. Example: if max_move=2, actions = [-2, -1, 0, 1, 2],</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># then boundaries = [-2.5, -1.5, -0.5, 0.5, 1.5, 2.5].</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    bounds <span class="op">=</span> np.arange(<span class="op">-</span>max_move <span class="op">-</span> <span class="fl">0.5</span>, max_move <span class="op">+</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">1e-6</span>, <span class="dv">1</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> BoundaryNorm(boundaries<span class="op">=</span>bounds, ncolors<span class="op">=</span>cmap.N)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    cax <span class="op">=</span> ax.pcolormesh(</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        X,</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        Y,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        policy_grid,</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        cmap<span class="op">=</span>cmap,</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        norm<span class="op">=</span>norm,</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        edgecolors<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        linewidth<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Axis labels and title</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"Cars at Location&nbsp;2"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"Cars at Location&nbsp;1"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title, fontsize<span class="op">=</span><span class="dv">14</span>, pad<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Square aspect ratio so each cell is a square:</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">"equal"</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ticks every 5 cars</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks(np.arange(<span class="dv">0</span>, max_cars <span class="op">+</span> <span class="dv">1</span>, step))</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks(np.arange(<span class="dv">0</span>, max_cars <span class="op">+</span> <span class="dv">1</span>, step))</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Colorbar (horizontal, at the bottom)</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    cbar <span class="op">=</span> fig.colorbar(</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        cax,</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span>ax,</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        orientation<span class="op">=</span><span class="st">"horizontal"</span>,</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        pad<span class="op">=</span><span class="fl">0.08</span>,</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        shrink<span class="op">=</span><span class="fl">0.85</span>,</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>        boundaries<span class="op">=</span>bounds,</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>        ticks<span class="op">=</span>actions,</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">"Action‚Äâ(Car Movement)"</span>,</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    cbar.ax.xaxis.set_label_position(<span class="st">"bottom"</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    cbar.ax.xaxis.tick_bottom()</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    fig.tight_layout(rect<span class="op">=</span>[<span class="dv">0</span>, <span class="fl">0.03</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_valueFn(title, config, val):</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="co">    3D surface plot of the value function</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>    max_cars <span class="op">=</span> config.max_cars</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build a (max_cars+1)√ó(max_cars+1) grid of value estimates</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    value_grid <span class="op">=</span> np.zeros((max_cars <span class="op">+</span> <span class="dv">1</span>, max_cars <span class="op">+</span> <span class="dv">1</span>), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (l1, l2), v <span class="kw">in</span> val.items():</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>        value_grid[l1, l2] <span class="op">=</span> v</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Meshgrid for locations on each axis</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.arange(max_cars <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.arange(max_cars <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">11</span>, <span class="dv">7</span>))</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">"3d"</span>)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shaded surface plot</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>    surf <span class="op">=</span> ax.plot_surface(</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>        X,</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>        Y,</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>        value_grid,</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>        rstride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>        cstride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>        cmap<span class="op">=</span>cm.viridis,</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>        edgecolor<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>        antialiased<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"Cars at Location‚ÄØ2"</span>, fontsize<span class="op">=</span><span class="dv">12</span>, labelpad<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"Cars at Location‚ÄØ1"</span>, fontsize<span class="op">=</span><span class="dv">12</span>, labelpad<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title, fontsize<span class="op">=</span><span class="dv">14</span>, pad<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>    ax.view_init(elev<span class="op">=</span><span class="dv">35</span>, azim<span class="op">=-</span><span class="dv">60</span>)</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>    fig.tight_layout()</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>‚Ä¶and now we can solve it:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Solving Jack's car rental ===</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter for "training"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>Œ≥ <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>Œ∏ <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># config and environment</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> JacksCarRentalConfig(max_cars<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> JacksCarRental(config)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># do policy iteration</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> policy_iteration(env, Œ∏, Œ≥)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># print last (optimal) policy and its value function</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plot_policy(</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Optimal Policy after </span><span class="sc">{</span><span class="bu">len</span>(history)<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>, config, history[<span class="op">-</span><span class="dv">1</span>].policy</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plot_valueFn(<span class="ss">f"Value function for optimal policy"</span>, config, history[<span class="op">-</span><span class="dv">1</span>].values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-jacks-car-rental-optimal-solution" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="4">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-jacks-car-rental-optimal-solution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-jacks-car-rental-optimal-solution-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-jacks-car-rental-optimal-solution-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="04-dynamic-programming_files/figure-html/fig-jacks-car-rental-optimal-solution-output-1.png" class="lightbox" data-gallery="fig-jacks-car-rental-optimal-solution" title="Figure&nbsp;4.1&nbsp;(a): Heatmap of the optimal policy \pi_‚àó(s). Each cell at coordinates (i,j) shows the number of cars moved from location 1 to location 2."><img src="04-dynamic-programming_files/figure-html/fig-jacks-car-rental-optimal-solution-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-jacks-car-rental-optimal-solution"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-jacks-car-rental-optimal-solution-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Heatmap of the optimal policy <span class="math inline">\(\pi_‚àó(s)\)</span>. Each cell at coordinates <span class="math inline">\((i,j)\)</span> shows the number of cars moved from location 1 to location 2.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-jacks-car-rental-optimal-solution-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-jacks-car-rental-optimal-solution-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="04-dynamic-programming_files/figure-html/fig-jacks-car-rental-optimal-solution-output-2.png" class="lightbox" data-gallery="fig-jacks-car-rental-optimal-solution" title="Figure&nbsp;4.1&nbsp;(b): 3D surface of the optimal state‚Äêvalue function v_‚àó‚Äã(s) corresponding to the policy in (a). The maximum value here is approximately \max‚Å°_s v_‚àó(s)\approx 625; Sutton &amp;‚ÄØBarto‚Äôs reported maximum is \approx 612."><img src="04-dynamic-programming_files/figure-html/fig-jacks-car-rental-optimal-solution-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-jacks-car-rental-optimal-solution"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-jacks-car-rental-optimal-solution-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) 3D surface of the optimal state‚Äêvalue function <span class="math inline">\(v_‚àó‚Äã(s)\)</span> corresponding to the policy in (a). The maximum value here is approximately <span class="math inline">\(\max‚Å°_s v_‚àó(s)\approx 625\)</span>; Sutton &amp;‚ÄØBarto‚Äôs reported maximum is <span class="math inline">\(\approx 612\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-jacks-car-rental-optimal-solution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: This is like the last two diagrmas in Figure 4.2 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>: the first diagrams shows the optimal policy - which looks to me identical to Sutton-Barto‚Äôs optimal policy. The second diagram shows its value function (the optimal value function), which seems to have a higher maximum as Sutton-Barto‚Äôs - I don‚Äôt know why.
</figcaption>
</figure>
</div>
</div>
<p>The runtime of this policy iteration is quite reasonable - just a few seconds. But considering we‚Äôre only dealing with <span class="math inline">\(441\)</span> states, it highlights how dynamic programming is limited to small MDPs.</p>
<p>Another thought: although we now have the optimal solution, the meaning of the value function is still somewhat abstract. Mathematically it‚Äôs clear, but I couldn‚Äôt, for example, say whether Jack can pay his monthly rent with this business.</p>
</div>
<div id="exr-4.4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.4</strong></span> The policy iteration algorithm <a href="#lst-policy-iteration" class="quarto-xref">Listing&nbsp;<span>4.2</span></a> has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the seudocode so that convergence is guaranteed.</p>
</div>
<div id="sol-4.4" class="proof solution">
<p><span class="proof-title"><em>Solution 4.4</em>. </span>If ties in the argmax are determined randomly, this could maybe result in a soft-lock when there are enough states with ties in their evaluation. Here we should add a condition that the policy is only changed if the change results in an actual improvement. Often in application there is an order on the actions, and we could also choose the smallest. However, this might also have consequences for the exploration of the algorithms.</p>
<p>So maybe the best solution is to only change the policy action if a better action improves the value by more than some <span class="math inline">\(\epsilon&gt;0\)</span>:</p>
<div id="lst-modified-policy-improvement" class="listing-block listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-modified-policy-improvement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4.3: Modified step <strong>Policy Improvement</strong> in Policy Iteration <a href="#lst-policy-iteration" class="quarto-xref">Listing&nbsp;<span>4.2</span></a>
</figcaption>
<div aria-describedby="lst-modified-policy-improvement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="line-block"><strong>Extra Paramater:</strong><br>
<span class="math inline">\(\epsilon\)</span>: small parameter determining of policy is stable<br>
<br>
<strong>Policy Improvement</strong><br>
<span class="math inline">\(\text{policy-stable} \gets \mathrm{true}\)</span><br>
<br>
for each <span class="math inline">\(s \in \mathcal{S}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{old-action} \gets \pi(s)\)</span><br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;for each <span class="math inline">\(a \in \mathcal{A}(s)\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(Q(a) \gets \sum_{s',r} p(s',r | s,a) [ r + \gamma V(s') ]\)</span><br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{best-value} \gets \max_{a} Q(a)\)</span><br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;if <span class="math inline">\(\text{best-value} - Q(\text{old-action}) &gt; \epsilon\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\pi(s) \gets a^*\)</span> for which <span class="math inline">\(Q(a_*) = \text{best-value}\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{policy‚Äêstable} \gets \mathrm{false}\)</span><br>
<br>
if policy‚Äêstable:<br>
&nbsp;&nbsp;&nbsp;&nbsp;return <span class="math inline">\(V \approx v_*\)</span> and <span class="math inline">\(œÄ ‚âà œÄ_*\)</span><br>
else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;go to <strong>Policy Evaluation</strong></div>
</div>
</figure>
</div>
</div>
<div id="exr-4.5" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.5</strong></span> How would policy iteration be defined for action values? Give a complete algorithm for computing <span class="math inline">\(q_*\)</span>, analogous to <a href="#lst-policy-iteration" class="quarto-xref">Listing&nbsp;<span>4.2</span></a> for computing <span class="math inline">\(v_*\)</span>. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.</p>
</div>
<div id="sol-4.5" class="proof solution">
<p><span class="proof-title"><em>Solution 4.5</em>. </span>The code depends on the convention that episodic tasks have an absorbing state that transitions only to itself and that generates only rewards of zero <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018, sec. 3.4</a>)</span>.</p>
<div id="lst-policy-iteration-for-q" class="listing-block listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-policy-iteration-for-q-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4.4: Policy Iteration (using iterative policy evaluation) for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</figcaption>
<div aria-describedby="lst-policy-iteration-for-q-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="line-block"><strong>Parameter</strong>:<br>
<span class="math inline">\(\theta\)</span>: a small positive number determining the accuracy of estimation<br>
<br>
<strong>Initialisation</strong>:<br>
<span class="math inline">\(Q(s,a) \in \mathbb{R}\)</span>, <span class="math inline">\(\pi(s) \in \mathcal{A}(s)\)</span> arbitrarily, but <span class="math inline">\(Q(\mathrm{terminal},a) = 0\)</span><br>
<br>
<strong>Policy Evaluation</strong><br>
Loop:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets 0\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;Loop for each <span class="math inline">\(s \in \mathcal{S}\)</span> and <span class="math inline">\(a \in \mathcal{A}(s)\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(q \gets Q(s,a)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(Q(s,a) \gets  \sum_{s',r}p(s',r|s,a)[r + \gamma Q(s',\pi(s'))]\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets \max(\Delta, |q - Q(s,a)|)\)</span><br>
until <span class="math inline">\(\Delta &lt; \theta\)</span><br>
<br>
<strong>Policy Improvement</strong><br>
<span class="math inline">\(\text{policy-stable} \gets \mathrm{true}\)</span><br>
For each <span class="math inline">\(s \in \mathcal{S}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{old-action} \gets \pi(s)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\pi(s) \gets \underset{a}{\mathrm{argmax}}  \; Q(s,a)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;If <span class="math inline">\(\text{old-action} \neq \pi(s)\)</span>, then <span class="math inline">\(\text{policy-stable} \gets \text{false}\)</span><br>
If <span class="math inline">\(\text{policy-stable}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;return <span class="math inline">\(Q \approx q_*\)</span> and <span class="math inline">\(\pi \approx \pi_*\)</span><br>
else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;go to <strong>Policy Evaluation</strong></div>
</div>
</figure>
</div>
</div>
<div id="exr-4.6" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.6</strong></span> Suppose you are restricted to considering only policies that are <span class="math inline">\(\varepsilon\)</span>-soft, meaning that the probability of selecting each action in each state, <span class="math inline">\(s\)</span>, is at least <span class="math inline">\(\varepsilon/|\mathcal{A}(s)|\)</span>. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm <a href="#lst-policy-iteration" class="quarto-xref">Listing&nbsp;<span>4.2</span></a> for <span class="math inline">\(v_*\)</span>.</p>
</div>
<div id="sol-4.6" class="proof solution">
<p><span class="proof-title"><em>Solution 4.6</em>. </span>First of all, we are now dealing with a policy <span class="math inline">\(\pi(a|s)\)</span> that assigns probabilities to actions instead of <span class="math inline">\(\pi(s)\)</span> that choses an action.</p>
<p>For step 3, policy improvement, we assign to every possible action <span class="math inline">\(a\)</span> a propability <span class="math inline">\(\varepsilon\)</span>, then the remainining propability <span class="math inline">\(1 - \varepsilon|\mathbfcal{A}(s)|\)</span> to the actions that maximize the expression under the argmax.</p>
<p>For step 2, policy evaluation, we have to change the assignment to <span class="math inline">\(V(s) \gets \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma V(s')]\)</span></p>
<p>For step 1, initialisation, we have to make sure that each action gets a probability of at least <span class="math inline">\(\varepsilon\)</span>.</p>
</div>
<div id="exr-4.7" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.7</strong></span> Write a program for policy iteration and re-solve Jack‚Äôs car rental problem with the following changes. One of Jack‚Äôs employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem.</p>
</div>
<div id="sol-4.7" class="proof solution">
<p><span class="proof-title"><em>Solution 4.7</em>. </span>We have already set up everyting in <a href="#exm-4.2" class="quarto-xref">Example&nbsp;<span>4.2</span></a>. I made sure in the implementation for the environment to include parameters for these changes.</p>
<div id="b0806148" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === solving Jack's car rental again ===</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter for "training"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>Œ≥ <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Œ∏ <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># config and environment</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> JacksCarRentalConfig(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    max_cars<span class="op">=</span><span class="dv">20</span>, free_moves_from_1_to_2<span class="op">=</span><span class="dv">1</span>, max_free_parking<span class="op">=</span><span class="dv">10</span>, extra_parking_cost<span class="op">=</span><span class="dv">4</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> JacksCarRental(config)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># do policy iteration</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> policy_iteration(env, Œ∏, Œ≥)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># print last optimal policy</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>plot_policy(</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Optimal Policy after </span><span class="sc">{</span><span class="bu">len</span>(history)<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>, config, history[<span class="op">-</span><span class="dv">1</span>].policy</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="04-dynamic-programming_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Interestingly this somewhat more complex problem, does need 1 less iteration than the original.</p>
</div>
</section>
<section id="value-iteration" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="value-iteration"><span class="header-section-number">4.4</span> Value Iteration</h2>
<p>Value iteration is policy iteration with a single sweep of policy evaluation per iteration.</p>
<p>In policy iteration, policy improvement is given by: <span class="math display">\[
\pi_k(s) = \mathrm{argmax}_{a} \sum_{s',r} p(s',r|s,a)[r + \gamma v_k(s')]
\]</span></p>
<p>This picks an action <span class="math inline">\(\hat{a}\)</span> that gives the best one-step lookahead from the current value function.</p>
<p>Then, policy evaluation, uses the one-step lookahead for the updates: <span class="math display">\[
v_{k+1}(s) = \sum_{s',r}p(s',r|s,\pi_k(s)) [r + \gamma v_k(s')]
\]</span></p>
<p>But if we‚Äôre only doing one sweep, we may as well just plug in <span class="math inline">\(\hat{a}\)</span> directly from the lookahead, which gives the maximum of this expression: <span id="eq-value-iteration-update"><span class="math display">\[
v_{k+1}(s) = \max_a \sum_{s',} p(s',r|s,a)[r + \gamma v_k(s')]
\tag{4.5}\]</span></span></p>
<p>So, value iteration performs both greedy action selection and value backup in one go.</p>
<p>And here is the respective pseudocode:</p>
<div id="lst-value-iteration" class="listing-block listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-value-iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4.5: Value Iteration, for estimating <span class="math inline">\(\pi \approx \pi_*\)</span>
</figcaption>
<div aria-describedby="lst-value-iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="line-block"><strong>Parameter</strong>:<br>
<span class="math inline">\(\theta\)</span>: a small positive number determining the accuracy of estimation<br>
<br>
<strong>Initialisation</strong>:<br>
<span class="math inline">\(V(s)\)</span> arbitrarily for <span class="math inline">\(s \in \mathcal{S}\)</span>, when episodic <span class="math inline">\(V(\mathrm{terminal}) = 0\)</span><br>
<br>
<strong>Value Iteration</strong><br>
Loop:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets 0\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;Loop for each <span class="math inline">\(s \in \mathcal{S}\)</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(v \gets V(s)\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(V(s) \gets  \max_a \sum_{s',r} p(s',p|s,a) [r + \gamma V(s')]\)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\Delta \gets \max(\Delta, |v - V(s)|)\)</span><br>
until <span class="math inline">\(\Delta &lt; \theta\)</span><br>
<br>
<strong>Greedy Policy Extraction</strong><br>
Output a deterministic policy <span class="math inline">\(\pi \approx \pi_*\)</span>, such that<br>
<span class="math inline">\(\pi(s) = \mathrm{argmax}_a \sum_{s',r} p(s',p|s,a) [r + \gamma V(s')]\)</span></div>
</div>
</figure>
</div>
<section id="gamblers-problem" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="gamblers-problem"><span class="header-section-number">4.4.1</span> gambler‚Äôs problem</h3>
<p>Let‚Äôs talk about Example 4.3 from <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span> - the Gambler‚Äôs Problem. It gets its own little subsection because there‚Äôs actually quite a bit to say about it.</p>
<p>First, a quick summary of the MPD:</p>
<ul>
<li>the idea is to win a target amount <span class="math inline">\(N\)</span> of chips by betting on coin flips</li>
<li>non-terminal states are the current amount of chips (captial): <span class="math inline">\(\mathcal{S} = \{0,1,\dots,N\}\)</span>, and the terminal states are <span class="math inline">\(0\)</span> and <span class="math inline">\(N\)</span>.</li>
<li>actions are how mamny chips you wager (the stake): <span class="math inline">\(\mathcal{A}(s) = \{1,\dots, \min(s, N-s)\}\)</span> for <span class="math inline">\(s \in \mathcal{S}\)</span></li>
<li>the environment dynamics are <span class="math display">\[
p(s' \mid a,s) = \begin{cases}p_{\mathrm{win}} &amp;\text{if }s' = s+a\\1-p_{\mathrm{win}} &amp;\text{if }s' = s-a \end{cases}
\]</span></li>
<li>rewards are 0 except for reaching the goal state <span class="math inline">\(N\)</span>, which gives a rewards of +1</li>
<li>the task is episodic and undiscounted (<span class="math inline">\(\gamma = 1\)</span>)</li>
</ul>
<p>The reward structure is set up so that the state-value function gives the probability of eventually reaching the goal from a given state: <span class="math display">\[
v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = 1 \cdot \mathrm{Pr}_{\pi}(S_T = N \mid S_t = s)
\]</span></p>
<p>So under any policy <span class="math inline">\(\pi\)</span>, the value of a state is just the probability of hitting <span class="math inline">\(N\)</span> before hitting <span class="math inline">\(0\)</span>.</p>
<section id="general-remarks" class="level4" data-number="4.4.1.1">
<h4 data-number="4.4.1.1" class="anchored" data-anchor-id="general-remarks"><span class="header-section-number">4.4.1.1</span> general remarks</h4>
<p>There‚Äôs a lot of content about this problem floating around online. Maybe that‚Äôs because it appears quite early in the book, or because it‚Äôs so simple to implement. Or maybe it‚Äôs because the problem hides a few trip wires. For one thing, if you implement it yourself without really understanding what‚Äôs going on, your correct solution might look completely wrong (see <a href="#fig-gamblers-problem-optimal-policy" class="quarto-xref">Figure&nbsp;<span>4.2 (b)</span></a>).</p>
<p>A lot of the articles I‚Äôve come across either lack substance or seem a bit confused - which is totally fair, but I personally prefer reading something more insightful when digging into a problem.</p>
<p>So here, I‚Äôm trying to give this a bit of extra depth.</p>
</section>
<section id="no-0-stakes" class="level4" data-number="4.4.1.2">
<h4 data-number="4.4.1.2" class="anchored" data-anchor-id="no-0-stakes"><span class="header-section-number">4.4.1.2</span> no 0 stakes</h4>
<p>Another issue you might stumble across is that the original example allows wagers of size 0. Which seems innocuous, but it actually complicates things when thinking about deterministic policies.</p>
<p>Since the problem is undiscounted, we have: <span class="math display">\[
q_*(s,0) = v_*(s) = \max_a q_*(s,a)
\]</span></p>
<p>So technically, wagering nothing could be considered an optimal action. However, any deterministic policy following such an action will not complete an episode.</p>
<p>Furthermore, zero stakes can also mess with value iteration. If we initialise the non-terminal states with a value greater than their optimal value, the algorithm will not update them, because it will just set <span class="math inline">\(v(s) = q(s,0)\)</span>.</p>
<p>It mostly leads to problems, so let‚Äôs leave them just out here.</p>
</section>
<section id="one-step-lookahead-and-environment" class="level4" data-number="4.4.1.3">
<h4 data-number="4.4.1.3" class="anchored" data-anchor-id="one-step-lookahead-and-environment"><span class="header-section-number">4.4.1.3</span> one-step lookahead and environment</h4>
<p>We‚Äôll follow the design proposed by <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span>: no rewards, and two terminal states:</p>
<ul>
<li>ruin: 0 capital, with value <span class="math inline">\(0\)</span></li>
<li>win: 100 captial, with value <span class="math inline">\(1\)</span></li>
</ul>
<p>The terminal states are not updated during value iteration, of course.</p>
<p>Given that, the one-step lookahead for stake <span class="math inline">\(s\)</span> and wager <span class="math inline">\(a\)</span> is especially simple to compute: <span class="math display">\[
Q(s,a) = p_\mathrm{win} \cdot V(s+a) + (1-p_\mathrm{win}) \cdot V(s-a).
\]</span></p>
<p>In code, it‚Äôs the environment‚Äôs responsibility to calculate the one-step lookaheads for a given state-value function (<code>one_step_lookaheads</code>), as well as to initialise the value function (<code>make_initial_values</code>). This gives us some nice encapsulation.</p>
<div id="699a3729" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>ValueFn <span class="op">=</span> List[<span class="bu">float</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>State <span class="op">=</span> <span class="bu">int</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EnvGamblersProblem:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p_win: <span class="bu">float</span>, goal: <span class="bu">int</span>):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p_win</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.goal <span class="op">=</span> goal</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.non_terminal_states <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, goal))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> make_initial_values(<span class="va">self</span>, non_terminal_value: <span class="bu">float</span>) <span class="op">-&gt;</span> ValueFn:</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> [<span class="fl">0.0</span>]  <span class="co"># terminal: ruin</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        v.extend([non_terminal_value] <span class="op">*</span> (<span class="va">self</span>.goal <span class="op">-</span> <span class="dv">1</span>))  <span class="co"># non-terminals</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        v.append(<span class="fl">1.0</span>)  <span class="co"># terminal: win</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> one_step_lookaheads(<span class="va">self</span>, s: State, v: ValueFn):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""returns a list of the q-values for state s.</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">        q[i] contains the value of betting an amount of i+1"""</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.p</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        goal <span class="op">=</span> <span class="va">self</span>.goal</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            p <span class="op">*</span> v[s <span class="op">+</span> a] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">*</span> v[s <span class="op">-</span> a] <span class="cf">for</span> a <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">min</span>(s, goal <span class="op">-</span> s) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="solving-the-problem" class="level4" data-number="4.4.1.4">
<h4 data-number="4.4.1.4" class="anchored" data-anchor-id="solving-the-problem"><span class="header-section-number">4.4.1.4</span> solving the problem</h4>
<p>Now we can implement value iteration, as described in <a href="#lst-value-iteration" class="quarto-xref">Listing&nbsp;<span>4.5</span></a>. We split the process into the two parts: <strong>value iteration</strong> and <strong>policy extraction</strong>.</p>
<p>I‚Äôve modified the value iteration function so that it returns all intermediate value functions - the final one is the optimal state-value function.</p>
<div id="05aca079" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="annotated-cell-2"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> value_iteration_gamblers_problem(</span>
<span id="annotated-cell-2-2"><a href="#annotated-cell-2-2" aria-hidden="true" tabindex="-1"></a>    env: EnvGamblersProblem, Œ∏: <span class="bu">float</span>, init_value<span class="op">=</span><span class="fl">0.0</span></span>
<span id="annotated-cell-2-3"><a href="#annotated-cell-2-3" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> List[ValueFn]:</span>
<span id="annotated-cell-2-4"><a href="#annotated-cell-2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># **Init**</span></span>
<span id="annotated-cell-2-5"><a href="#annotated-cell-2-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> env.make_initial_values(init_value)</span>
<span id="annotated-cell-2-6"><a href="#annotated-cell-2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-2-7"><a href="#annotated-cell-2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># **Value iteration**</span></span>
<span id="annotated-cell-2-8"><a href="#annotated-cell-2-8" aria-hidden="true" tabindex="-1"></a>    value_functions <span class="op">=</span> []</span>
<span id="annotated-cell-2-9"><a href="#annotated-cell-2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="annotated-cell-2-10"><a href="#annotated-cell-2-10" aria-hidden="true" tabindex="-1"></a>        value_functions.append(v.copy())</span>
<span id="annotated-cell-2-11"><a href="#annotated-cell-2-11" aria-hidden="true" tabindex="-1"></a>        Œî <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="annotated-cell-2-12"><a href="#annotated-cell-2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> env.non_terminal_states:</span>
<span id="annotated-cell-2-13"><a href="#annotated-cell-2-13" aria-hidden="true" tabindex="-1"></a>            v_old <span class="op">=</span> v[s]</span>
<span id="annotated-cell-2-14"><a href="#annotated-cell-2-14" aria-hidden="true" tabindex="-1"></a>            v_new <span class="op">=</span> <span class="bu">max</span>(env.one_step_lookaheads(s, v))</span>
<span id="annotated-cell-2-15"><a href="#annotated-cell-2-15" aria-hidden="true" tabindex="-1"></a>            v[s] <span class="op">=</span> v_new</span>
<span id="annotated-cell-2-16"><a href="#annotated-cell-2-16" aria-hidden="true" tabindex="-1"></a>            Œî <span class="op">=</span> <span class="bu">max</span>(Œî, <span class="bu">abs</span>(v_old <span class="op">-</span> v_new))</span>
<span id="annotated-cell-2-17"><a href="#annotated-cell-2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> Œî <span class="op">&lt;</span> Œ∏:</span>
<span id="annotated-cell-2-18"><a href="#annotated-cell-2-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="annotated-cell-2-19"><a href="#annotated-cell-2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-2-20"><a href="#annotated-cell-2-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> value_functions</span>
<span id="annotated-cell-2-21"><a href="#annotated-cell-2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-2-22"><a href="#annotated-cell-2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-2-23"><a href="#annotated-cell-2-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_greedy_policy(v: ValueFn, env: EnvGamblersProblem):</span>
<span id="annotated-cell-2-24"><a href="#annotated-cell-2-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ** Greedy Policy Extraction **</span></span>
<span id="annotated-cell-2-25"><a href="#annotated-cell-2-25" aria-hidden="true" tabindex="-1"></a>    policy <span class="op">=</span> {}</span>
<span id="annotated-cell-2-26"><a href="#annotated-cell-2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> env.non_terminal_states:</span>
<span id="annotated-cell-2-27"><a href="#annotated-cell-2-27" aria-hidden="true" tabindex="-1"></a>        action_values <span class="op">=</span> env.one_step_lookaheads(s, v)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1">1</button><span id="annotated-cell-2-28" class="code-annotation-target"><a href="#annotated-cell-2-28" aria-hidden="true" tabindex="-1"></a>        greedy_action_idx <span class="op">=</span> action_values.index(<span class="bu">max</span>(action_values))</span>
<span id="annotated-cell-2-29"><a href="#annotated-cell-2-29" aria-hidden="true" tabindex="-1"></a>        policy[s] <span class="op">=</span> greedy_action_idx <span class="op">+</span> <span class="dv">1</span>  <span class="co"># convert idx -&gt; stake</span></span>
<span id="annotated-cell-2-30"><a href="#annotated-cell-2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> policy</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="28" data-code-annotation="1">this isn‚Äôt the most efficient way to compute the argmax of a list, but it‚Äôs fine for a simple problem like this.</span>
</dd>
</dl>
</div>
</div>
<p>Now, using the code below, we can solve the gambler‚Äôs problem for <span class="math inline">\(N = 100\)</span> and <span class="math inline">\(p_\mathrm{win} = 0.4\)</span>.</p>
<div id="10a19981" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set up environment</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.4</span>, goal<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># run value_iteration</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># optain optimal value function (result of last sweep)</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># optain optimal policy (greedy w.r.t to v_star)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>policy_star <span class="op">=</span> get_greedy_policy(v_star, env)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We plot the value function sweeps and a (somewhat strangely shaped) optimal policy below in <a href="#fig-gamblers-problem-solution" class="quarto-xref">Figure&nbsp;<span>4.2</span></a>.</p>
<p>We‚Äôre not going to go into the mathematical analysis of the exact solution here, but if you‚Äôre curious about exact formulas for the value function, check out the section about ‚ÄòBold Strategy‚Äô for the game ‚ÄòRed and Black‚Äô - which is apparently the mathematicans way of calling the gambler‚Äôs problem - in <span class="citation" data-cites="randomservices">Siegrist (<a href="#ref-randomservices" role="doc-biblioref">2023</a>)</span>. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div id="fig-gamblers-problem-solution" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gamblers-problem-solution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gamblers-problem-solution" style="flex-basis: 50.0%;justify-content: flex-start;">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_value_function(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    value_functions: List[Tuple[<span class="bu">str</span>, ValueFn]],</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    title: <span class="bu">str</span> <span class="op">=</span> <span class="st">"Value Function Sweeps"</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label, value_function <span class="kw">in</span> value_functions:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        plt.plot(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(value_function)), value_function, label<span class="op">=</span>label)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"State (Capital)"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># sweeps to plot</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>sweeps <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="bu">len</span>(value_functions) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>value_functions_with_labels <span class="op">=</span> [</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span><span class="st">"final "</span> <span class="cf">if</span> i <span class="op">==</span> <span class="bu">len</span>(value_functions) <span class="op">-</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">""</span> <span class="sc">}</span><span class="ss">sweep </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        value_functions[i],</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> sweeps</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>plot_value_function(</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    value_functions_with_labels,</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="vs">r"Value Function Sweeps for </span><span class="dv">$</span><span class="vs">N=100</span><span class="dv">$</span><span class="vs"> and </span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win}=0</span><span class="dv">.</span><span class="vs">4</span><span class="dv">$</span><span class="vs">"</span>,</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-gamblers-problem-value-iteration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gamblers-problem-value-iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="d16d3aaf" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-9-output-1.png" class="lightbox" data-gallery="fig-gamblers-problem-solution" title="Figure&nbsp;4.2&nbsp;(a): This shows the value function over several sweeps of value iteration. The final sweep is the optimal value function. The first few sweeps (1-3) look very similar to what @sutton2018 show. Interestingly, though, in their plot it seems to take more sweeps, even sweep 32 still noticeably differs from the final result."><img src="04-dynamic-programming_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-gamblers-problem-solution"></a></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gamblers-problem-value-iteration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) This shows the value function over several sweeps of value iteration. The final sweep is the optimal value function. The first few sweeps (1-3) look very similar to what <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span> show. Interestingly, though, in their plot it seems to take more sweeps, even sweep 32 still noticeably differs from the final result.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gamblers-problem-solution" style="flex-basis: 50.0%;justify-content: flex-start;">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple, Dict</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_policy(policy: Dict[<span class="bu">int</span>, <span class="bu">int</span>], title: <span class="bu">str</span> <span class="op">=</span> <span class="st">"Policy"</span>):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> <span class="bu">sorted</span>(policy.keys())</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    actions <span class="op">=</span> [policy[s] <span class="cf">for</span> s <span class="kw">in</span> states]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    plt.scatter(states, actions)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"State (Capital)"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Action"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plot_policy(policy_star, title<span class="op">=</span><span class="vs">r"An optimal policy for </span><span class="dv">$</span><span class="vs">N=100</span><span class="dv">$</span><span class="vs"> and p_</span><span class="dv">\m</span><span class="vs">athrm{win</span><span class="dv">$</span><span class="vs"> = 0</span><span class="dv">.</span><span class="vs">4</span><span class="dv">$</span><span class="vs">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-gamblers-problem-optimal-policy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gamblers-problem-optimal-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="e56981a7" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-10-output-1.png" class="lightbox" data-gallery="fig-gamblers-problem-solution" title="Figure&nbsp;4.2&nbsp;(b): This shows one optimal policy. It looks quite different to the one shown by @sutton2018. That‚Äôs because many states have multiple optimal actions. The greedy action selection just picks one, and the result is ultimatively decided by floating-point imprecisions."><img src="04-dynamic-programming_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-gamblers-problem-solution"></a></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gamblers-problem-optimal-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) This shows <strong>one</strong> optimal policy. It looks quite different to the one shown by <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span>. That‚Äôs because many states have multiple optimal actions. The greedy action selection just picks one, and the result is ultimatively decided by floating-point imprecisions.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gamblers-problem-solution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: This is the analogue of Figure 4.3 from <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span>. It shows the solution to the gambler‚Äôs problem for <span class="math inline">\(N=100\)</span> and <span class="math inline">\(p_\mathrm{win} = 0.4\)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-gamblers-problem-optimal-actions" class="level4" data-number="4.4.1.5">
<h4 data-number="4.4.1.5" class="anchored" data-anchor-id="sec-gamblers-problem-optimal-actions"><span class="header-section-number">4.4.1.5</span> optimal actions</h4>
<p>So, <a href="#fig-gamblers-problem-optimal-policy" class="quarto-xref">Figure&nbsp;<span>4.2 (b)</span></a> looks jagged because, in some states, there are multiple optimal actions. The choice of which one is used isn‚Äôt governed by any particular logic in the implementation - it‚Äôs effectively decided by floating-point imprecision.</p>
<p>Here we want to answer this somewhat understated question from <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018, 84</a>)</span>:</p>
<blockquote class="blockquote">
<p>This policy [shown in Figure 4.3] is optimal, but not unique. In fact, there is a whole family of optimal policies, all corresponding to ties for the argmax action selection with respect to the optimal value function. Can you guess what the entire family looks like?</p>
</blockquote>
<p>The short answer is: no, I can‚Äôt. It‚Äôs not something that‚Äôs easy to guess, in my view.</p>
<p>However, <span class="citation" data-cites="randomservices">Siegrist (<a href="#ref-randomservices" role="doc-biblioref">2023</a>)</span> gives an account of some well-developed theory addressing this question in. The section about bold play in the game Red and Black proves the existence of optimal strategies.</p>
<p>One such optimal strategy pattern applicable for all <span class="math inline">\(p_\mathrm{win} &lt; 0.5\)</span> and <span class="math inline">\(N\)</span> is bold play, where you always bet the maximum amount possible: <span class="math display">\[
B(s) := \max \mathcal{A}(s) =  \min(\{s, N-s\})
\]</span></p>
<p>Actually, for any <span class="math inline">\(p_\mathrm{win} &lt; 0.5\)</span>, all optimal actions are the same - only the underlying value functions change. The shape of <span class="math inline">\(B(s)\)</span> is triangular, and if <span class="math inline">\(N\)</span> is odd, it is the unique optimal policy, as seen in <a href="#fig-gamblers-problem-optimal-odd" class="quarto-xref">Figure&nbsp;<span>4.3 (c)</span></a>.</p>
<p>If <span class="math inline">\(N\)</span> is even, then there exists a second-order bold strategy <span class="math inline">\(B_2\)</span>, which effectively applies divide and conquer to the problem (although I don‚Äôt have a really good intuition why it works). Create two subproblems, one from <span class="math inline">\(0\)</span> to <span class="math inline">\(N/2\)</span>, and another from <span class="math inline">\(N/2\)</span> to <span class="math inline">\(N\)</span>, each treated with their own bold strategy. It looks a bit like this (where <span class="math inline">\(\left\lfloor \frac{N}{4} \right\rfloor\)</span> means <span class="math inline">\(N/4\)</span> rounded down):</p>
<div id="5c2db602" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the x values</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the B_2(x)</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> B2(x):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">&lt;</span> <span class="fl">0.25</span>, x, np.where(x <span class="op">&lt;</span> <span class="fl">0.5</span>, <span class="fl">0.5</span> <span class="op">-</span> x, np.where(x <span class="op">&lt;</span> <span class="fl">0.75</span>, <span class="op">-</span><span class="fl">0.5</span> <span class="op">+</span> x, <span class="dv">1</span> <span class="op">-</span> x))</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>ax.plot(x, B2(x), color<span class="op">=</span><span class="st">"blue"</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight key points</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="fl">0.5</span>], [<span class="fl">0.5</span>], <span class="st">"o"</span>, color<span class="op">=</span><span class="st">"blue"</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.5</span>], [<span class="dv">0</span>], <span class="st">"o"</span>, mfc<span class="op">=</span><span class="st">"white"</span>, mec<span class="op">=</span><span class="st">"blue"</span>, zorder<span class="op">=</span><span class="dv">5</span>, clip_on<span class="op">=</span><span class="va">False</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Draw over axis</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Add dashed grid lines for key points</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="fl">0.25</span>, color<span class="op">=</span><span class="st">"gray"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="fl">0.5</span>, color<span class="op">=</span><span class="st">"gray"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>ax.axvline(<span class="fl">0.5</span>, color<span class="op">=</span><span class="st">"gray"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="fl">0.25</span>, <span class="fl">0.5</span>], [<span class="fl">0.25</span>, <span class="fl">0.5</span>], <span class="st">"gray"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="fl">0.75</span>, <span class="fl">0.5</span>], [<span class="fl">0.25</span>, <span class="fl">0.5</span>], <span class="st">"gray"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Add axis labels and ticks</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>])</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">"0"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="ch">\f</span><span class="vs">rac{N}</span><span class="op">{2}</span><span class="dv">$</span><span class="vs">"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="vs">N</span><span class="dv">$</span><span class="vs">"</span>])</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>ax.set_yticks([<span class="fl">0.25</span>, <span class="fl">0.5</span>])</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([<span class="vs">r"</span><span class="dv">$</span><span class="er">\</span><span class="vs">left</span><span class="er">\</span><span class="vs">lfloor </span><span class="ch">\f</span><span class="vs">rac{N}</span><span class="op">{4}</span><span class="vs"> </span><span class="ch">\r</span><span class="vs">ight</span><span class="ch">\r</span><span class="vs">floor</span><span class="dv">$</span><span class="vs">"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="ch">\f</span><span class="vs">rac{N}</span><span class="op">{2}</span><span class="dv">$</span><span class="vs">"</span>])</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">s</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">B_2</span><span class="kw">(</span><span class="vs">s</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>, rotation<span class="op">=</span><span class="dv">0</span>, labelpad<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Style the plot</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"top"</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"right"</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"left"</span>].set_position(<span class="st">"zero"</span>)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>ax.spines[<span class="st">"bottom"</span>].set_position(<span class="st">"zero"</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">0.55</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">False</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="04-dynamic-programming_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>If <span class="math inline">\(N\)</span> is divisible by <span class="math inline">\(4\)</span>, we can dividive and conquer again to get <span class="math inline">\(B_3\)</span>. This is exactly what we see in the original problem for <span class="math inline">\(N = 100\)</span> (see <a href="#fig-gamblers-problem-optimal-sutton" class="quarto-xref">Figure&nbsp;<span>4.3 (a)</span></a>).</p>
<p>Basically, if <span class="math inline">\(N\)</span> is divisible by <span class="math inline">\(2^\ell\)</span> then <span class="math inline">\(B_\ell\)</span> is an optimal stategy. In the limit, this family gives rise to a kind of fractal pattern of stacked diamonds (see <a href="#fig-gamblers-problem-optimal-fractal" class="quarto-xref">Figure&nbsp;<span>4.3 (b)</span></a>) - although these diamonds are missing their bottom tips, which would correspond to wagering 0.</p>
<p>So my finally answer to the inital question is. The family of optimal policies consists of any selection of actions from the bold-strategy hierarchy: <span class="math inline">\(B\)</span> (one big triangle), <span class="math inline">\(B_2\)</span> (two triangles), and <span class="math inline">\(B_3\)</span> (four triangles).</p>
<div id="fig-gamblers-problem-optimal-actions" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gamblers-problem-optimal-actions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gamblers-problem-optimal-actions" style="flex-basis: 33.3%;justify-content: flex-start;">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_greedy_actions(v, env, Œµ):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    optimal_actions <span class="op">=</span> {}</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> env.non_terminal_states:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        q_values <span class="op">=</span> env.one_step_lookaheads(s, v)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        max_value <span class="op">=</span> <span class="bu">max</span>(q_values)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        bests <span class="op">=</span> []</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a, q <span class="kw">in</span> <span class="bu">enumerate</span>(q_values):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> max_value <span class="op">-</span> q <span class="op">&lt;</span> Œµ:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>                bests.append(a <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        optimal_actions[s] <span class="op">=</span> bests</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> optimal_actions</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiPolicy:</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    actions: Dict[<span class="bu">int</span>, List[<span class="bu">int</span>]]  <span class="co"># state -&gt; list of stakes</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    name: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    marker: <span class="bu">str</span> <span class="op">=</span> <span class="st">"o"</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    size: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_multi_policy(multi_policies: List[MultiPolicy], title: <span class="bu">str</span>):</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    draw_legend <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pol <span class="kw">in</span> multi_policies:</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        pts <span class="op">=</span> [(s, a) <span class="cf">for</span> s, al <span class="kw">in</span> pol.actions.items() <span class="cf">for</span> a <span class="kw">in</span> al]</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        states, acts <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>pts)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pol.name:</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            draw_legend <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        plt.scatter(</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            states,</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>            acts,</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span>pol.marker,</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>            s<span class="op">=</span>pol.size,</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span>pol.name,</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"State (Capital)"</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Action (Stake)"</span>)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> draw_legend:</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-8</span>)</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>best_minimal_actions <span class="op">=</span> {}</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> s <span class="kw">in</span> best_actions:</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    best_min <span class="op">=</span> <span class="bu">min</span>(best_actions[s])</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>    best_minimal_actions[s] <span class="op">=</span> [best_min]</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>optimals <span class="op">=</span> [</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    MultiPolicy(best_actions, name<span class="op">=</span><span class="st">"optimal actions"</span>),</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>    MultiPolicy(best_minimal_actions, name<span class="op">=</span><span class="st">"third-order bold strategy"</span>, marker<span class="op">=</span><span class="st">"x"</span>, size<span class="op">=</span><span class="dv">30</span>),</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>plot_multi_policy(optimals, title<span class="op">=</span><span class="st">"Optimal Actions for N=100"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-gamblers-problem-optimal-sutton" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gamblers-problem-optimal-sutton-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="c492073d" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-12-output-1.png" class="lightbox" data-gallery="fig-gamblers-problem-optimal-actions" title="Figure&nbsp;4.3&nbsp;(a): The optimal actions for the gambler‚Äôs problem of example 4.3 of @sutton2018 for p_\mathrm{win} = 0.4 and N = 100. The actions chosen by ‚Äòthird-order bold strategy‚Äô is the optimal policy shown by @sutton2018."><img src="04-dynamic-programming_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-gamblers-problem-optimal-actions"></a></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gamblers-problem-optimal-sutton-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) The optimal actions for the gambler‚Äôs problem of example 4.3 of <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span> for <span class="math inline">\(p_\mathrm{win} = 0.4\)</span> and <span class="math inline">\(N = 100\)</span>. The actions chosen by ‚Äòthird-order bold strategy‚Äô is the optimal policy shown by <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span>.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gamblers-problem-optimal-actions" style="flex-basis: 33.3%;justify-content: flex-start;">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>goal <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">32</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.4</span>, goal<span class="op">=</span>goal)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-14</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-8</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plot_multi_policy([MultiPolicy(best_actions, size<span class="op">=</span><span class="dv">12</span>)], title<span class="op">=</span><span class="ss">f"Optimal actions for N=</span><span class="sc">{</span>goal<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-gamblers-problem-optimal-fractal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gamblers-problem-optimal-fractal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="e28ab78c" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-13-output-1.png" class="lightbox" data-gallery="fig-gamblers-problem-optimal-actions" title="Figure&nbsp;4.3&nbsp;(b): The optimal actions for the gambler‚Äôs problem for N = 5\cdot 32 and p_\mathrm{win} < 0.5. It consists of the bold strategies up to order 6."><img src="04-dynamic-programming_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-gamblers-problem-optimal-actions"></a></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gamblers-problem-optimal-fractal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) The optimal actions for the gambler‚Äôs problem for <span class="math inline">\(N = 5\cdot 32\)</span> and <span class="math inline">\(p_\mathrm{win} &lt; 0.5\)</span>. It consists of the bold strategies up to order <span class="math inline">\(6\)</span>.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gamblers-problem-optimal-actions" style="flex-basis: 33.3%;justify-content: flex-start;">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>goal <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.4</span>, goal<span class="op">=</span>goal)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-8</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plot_multi_policy([MultiPolicy(best_actions)], title<span class="op">=</span><span class="ss">f"Optimal actions for N=</span><span class="sc">{</span>goal<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-gamblers-problem-optimal-odd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gamblers-problem-optimal-odd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="5ca3a2f6" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-14-output-1.png" class="lightbox" data-gallery="fig-gamblers-problem-optimal-actions" title="Figure&nbsp;4.3&nbsp;(c): For N = 101 only bold play is optimal. The same is true for all odd N and p_\mathrm{win} < 0.5."><img src="04-dynamic-programming_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-gamblers-problem-optimal-actions"></a></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gamblers-problem-optimal-odd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) For <span class="math inline">\(N = 101\)</span> only bold play is optimal. The same is true for all odd <span class="math inline">\(N\)</span> and <span class="math inline">\(p_\mathrm{win} &lt; 0.5\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gamblers-problem-optimal-actions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Showcase of all optimal actions for <span class="math inline">\(p &lt; 0.5\)</span> for various <span class="math inline">\(N\)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="gamblers-ruin" class="level4" data-number="4.4.1.6">
<h4 data-number="4.4.1.6" class="anchored" data-anchor-id="gamblers-ruin"><span class="header-section-number">4.4.1.6</span> gambler‚Äôs ruin</h4>
<p>Check out the value function in <a href="#fig-gamblers-problem-value-iteration" class="quarto-xref">Figure&nbsp;<span>4.2 (a)</span></a>. It looks like, even though a single coin flip is stacked against the gambler with <span class="math inline">\(p_\mathrm{win} = 0.4\)</span>, if they start with a large capital, say 80 chips, they still reach the goal of <span class="math inline">\(N = 100\)</span> about 70% of the time. Doesn‚Äôt sound too bad. But of course, if they succeed, they only win 20 chips. If they lose, they lose 80 chips.</p>
<p>One general result that captures this asymmetry is gambler‚Äôs ruin, which states essentially that when the odds are stacked against the gambler, there is no strategy to turn the odds in their favour.</p>
<p>We can make this concrete by calculating the expected monetary return when following the optimal strategy, starting with <span class="math inline">\(s\)</span> chips: <span class="math display">\[
\mathbb{E}_{\pi_*}[S_T - S_0 \mid S_0 = s] = p_{\mathrm{goal}}(N-s) + (1-p_{\mathrm{goal}})(-s),
\]</span></p>
<p>where <span class="math display">\[
p_{\mathrm{goal}}(s) = \mathrm{Pr}_{\pi_*}(S_T = N \mid S_0 = s) = v_*(s).
\]</span></p>
<p>Let‚Äôs compute and plot it for <span class="math inline">\(p = 0.4\)</span> and <span class="math inline">\(N = 100\)</span>:</p>
<div id="30a51f0a" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>goal <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>p_win <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span>p_win, goal<span class="op">=</span>goal)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>expected_profit <span class="op">=</span> [</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    value <span class="op">*</span> (env.goal <span class="op">-</span> s) <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> value) <span class="op">*</span> s <span class="cf">for</span> s, value <span class="kw">in</span> <span class="bu">enumerate</span>(v_star)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(expected_profit)), expected_profit)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Startign Capital"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Expected profit"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>plt.title(</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="vs">r"Profit when following an optimal strategy for </span><span class="dv">$</span><span class="vs">N = 100</span><span class="dv">$</span><span class="vs"> and </span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">4</span><span class="dv">$</span><span class="vs">"</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-15-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="04-dynamic-programming_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Even though the plot shows a nice (fractal-looking) w shape, you‚Äôre expected to lose chips, no matter what your initial capital is.</p>
<p>If you have to play, then the best is to start with either very low or very high capital - this is not financial advice though ü•∏.</p>
<p>As a risk-averse person, my actual advice is: start with <span class="math inline">\(0\)</span> or <span class="math inline">\(100\)</span> chips - i.e., don‚Äôt play at all.</p>
</section>
<section id="gamblers-fortune" class="level4" data-number="4.4.1.7">
<h4 data-number="4.4.1.7" class="anchored" data-anchor-id="gamblers-fortune"><span class="header-section-number">4.4.1.7</span> gambler‚Äôs fortune</h4>
<p>When we stack the game in favour of the gambler <span class="math inline">\(p_{\mathrm{win}} &gt; 0.5\)</span> everything becomes somewhat easier.</p>
<p>There is just one optimal strategy, timid play, that is, always bet exactly one coin <span class="math display">\[
\pi_*(s) = 1.
\]</span></p>
<p>The value function in this case has a clean analytical form: <span class="math display">\[
v(s) = \frac{1 - \left(\frac{1-p}{p}\right)^s}{1 - \left(\frac{1-p}{p}\right)^N}
\]</span></p>
<p>And‚Ä¶ well, that‚Äôs basically it. Winnig all the time doesn‚Äôt require any creative policies.</p>
</section>
<section id="sec-gamblers-problem-floating-points" class="level4" data-number="4.4.1.8">
<h4 data-number="4.4.1.8" class="anchored" data-anchor-id="sec-gamblers-problem-floating-points"><span class="header-section-number">4.4.1.8</span> floating point imprecissions</h4>
<p>There‚Äôs one interesting thing about the gambler‚Äôs fortune case. The high win probabilities can lead to floating point imprecisions.</p>
<p>When we run value iteration with <span class="math inline">\(N = 100\)</span> and <span class="math inline">\(p_\mathrm{win} = 0.6\)</span>, these imprecisions can bleed into the computed optimal policy.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.6</span>, goal<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plot_value_function([(<span class="st">"optimal value function"</span>, v_star)])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-10</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plot_multi_policy([MultiPolicy(best_actions)], title<span class="op">=</span><span class="st">"bla"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="quarto-layout-panel" data-layout="[ 50, 50 ]">
<div class="quarto-layout-row">
<div id="first-column" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="6dc71480" class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-16-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="04-dynamic-programming_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The value function approaches <span class="math inline">\(1\)</span> very quickly. That means from a certain point onward, the <span class="math inline">\(q(s, a)\)</span> values are all indistinguishably close to <span class="math inline">\(1\)</span>‚Ä¶</p>
</div>
<div id="second-column" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="b745063d" class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-17-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="04-dynamic-programming_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>‚Ä¶which leads to floating point imprecisions during optimal action selection. The code ends up including some sub-optimal actions.</p>
</div>
</div>
</div>
<p>Two thoughts on this:</p>
<ol type="1">
<li>Practically speaking, this isn‚Äôt really a problem. When all candidate actions yield a value practically indistinguishable from the max, it doesn‚Äôt matter which one we take. They‚Äôre all effectively optimal.</li>
<li>Theoretically, it‚Äôs a useful reminder: a computation is not a proof. It can return wrong answers. Our algorithms produce approximations of the optimal policy, and here it‚Äôs harmless, but we should keep it in mind.</li>
</ol>
<p>This kind of issue always arises when the <span class="math inline">\(q\)</span>-values are close together. You can also see it with very low win probabilities:</p>
<div id="b3f335f5" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.01</span>, goal<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-8</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plot_multi_policy([MultiPolicy(best_actions)], title<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">01</span><span class="dv">$</span><span class="vs"> and </span><span class="dv">$</span><span class="vs">N=100</span><span class="dv">$</span><span class="vs">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-18-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="04-dynamic-programming_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Note: For illustration, I‚Äôve deliberately used ‚Äòlarge‚Äô <span class="math inline">\(\varepsilon\)</span> values (<span class="math inline">\(10^{-10}\)</span> for <span class="math inline">\(p_\mathrm{win} = 0.6\)</span> and <span class="math inline">\(10^{-8}\)</span> for <span class="math inline">\(p_\mathrm{win} = 0.01\)</span>) when deciding which actions to treat as equally good, that is, if <span class="math inline">\(|q(s,a) - q(s,a')| &lt; \varepsilon\)</span>, we consider both actions equally good.</p>
<div id="exr-4.8" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.8</strong></span> Why does the optimal policy for the gambler‚Äôs problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?</p>
</div>
<div id="sol-4.8" class="proof solution">
<p><span class="proof-title"><em>Solution 4.8</em>. </span>As discussed in <a href="#sec-gamblers-problem-optimal-actions" class="quarto-xref"><span>Section 4.4.1.5</span></a>, there is no single optimal policy.</p>
<p>For capital 50, there‚Äôs only one optimal action - bet it all. This reflects the general strategy of bold play, which intuitively limits the number of steps (and thus the compounding risk) needed to reach the goal.</p>
<p>For capital 51, though, there are two optimal actions: continue being bold (bet 49), or - as shown in the policy from <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span> - just bet 1.</p>
<p>That both are equally good and optimal follows from the maths. But in my opinion, any simple intuitive explanation is just a posteriori justification of the mathematical facts.</p>
</div>
<div id="exr-4.9" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.9</strong></span> Implement value iteration for the gambler‚Äôs problem and solve it for <span class="math inline">\(p_\mathrm{win} = 0.25\)</span> and <span class="math inline">\(p_{\mathrm{win}} = 0.55\)</span>. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital of <span class="math inline">\(0\)</span> and <span class="math inline">\(100\)</span>, giving them values of <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> respectively. Show your results graphically, as in <a href="#fig-gamblers-problem-solution" class="quarto-xref">Figure&nbsp;<span>4.2</span></a>. Are your results stable as <span class="math inline">\(\theta \to 0\)</span>?</p>
</div>
<div id="sol-4.9" class="proof solution">
<p><span class="proof-title"><em>Solution 4.9</em>. </span>Here are the computed solutions - both the state-value functions and the optimal policies. For the policies, we show all optimal actions.</p>
<p>There are no surprises here; everything behaves as previously discussed.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.25</span>, goal<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-10</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plot_value_function([(<span class="st">"optimal value function"</span>, v_star)], title<span class="op">=</span><span class="vs">r"optimal value function for </span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">25</span><span class="dv">$</span><span class="vs"> and </span><span class="dv">$</span><span class="vs">N=100</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plot_multi_policy(</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    [MultiPolicy(best_actions)], title<span class="op">=</span><span class="vs">r"Optimal actions for </span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">25</span><span class="dv">$</span><span class="vs">"</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.55</span>, goal<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-12</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-10</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>plot_value_function([(<span class="st">"optimal value function"</span>, v_star)], title<span class="op">=</span><span class="vs">r"optimal value function for </span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">55</span><span class="dv">$</span><span class="vs"> and </span><span class="dv">$</span><span class="vs">N=100</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plot_multi_policy(</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    [MultiPolicy(best_actions)], title<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">Optimal actions for p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">55</span><span class="dv">$</span><span class="vs">"</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="94ca3ca4" class="cell quarto-layout-panel" data-layout-ncol="2" data-execution_count="19">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-19-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="State value function for p_\mathrm{win} = 0.25 and N = 100"><img src="04-dynamic-programming_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img" alt="State value function for p_\mathrm{win} = 0.25 and N = 100"></a></p>
<figcaption>State value function for <span class="math inline">\(p_\mathrm{win} = 0.25\)</span> and <span class="math inline">\(N = 100\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-19-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Optimal actions for p_\mathrm{win} = 0.25 and N = 100"><img src="04-dynamic-programming_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img" alt="Optimal actions for p_\mathrm{win} = 0.25 and N = 100"></a></p>
<figcaption>Optimal actions for <span class="math inline">\(p_\mathrm{win} = 0.25\)</span> and <span class="math inline">\(N = 100\)</span></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-19-output-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="State value function for p_\mathrm{win} = 0.55 and N = 100"><img src="04-dynamic-programming_files/figure-html/cell-19-output-3.png" class="img-fluid figure-img" alt="State value function for p_\mathrm{win} = 0.55 and N = 100"></a></p>
<figcaption>State value function for <span class="math inline">\(p_\mathrm{win} = 0.55\)</span> and <span class="math inline">\(N = 100\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-19-output-4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Optimal actions for p_\mathrm{win} = 0.55 and N = 100"><img src="04-dynamic-programming_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img" alt="Optimal actions for p_\mathrm{win} = 0.55 and N = 100"></a></p>
<figcaption>Optimal actions for <span class="math inline">\(p_\mathrm{win} = 0.55\)</span> and <span class="math inline">\(N = 100\)</span></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>All used <span class="math inline">\(\theta = 10^{-10}\)</span></p>
</div>
</div>
</div>
<p>These solutions are not stable as <span class="math inline">\(\theta \to 0\)</span> (pushing <span class="math inline">\(\theta &lt; 10^{-18}\)</span> seems to be numerically fragile).</p>
<p>If we go that low, we get residual differences in the values, which can make some optimal actions appear non-optimal:</p>
<div id="f23a2780" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EnvGamblersProblem(p_win<span class="op">=</span><span class="fl">0.25</span>, goal<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>value_functions <span class="op">=</span> value_iteration_gamblers_problem(env, <span class="fl">1e-18</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>v_star <span class="op">=</span> value_functions[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>best_actions <span class="op">=</span> get_greedy_actions(v_star, env, <span class="fl">1e-16</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>plot_multi_policy(</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    [MultiPolicy(best_actions)], title<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">p_</span><span class="dv">\m</span><span class="vs">athrm{win} = 0</span><span class="dv">.</span><span class="vs">25</span><span class="dv">$</span><span class="vs"> and </span><span class="dv">$</span><span class="vs">N=100</span><span class="dv">$</span><span class="vs"> with </span><span class="dv">$</span><span class="ch">\t</span><span class="vs">heta = 10</span><span class="dv">^</span><span class="vs">{-18}"</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="04-dynamic-programming_files/figure-html/cell-20-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="04-dynamic-programming_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>That said, unlike the issues in <a href="#sec-gamblers-problem-floating-points" class="quarto-xref"><span>Section 4.4.1.8</span></a>, this doesn‚Äôt lead to wrong policies - they still contain only optimal actions.</p>
</div>
<div id="exr-4.10" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.10</strong></span> What is the analog of the value iteration update <a href="#eq-value-iteration-update" class="quarto-xref">Equation&nbsp;<span>4.5</span></a> for action values, <span class="math inline">\(q_{k+1}(s, a)\)</span>?</p>
</div>
<div id="solt-4.10">
<p>From the greedy policy update: <span class="math display">\[
\pi_k(s) = \mathrm{argmax}_a q_k(s,a)
\]</span> and the action-value Bellman update: <span class="math display">\[
q_{k+1}(s,a) = \sum_{s',r}p(s',r|s,a) [r + \gamma Q(s', \pi_k(s'))]
\]</span> we get the action-value version of the value iteration update: <span class="math display">\[
q_{k+1}(s, a) = \sum_{s',r} p(s',r|s,a)[r + \gamma \max_{a'} Q(s',a')]
\]</span></p>
</div>
</section>
</section>
</section>
<section id="asynchronous-dynamic-programming" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="asynchronous-dynamic-programming"><span class="header-section-number">4.5</span> Asynchronous Dynamic Programming</h2>
<p>Nothing to add here,</p>
</section>
<section id="generalized-policy-iteration" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="generalized-policy-iteration"><span class="header-section-number">4.6</span> Generalized Policy Iteration</h2>
<p>nor here,</p>
</section>
<section id="efficiency-of-dynamic-programming" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="efficiency-of-dynamic-programming"><span class="header-section-number">4.7</span> Efficiency of Dynamic Programming</h2>
<p>or here,</p>
</section>
<section id="summary" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="summary"><span class="header-section-number">4.8</span> Summary</h2>
<p>and not even here.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-randomservices" class="csl-entry" role="listitem">
Siegrist, Kyle. 2023. <span>‚ÄúProbability, Mathematical Statistics, Stochastic Processes.‚Äù</span> <a href="https://www.randomservices.org/random/index.html" class="uri">https://www.randomservices.org/random/index.html</a>.
</div>
<div id="ref-sutton2018" class="csl-entry" role="listitem">
Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. <a href="https://mitpress.mit.edu/9780262039246/reinforcement-learning/">https://mitpress.mit.edu/9780262039246/reinforcement-learning/</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>They use a continuous version of the gambler‚Äôs problem, but the continous value function <span class="math inline">\(F\)</span> can be translated by <span class="math inline">\(v_\ast(s) = F(\frac{x}{N})\)</span>. That‚Äôs because the optimal strategy, ‚Äòbold play‚Äô, can also be used in the discrete caes.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            trigger: 'click',
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            positionFixed: true,
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/03-finite-markov-decision-processes.html" class="pagination-link" aria-label="Finite Markov Decision Processes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/05-monte-carlo-methods.html" class="pagination-link" aria-label="Monte Carlo Methods">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>