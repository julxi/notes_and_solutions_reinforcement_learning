<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Multi-armed Bandits – Notes on Sutton &amp; Barto</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/03-finite-markov-decision-processes.html" rel="next">
<link href="../chapters/01-intro.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-6fc64a0c1cda8d1c841de64652c337fd.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-6fc64a0c1cda8d1c841de64652c337fd.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fb02f3c877dd64dca3cf6a6e57462b95.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-febd3d0b452fc20adeeab4d20a8c1a88.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const darkModeDefault = authorPrefersDark;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-multi-armed-bandits.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Notes on Sutton &amp; Barto</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/julxi/notes_and_solutions_reinforcement_learning" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-multi-armed-bandits.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-finite-markov-decision-processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-dynamic-programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dynamic Programming</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-k-armed-bandit-problem" id="toc-a-k-armed-bandit-problem" class="nav-link active" data-scroll-target="#a-k-armed-bandit-problem"><span class="header-section-number">2.1</span> A k-armed Bandit Problem</a>
  <ul class="collapse">
  <li><a href="#random-variables-and-probability" id="toc-random-variables-and-probability" class="nav-link" data-scroll-target="#random-variables-and-probability">random variables and probability</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability">conditional probability</a></li>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value">expected value</a></li>
  </ul></li>
  <li><a href="#action-value-methods" id="toc-action-value-methods" class="nav-link" data-scroll-target="#action-value-methods"><span class="header-section-number">2.2</span> Action-value Methods</a></li>
  <li><a href="#the-10-armed-testbed" id="toc-the-10-armed-testbed" class="nav-link" data-scroll-target="#the-10-armed-testbed"><span class="header-section-number">2.3</span> The 10-armed Testbed</a></li>
  <li><a href="#sec-incremental-implementation" id="toc-sec-incremental-implementation" class="nav-link" data-scroll-target="#sec-incremental-implementation"><span class="header-section-number">2.4</span> Incremental Implementation</a></li>
  <li><a href="#tracking-a-nonstationary-problem" id="toc-tracking-a-nonstationary-problem" class="nav-link" data-scroll-target="#tracking-a-nonstationary-problem"><span class="header-section-number">2.5</span> Tracking a Nonstationary Problem</a></li>
  <li><a href="#optimistic-initial-values" id="toc-optimistic-initial-values" class="nav-link" data-scroll-target="#optimistic-initial-values"><span class="header-section-number">2.6</span> Optimistic Initial Values</a></li>
  <li><a href="#upper-confidence-bound-action-selection" id="toc-upper-confidence-bound-action-selection" class="nav-link" data-scroll-target="#upper-confidence-bound-action-selection"><span class="header-section-number">2.7</span> Upper-Confidence-Bound Action Selection</a></li>
  <li><a href="#gradient-bandit-algorithms" id="toc-gradient-bandit-algorithms" class="nav-link" data-scroll-target="#gradient-bandit-algorithms"><span class="header-section-number">2.8</span> Gradient Bandit Algorithms</a>
  <ul class="collapse">
  <li><a href="#sec-derivation-bandit-gradient-update-formula" id="toc-sec-derivation-bandit-gradient-update-formula" class="nav-link" data-scroll-target="#sec-derivation-bandit-gradient-update-formula"><span class="header-section-number">2.8.1</span> the bandit gradient algorithm as stochastic gradient ascent</a></li>
  </ul></li>
  <li><a href="#associative-search-contextual-bandits" id="toc-associative-search-contextual-bandits" class="nav-link" data-scroll-target="#associative-search-contextual-bandits"><span class="header-section-number">2.9</span> Associative Search (Contextual Bandits)</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.10</span> Summary</a></li>
  <li><a href="#sec-appendix-multi-arm-bandits" id="toc-sec-appendix-multi-arm-bandits" class="nav-link" data-scroll-target="#sec-appendix-multi-arm-bandits"><span class="header-section-number">2.11</span> appendix</a>
  <ul class="collapse">
  <li><a href="#distribution" id="toc-distribution" class="nav-link" data-scroll-target="#distribution"><span class="header-section-number">2.11.1</span> distribution</a></li>
  <li><a href="#sec-iid" id="toc-sec-iid" class="nav-link" data-scroll-target="#sec-iid"><span class="header-section-number">2.11.2</span> independent and identically distributed random variables</a></li>
  <li><a href="#lotus" id="toc-lotus" class="nav-link" data-scroll-target="#lotus"><span class="header-section-number">2.11.3</span> lotus</a></li>
  <li><a href="#multiplication-rule-of-conditional-probabilities" id="toc-multiplication-rule-of-conditional-probabilities" class="nav-link" data-scroll-target="#multiplication-rule-of-conditional-probabilities"><span class="header-section-number">2.11.4</span> multiplication rule of conditional probabilities</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="header-section-number">2.11.5</span> variance</a></li>
  <li><a href="#independent-variables" id="toc-independent-variables" class="nav-link" data-scroll-target="#independent-variables"><span class="header-section-number">2.11.6</span> independent variables</a></li>
  <li><a href="#estimators" id="toc-estimators" class="nav-link" data-scroll-target="#estimators"><span class="header-section-number">2.11.7</span> estimators</a></li>
  <li><a href="#sec-geometric-series" id="toc-sec-geometric-series" class="nav-link" data-scroll-target="#sec-geometric-series"><span class="header-section-number">2.11.8</span> geometric series</a></li>
  <li><a href="#sec-recurrence-relations" id="toc-sec-recurrence-relations" class="nav-link" data-scroll-target="#sec-recurrence-relations"><span class="header-section-number">2.11.9</span> recurrence relations</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>(Some imports for python, can be ignored)</p>
<div id="24369fda" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, field</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Sequence, Tuple</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="a-k-armed-bandit-problem" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-k-armed-bandit-problem"><span class="header-section-number">2.1</span> A k-armed Bandit Problem</h2>
<p>The k-armed bandit problem is a very simple example that already introduces a key structure of reinforcement learning: the reward depends on the action taken. It’s technically not a full Markov chain (more on that in <a href="03-finite-markov-decision-processes.html#sec-the-agent-environment-interface" class="quarto-xref"><span>Section 3.1</span></a>)—since there are no states or transitions—but we still get a sequence of dependent random variables <span class="math inline">\(A_1, R_1, A_2, \dots\)</span>, where each reward <span class="math inline">\(R_t\)</span> depends on the corresponding action <span class="math inline">\(A_t\)</span>.</p>
<p>The true value of an action is defined as: <span class="math display">\[
q_*(a) := \mathbb{E}[ R_t \mid A_t = a].
\]</span></p>
<p>The time index here doesn’t play a special role as the action-reward probabilities in the armed bandit are stationary. You can think of it as “when <span class="math inline">\(a\)</span> is picked”—that is, the expected reward when action <span class="math inline">\(a\)</span> is chosen.</p>
<p>If you’re feeling queasy about the conditional expected value, here’s a quick refresher on the relevant notation and concepts.</p>
<section id="random-variables-and-probability" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="random-variables-and-probability">random variables and probability</h3>
<p>The foundations of all processes we discuss here are discrete probability spaces <span class="math inline">\((\Omega, \mathrm{Pr})\)</span>. <!-- todo: can I be more precise about these processes? We need armed bandits + MDPs --> <span class="math inline">\(\Omega\)</span> is the set of all possible trajectories—that is, complete sequences of outcomes for the random variables in a single run of the process—and <span class="math inline">\(\mathrm{Pr}\)</span> assigns a probability to each trajectory. That is, <span class="math display">\[
\mathrm{Pr}\colon \Omega \to [0,1] \quad \text{with} \quad
\sum_{\omega \in \Omega} \mathrm{Pr}(\omega) = 1.
\]</span></p>
<p>The random variables <span class="math inline">\(X\)</span> are simply functions <span class="math inline">\(X\colon \Omega \to \mathcal{X}\)</span> from <span class="math inline">\(\Omega\)</span> to a result space <span class="math inline">\(\mathcal{X}\)</span>.</p>
<p>We follow the convention of <span class="citation" data-cites="sutton2018">Sutton and Barto (<a href="#ref-sutton2018" role="doc-biblioref">2018</a>)</span>: random variables are written in capital letters, and their possible values are in lowercase.</p>
<p>If we want to refer to the concrete outcome of a single trajectory (which we actually don’t often do in theory crafting), we evaluate random variables on a specific <span class="math inline">\(\omega \in \Omega\)</span>, which fixes their values. <!-- I don't like "apply" here. It's more the chosen ω sets the values of the random variables --> So an arbitrary trajectory looks like this <span class="math inline">\(A_1(\omega), R_1(\omega), A_2(\omega), R_2(\omega) \dots\)</span></p>
<p>Let’s bring up two common conventions we can find in <span class="math inline">\(\mathbb{E}[ R_t \mid A_t = a]\)</span>:</p>
<ol type="1">
<li>We usually omit the argument <span class="math inline">\(\omega\)</span> when referring to the value of a random variable. This is what makes the randomness implicit in a random variable <span class="math inline">\(X\)</span>. This mathematically conflates the function <span class="math inline">\(X ⁣\colon\Omega\to \mathcal{X}\)</span> with the value <span class="math inline">\(X(\omega)\)</span>, but context sorts that out.</li>
<li>When writing functions of sets, we abbreviate expressions like <span class="math inline">\(F({\omega \in \Omega : \text{statement true in }\omega})\)</span> to simply <span class="math inline">\(F(\text{statement})\)</span>.</li>
</ol>
<p>With both conventions in play, we can see that <span class="math inline">\(\mathrm{Pr}(X = x)\)</span> is just shorthand for <span class="math inline">\(\mathrm{Pr}(\omega \in \Omega : X(\omega) = x)\)</span></p>
</section>
<section id="conditional-probability" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="conditional-probability">conditional probability</h3>
<p>The reward <span class="math inline">\(R_t\)</span>​ depends on the action <span class="math inline">\(A_t\)</span>​ taken. If we know the value of <span class="math inline">\(A_t\)</span>, then the conditional probability that <span class="math inline">\(R_t=r\)</span> given <span class="math inline">\(A_t = a\)</span> is: <span class="math display">\[
\mathrm{Pr}(R_t = r \mid A_t = a) = \frac{\mathrm{Pr}(R_t = r, A_t = a)}{\mathrm{Pr}(A_t = a)},
\]</span></p>
<p>where the comma denotes conjunction of the statements.</p>
<p>This is only well-defined if <span class="math inline">\(\mathrm{Pr}(A_t = a) &gt; 0\)</span> but that’s a technicality we won’t worry too much about—it won’t bite us.</p>
</section>
<section id="expected-value" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="expected-value">expected value</h3>
<p>Real-valued random variables like <span class="math inline">\(R_t\colon \Omega \to \mathbb{R}\)</span> have an expected value—also called the mean—<span class="math inline">\(\mathbb{E}[R_t]\)</span>, defined as: <span class="math display">\[
\mathbb{E}[ R_t ] := \sum_{\omega \in \Omega} R_t(\omega) \mathrm{Pr}(\omega)
\]</span></p>
<p>A more commonly used form—sometimes called the “law of the unconscious statistician” (see appendix <a href="#thm-law-of-the-unconscious-statistician" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a>)—is: <span class="math display">\[
\mathbb{E}[R_t] = \sum_{r \in \mathcal{R}} r \; \mathrm{Pr}(R_t = r),
\]</span></p>
<p>To compute a conditional expectation, we just switch probabilities with conditional probabilities: <span class="math display">\[
\mathbb{E}[R_t \mid A_t = a] = \sum_{\omega \in \Omega} R_t(\omega) \mathrm{Pr}(\omega \mid A_t = a).
\]</span></p>
<p>Or, using the more practical LOTUS form: <span class="math display">\[
\mathbb{E}[R_t \mid A_t = a] = \sum_{r \in \mathcal{R}} r \; \mathrm{Pr}(R_t = r \mid A_t = a).
\]</span></p>
<p>To close the loop, a more explicit formulation of the true value of an action is: <span class="math display">\[
q_*(a) = \sum_{r \in \mathcal{R}} r \; \frac{\mathrm{Pr}(R_t = r, A_t = a)}{\mathrm{Pr}(A_t = a)}
\]</span> (This form is arguably less intuitive, but it’s included here to jog our memory.)</p>
</section>
</section>
<section id="action-value-methods" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="action-value-methods"><span class="header-section-number">2.2</span> Action-value Methods</h2>
<p>This part always trips me up, so let me clarify it for myself: <span class="math inline">\(Q_t(a)\)</span> is the estimated value of action <span class="math inline">\(a\)</span> <em>prior</em> to time <span class="math inline">\(t\)</span>, so not included are <span class="math inline">\(A_t\)</span> and it’s corresponding reward <span class="math inline">\(R_t\)</span>.</p>
<p>Instead, <span class="math inline">\(A_t\)</span> is selected based on the current estimates <span class="math inline">\(\{Q_{t}(a):a \in \mathcal{A}\}\)</span>. For example, our algorithm could pick <span class="math inline">\(A_t\)</span>​ greedily as <span class="math inline">\(A_t:=\mathrm{argmax}_{a \in \mathcal{A}} Q_t(a)\)</span>, or <span class="math inline">\(\varepsilon\)</span>-greedily.</p>
<div id="exr-2.1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1</strong></span> In <span class="math inline">\(\varepsilon\)</span>-greedy action selection, for the case of two actions and <span class="math inline">\(\varepsilon = 0.5\)</span>, what is the probability that the greedy action is selected?</p>
</div>
<div id="sol-2.1" class="proof solution">
<p><span class="proof-title"><em>Solution 2.1</em>. </span>The total probability of selecting the greedy action is: <span class="math display">\[
\mathrm{Pr}(\text{greedy action}) + \mathrm{Pr}(\text{exploratory action}) \cdot \frac{1}{2} = 0.75
\]</span></p>
</div>
</section>
<section id="the-10-armed-testbed" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="the-10-armed-testbed"><span class="header-section-number">2.3</span> The 10-armed Testbed</h2>
<p>The 10-armed testbed will accompany us through the rest of this chapter (I had to keep it variable in size just for the sake of generalization though).</p>
<div id="c78407c1" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the armed bandit ===</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ArmedBandit:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""k-armed Gaussian bandit."""</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, action_mu, action_sd, seed):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_mu <span class="op">=</span> np.asarray(action_mu, dtype<span class="op">=</span>np.float64)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_sd <span class="op">=</span> np.asarray(action_sd, dtype<span class="op">=</span>np.float64)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.seed <span class="op">=</span> seed</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(<span class="va">self</span>.seed)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pull_arm(<span class="va">self</span>, action):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.rng.normal(loc<span class="op">=</span><span class="va">self</span>.action_mu[action], scale<span class="op">=</span><span class="va">self</span>.action_sd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And here’s the code for the sample-average bandit algorithm. For clarity, I’ll refer to this and upcoming algorithms as ‘agents’, given their autonomous implementation. Note that we’re also using the incremental implementation from section 2.4.</p>
<div id="3755aae8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the simple average bandit agent ===</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SampleAverageBanditAgent:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, Q1, ε, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_actions <span class="op">=</span> <span class="bu">len</span>(Q1) </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q1 <span class="op">=</span> np.asarray(Q1, dtype<span class="op">=</span>np.float64)  <span class="co"># initial action-value estimates</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ε <span class="op">=</span> ε  </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> <span class="va">self</span>.Q1.copy()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts <span class="op">=</span> np.zeros(<span class="va">self</span>.num_actions, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, bandit):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ε-greedy action selection</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.rng.random() <span class="op">&lt;</span> <span class="va">self</span>.ε:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.rng.integers(<span class="va">self</span>.num_actions)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> np.argmax(<span class="va">self</span>.Q)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take action and observe the reward</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> bandit.pull_arm(action)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update count and value estimate</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts[action] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        α <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="va">self</span>.counts[action]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[action] <span class="op">+=</span> α <span class="op">*</span> (reward <span class="op">-</span> <span class="va">self</span>.Q[action])</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (action, reward)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, I’ll define an experiment function <code>bandit_experiment</code> that we’ll use throughout this chapter. In short, it takes multiple agents, length of each episode, and the number of episodes, then executes these agents repeatedly in a shared bandit environment, which gets reset after each episode. It returns two arrays: the average reward per step and the percentage of optimal actions taken per step—both averaged over all runs. These results can then be visualised using the plotting functions also provided. You don’t have to read the code unless you’re curious…</p>
<div id="ef2cdafc" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the core bandit experiment ===</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --- config</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Config:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    bandit_num_arms: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    bandit_setup_mu: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    bandit_setup_sd: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    bandit_action_sd: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    bandit_value_drift: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    bandit_value_drift_mu: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    bandit_value_drift_sd: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    exp_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    exp_runs: <span class="bu">int</span> <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    exp_seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># -- core experiment</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bandit_experiment(agents, config: Config) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray]:</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Run `exp_runs` × `exp_steps` episodes and return:</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">    average_rewards         shape = (len(agents), exp_steps)</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">    optimal_action_percent  shape = (len(agents), exp_steps)</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.default_rng(config.exp_seed)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    num_agents <span class="op">=</span> <span class="bu">len</span>(agents)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    average_rwds <span class="op">=</span> np.zeros((num_agents, config.exp_steps))</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    optimal_acts <span class="op">=</span> np.zeros((num_agents, config.exp_steps))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># allocate a single bandit and reuse its object shell</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    bandit <span class="op">=</span> ArmedBandit(</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        action_mu<span class="op">=</span>np.empty(config.bandit_num_arms),  <span class="co"># placeholder</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        action_sd<span class="op">=</span>config.bandit_action_sd,</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        seed<span class="op">=</span>config.exp_seed,</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> run <span class="kw">in</span> <span class="bu">range</span>(config.exp_runs):</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fresh true values for this run</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        bandit.action_mu[:] <span class="op">=</span> rng.normal(</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>            config.bandit_setup_mu, config.bandit_setup_sd, size<span class="op">=</span>config.bandit_num_arms</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        best_action <span class="op">=</span> np.argmax(bandit.action_mu)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reset all agents</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> agent <span class="kw">in</span> agents:</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>            agent.reset()</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># vectorised drift noise: shape = (exp_steps, bandit_num_arms)</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> config.bandit_value_drift:</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            drift_noise <span class="op">=</span> rng.normal(</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>                config.bandit_value_drift_mu,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                config.bandit_value_drift_sd,</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                size<span class="op">=</span>(config.exp_steps, config.bandit_num_arms),</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># main loop</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(config.exp_steps):</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents):</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>                act, rwd <span class="op">=</span> agent.act(bandit)</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>                average_rwds[i, t] <span class="op">+=</span> rwd</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>                optimal_acts[i, t] <span class="op">+=</span> act <span class="op">==</span> best_action</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> config.bandit_value_drift:</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>                bandit.action_mu <span class="op">+=</span> drift_noise[t]</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>                best_action <span class="op">=</span> np.argmax(bandit.action_mu)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mean over runs</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    average_rwds <span class="op">/=</span> config.exp_runs</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    optimal_acts <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> optimal_acts <span class="op">/</span> config.exp_runs</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> average_rwds, optimal_acts</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="co"># --- thin plotting helpers</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_average_reward(</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    average_rewards: np.ndarray,</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    <span class="op">*</span>,</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>    labels: Sequence[<span class="bu">str</span>] <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>    ax: plt.Axes <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> plt.Axes:</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One line per agent: average reward versus step."""</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>        _, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    steps  <span class="op">=</span> np.arange(<span class="dv">1</span>, average_rewards.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="ss">f"agent </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(average_rewards.shape[<span class="dv">0</span>])]</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, lbl <span class="kw">in</span> <span class="bu">enumerate</span>(labels):</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>        ax.plot(steps, average_rewards[i], label<span class="op">=</span>lbl)</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"Step"</span>)</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"Average reward"</span>)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">"Average reward per step"</span>)</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_optimal_action_percent(</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>    optimal_action_percents: np.ndarray,</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>    <span class="op">*</span>,</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>    labels: Sequence[<span class="bu">str</span>] <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>    ax: plt.Axes <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> plt.Axes:</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One line per agent: % optimal action versus step."""</span></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>        _, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    steps  <span class="op">=</span> np.arange(<span class="dv">1</span>, optimal_action_percents.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="ss">f"agent </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(optimal_action_percents.shape[<span class="dv">0</span>])]</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, lbl <span class="kw">in</span> <span class="bu">enumerate</span>(labels):</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>        ax.plot(steps, optimal_action_percents[i], label<span class="op">=</span>lbl)</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"Step"</span>)</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"</span><span class="sc">% o</span><span class="st">ptimal action"</span>)</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">"Optimal-action frequency"</span>)</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>    ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>…, but it’s still helpful to see such an experiment in action. We can, for example, recreate Figure 2.2 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>. It compares the performance of a greedy agent (<span class="math inline">\(\varepsilon = 0\)</span>), and two <span class="math inline">\(\varepsilon\)</span>-greedy agents with <span class="math inline">\(\varepsilon =0.1\)</span> and <span class="math inline">\(\varepsilon=0.01\)</span>. We let them run for a couple of steps and then repeat this process for a couple of runs to get a smoother curve.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === comparison greediness ===</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># configuration of the experiment</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">2_000</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize agents with different epsilon values</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>epsilons <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [SampleAverageBanditAgent(Q1<span class="op">=</span>np.zeros(config.bandit_num_arms), ε<span class="op">=</span>ε, seed<span class="op">=</span>config.exp_seed) <span class="cf">for</span> ε <span class="kw">in</span> epsilons]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># run bandit experiment</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    config    </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plots</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="ss">f"ε=</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> e <span class="kw">in</span> epsilons]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plot_average_reward(avg_rwd, labels<span class="op">=</span>labels)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>labels)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-bandit-comparison-greediness" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="6">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bandit-comparison-greediness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-bandit-comparison-greediness-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bandit-comparison-greediness-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="02-multi-armed-bandits_files/figure-html/fig-bandit-comparison-greediness-output-1.png" class="lightbox" data-gallery="fig-bandit-comparison-greediness" title="Figure&nbsp;2.1&nbsp;(a): The average reward."><img src="02-multi-armed-bandits_files/figure-html/fig-bandit-comparison-greediness-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-bandit-comparison-greediness"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bandit-comparison-greediness-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) The average reward.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-bandit-comparison-greediness-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bandit-comparison-greediness-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="02-multi-armed-bandits_files/figure-html/fig-bandit-comparison-greediness-output-2.png" class="lightbox" data-gallery="fig-bandit-comparison-greediness" title="Figure&nbsp;2.1&nbsp;(b): The percentage of optimal step selection"><img src="02-multi-armed-bandits_files/figure-html/fig-bandit-comparison-greediness-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-bandit-comparison-greediness"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bandit-comparison-greediness-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) The percentage of optimal step selection
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bandit-comparison-greediness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: This is like Figure 2.2 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>: the average performance of different ε-greedy sample average methods over 2000 runs. On the 10-armed testbed.
</figcaption>
</figure>
</div>
</div>
<p>Out of curiosity, let’s see what happens when we have only one run (so it’s not the average anymore but just the reward). It’s a mess, wow. Without averaging over a couple of runs, we can’t make out anything.</p>
<div id="e98a37ef" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === experiment with only one run ===</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>epsilons <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [SampleAverageBanditAgent(Q1<span class="op">=</span>np.zeros(config.bandit_num_arms), ε<span class="op">=</span>ε, seed<span class="op">=</span>config.exp_seed) <span class="cf">for</span> ε <span class="kw">in</span> epsilons]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plot_average_reward(avg_rwd, labels<span class="op">=</span>[<span class="ss">f"ε=</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> e <span class="kw">in</span> epsilons])</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-7-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="02-multi-armed-bandits_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="exr-2.2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2 (Bandit example)</strong></span> Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using <span class="math inline">\(\varepsilon\)</span>-greedy action selection, sample-average action-value estimates, and initial estimates of <span class="math inline">\(Q_1(a) = 0\)</span>, for all a. Suppose the initial sequence of actions and rewards is <span class="math inline">\(A_1 = 1\)</span>, <span class="math inline">\(R_1 = -1\)</span>, <span class="math inline">\(A_2 = 2\)</span>, <span class="math inline">\(R_2 = 1\)</span>, <span class="math inline">\(A_3 = 2\)</span>, <span class="math inline">\(R_3 = -2\)</span>, <span class="math inline">\(A_4 = 2\)</span>, <span class="math inline">\(R_4 = 2\)</span>, <span class="math inline">\(A_5 = 3\)</span>, <span class="math inline">\(R_5 = 0\)</span>. On some of these time steps the <span class="math inline">\(\varepsilon\)</span> case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?</p>
</div>
<div id="sol-2.2" class="proof solution">
<p><span class="proof-title"><em>Solution 2.2</em>. </span>Step 1 could have been exploratory, as all actions have the same estimates. After that, the value function is: <span class="math display">\[
Q_2(a) = \begin{cases}
            -1,&amp; \text{if $a = 1$}\\
            0,&amp; \text{otherwise}
         \end{cases}
\]</span></p>
<p>Also, step 2 could have been exploratory. Now the value function is: <span class="math display">\[
Q_3(a) = \begin{cases}
            -1,&amp; \text{if $a = 1$}\\
            1,&amp; \text{if $a = 2$}\\
            0,&amp; \text{otherwise}
         \end{cases}
\]</span></p>
<p>In step 3, the greedy action is taken. But it could also have been an exploratory action that selected 2. At this point, the value function is: <span class="math display">\[
Q_4(a) = \begin{cases}
            -1,&amp; \text{if $a = 1$}\\
            -0.5,&amp; \text{if $a = 2$}\\
            0,&amp; \text{otherwise}
         \end{cases}
\]</span></p>
<p>In step 4, a non-greedy action is taken, so this must have been an exploratory move. The value function is: <span class="math display">\[
Q_5(a) = \begin{cases}
            -1,&amp; \text{if $a = 1$}\\
            0.33,&amp; \text{if $a = 2$}\\
            0,&amp; \text{otherwise}
         \end{cases}
\]</span></p>
<p>In step 5, again a non-greedy action was taken, so this must have been an exploratory move as well.</p>
</div>
<div id="exr-2.3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.3</strong></span> In the comparison shown in <a href="#fig-bandit-comparison-greediness" class="quarto-xref">Figure&nbsp;<span>2.1</span></a>, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.</p>
</div>
<div id="sol-2.3" class="proof solution">
<p><span class="proof-title"><em>Solution 2.3</em>. </span>Obviously, we can disregard <span class="math inline">\(\varepsilon = 0\)</span>. It’s just rubbish. Before we do the quantitative analysis, let’s see what happens when we just crank up the number of steps (and reduce the runs even though now it’s a bit noisier).</p>
<div id="f2adf529" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === battle between ε=0.1 and ε=0.01 ===</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">15_000</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>epsilons <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.01</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [SampleAverageBanditAgent(Q1<span class="op">=</span>np.zeros(config.bandit_num_arms), ε<span class="op">=</span>ε, seed<span class="op">=</span>config.exp_seed) <span class="cf">for</span> ε <span class="kw">in</span> epsilons]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>plot_average_reward(avg_rwd, labels<span class="op">=</span>[<span class="ss">f"ε=</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> e <span class="kw">in</span> epsilons])</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>[<span class="ss">f"ε=</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> e <span class="kw">in</span> epsilons])</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-8-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="02-multi-armed-bandits_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-8-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="02-multi-armed-bandits_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>We can see that <span class="math inline">\(\varepsilon=0.01\)</span> outperforms <span class="math inline">\(\varepsilon=0.1\)</span> in average reward around step <span class="math inline">\(2000\)</span>. However, achieving a higher percentage of optimal actions takes more than <span class="math inline">\(10,000\)</span> steps. It’s actually quite interesting that achieving a higher percentage of optimal actions takes significantly longer.</p>
<p>Now, let’s consider the long-term behaviour. In the limit, we can assume both methods have near-perfect <span class="math inline">\(Q\)</span>-values and the only reason they select non-optimal actions is due to their <span class="math inline">\(\varepsilon\)</span>-softness.</p>
<p>This makes calculating the optimal action probability quite easy. <span class="math display">\[
\mathrm{Pr}(\text{optimal action}) = (1-\varepsilon) + \varepsilon \frac{1}{10} = 1 - 0.9 \varepsilon
\]</span></p>
<p>So for <span class="math inline">\(\varepsilon=0.1\)</span> this probability is <span class="math inline">\(0.91\)</span>, and for <span class="math inline">\(\varepsilon=0.01\)</span> this is <span class="math inline">\(0.991\)</span>.</p>
<p>Now the average reward is trickier to compute. It can be done, but it’s quite messy and we’re here to learn reinforcement learning so we don’t need to figure out perfect analytical solutions anymore. Luckily, we get this value directly from the book</p>
<blockquote class="blockquote">
<p>It [greedy algorithm] achieved a reward-per-step of only about <span class="math inline">\(1\)</span>, compared with the best possible of about <span class="math inline">\(1.55\)</span> on this testbed <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018, 29</a>)</span>.</p>
</blockquote>
<p>Great—they’ve done the work for us. Selecting the optimal action gives an average reward of <span class="math inline">\(1.55\)</span>. Selecting a random action has an average reward of <span class="math inline">\(0\)</span> because it’s basically drawing a sample from a normal distribution with mean <span class="math inline">\(0\)</span>. That gives:</p>
<p><span class="math display">\[
\mathbb{E}[R_t] = (1-\varepsilon) 1.55 + \varepsilon 0 = 1.55 (1-\varepsilon)
\]</span></p>
<p>This results in <span class="math inline">\(1.40\)</span> for <span class="math inline">\(\varepsilon = 0.1\)</span> and <span class="math inline">\(1.53\)</span> for <span class="math inline">\(\varepsilon = 0.01\)</span>.</p>
</div>
</section>
<section id="sec-incremental-implementation" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-incremental-implementation"><span class="header-section-number">2.4</span> Incremental Implementation</h2>
<p>The sample average can be updated incrementally using: <span id="eq-bandit-sample-average-update"><span class="math display">\[
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n].
\tag{2.1}\]</span></span></p>
<p>This is an instance of a general pattern that is central to reinforcement learning: <span class="math display">\[
\text{NewEstimate} \gets \text{OldEstimate} + \text{StepSize}
\Big[\overbrace{
    \text{Target} - \text{OldEstimate}
    }^\text{error} \Big]
\]</span></p>
<p>I especially like how nice it looks in python. In value-based algorithms, this typically corresponds to the following line of code:</p>
<div id="c3a6f27d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Q[action] <span class="op">+=</span> α <span class="op">*</span> (reward <span class="op">-</span> Q[action])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tracking-a-nonstationary-problem" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="tracking-a-nonstationary-problem"><span class="header-section-number">2.5</span> Tracking a Nonstationary Problem</h2>
<p>To avoid the learning rate decreasing over time—at the cost of convergence—we can use a constant step size <span class="math inline">\(\alpha \in (0,1]\)</span>. <span id="eq-bandit-constant-step-size-update"><span class="math display">\[
Q_{n+1} := Q_n + \alpha \Big[ R_n - Q_n \Big],
\tag{2.2}\]</span></span></p>
<p>for <span class="math inline">\(n \geq 1\)</span> and <span class="math inline">\(Q_1\)</span> is our given initial estimate.</p>
<p>This can be phrased as a recurrence relation of the form <span class="math inline">\(Q_n = \sum_{i=0}^n \gamma^{n-i} r_i\)</span>, as discussed in the appendix <a href="#sec-recurrence-relations" class="quarto-xref"><span>Section 2.11.9</span></a>: We add <span class="math inline">\(Q_0\)</span> and <span class="math inline">\(R_0\)</span>. Then, <a href="#eq-bandit-constant-step-size-update" class="quarto-xref">Equation&nbsp;<span>2.2</span></a> is equivalent to: <span id="eq-bandit-action-values-convolution"><span class="math display">\[
Q_{n+1} = \alpha R_n + (1 - \alpha) Q_n \quad \text{and} \quad Q_0 = 0,
\tag{2.3}\]</span></span></p>
<p>This is a recurrence relation of the form <a href="#eq-non-homogenous-geometric-series-recurrence-relation" class="quarto-xref">Equation&nbsp;<span>2.18</span></a> and thus has the explicit form <span class="math display">\[
Q_{n+1} = \sum_{i=0}^n (1 - \alpha)^{n-i} \alpha R_{i}
\]</span></p>
<p>Substituting <span class="math inline">\(R_0 = \frac{Q_1}{\alpha}\)</span>—so that <span class="math inline">\(Q_1\)</span> becomes our arbitrary initial value—yields the form used by Sutton and Barto: <span id="eq-weighted-average"><span class="math display">\[
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} R_i
\tag{2.4}\]</span></span></p>
<p>This is a weighted average of the random variables involved, as the weights sum to 1. The sum of the weights for the <span class="math inline">\(R_i\)</span> is (using the geometric series identity <a href="#eq-finite-geometric-series" class="quarto-xref">Equation&nbsp;<span>2.14</span></a>): <span class="math display">\[
\begin{split}
\sum_{i=1}^n \alpha (1- \alpha)^{n-i} &amp;= \alpha \sum_{i=o}^{n-1}(1-\alpha)^i \\
&amp;= \alpha \frac{1 - (1-\alpha)^n}{\alpha}\\
&amp;= 1 - (1 - \alpha)^n.
\end{split}
\]</span></p>
<p>Thus, the total weight sums to 1: <span class="math display">\[
\begin{split}
(1-\alpha)^n + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} &amp;= 1
\end{split}
\]</span></p>
<p>The <span class="math inline">\(Q_n\)</span> are estimators for the true action value <span class="math inline">\(q_*\)</span>. And depending on how we determine the <span class="math inline">\(Q_n\)</span>, they have different qualities. We note that the <span class="math inline">\(R_i\)</span> are IID with mean <span class="math inline">\(q_*\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. (I refer to the appendix <a href="#sec-appendix-multi-arm-bandits" class="quarto-xref"><span>Section 2.11</span></a> for more information about all the new terms appearing all of a sudden, as this has gotten quite a bit more technical)</p>
<p>If <span class="math inline">\(Q_n\)</span>​ is the sample average, the estimator is unbiased, that is <span class="math display">\[
\mathbb{E}[Q_n] = q_* \quad \text{for all } n \in \mathbb{N}.
\]</span> Which is easy to show. Its mean squared error <span class="math inline">\(\mathrm{MSE}(Q_n) := \mathbb{E}[(Q_n - q_*)^2]\)</span> is decreasing (<a href="#lem-mse-sample-average" class="quarto-xref">Lemma&nbsp;<span>2.4</span></a>): <span class="math display">\[
\mathrm{MSE}(Q_n) = \frac{\sigma^2}{n}.
\]</span></p>
<p>If the <span class="math inline">\(Q_n\)</span> are calculated using a constant step size, they are biased (there is always a small dependency on <span class="math inline">\(Q_1\)</span>): <span id="eq-mean-of-constant-step-size-average"><span class="math display">\[
\begin{split}
\mathbb{E}[Q_{n+1}] &amp;=  (1-\alpha)^n Q_1 + q\sum_{i=1}^n \alpha (1 - \alpha)^{n-i}   \\
&amp;= (1-\alpha)^n Q_1 + q (1 - (1 - \alpha)^n)
\end{split}
\tag{2.5}\]</span></span></p>
<p>And even though they are asymptotically unbiased, i.e., <span class="math inline">\(\lim_{n\to\infty} \mathbb{E}[Q_{n}] = q\)</span>, their mean squared error is bounded away from zero (<a href="#lem-mse-constant-step-size" class="quarto-xref">Lemma&nbsp;<span>2.5</span></a>): <span class="math display">\[
\mathrm{MSE}(Q_n) &gt; \sigma^2 \frac{\alpha}{2-\alpha}.
\]</span></p>
<p>It’s hard for me to translate these stochastic results in statistic behaviour, but I think that means that constant step size will end up wiggling around the true value.</p>
<p>Let’s define the constant step size agent to compare it with the sample average method later.</p>
<div id="7bcdd4f2" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the constant step bandit agent ===</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConstantStepBanditAgent:</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, Q1, α, ε, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_actions <span class="op">=</span> <span class="bu">len</span>(Q1)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q1 <span class="op">=</span> Q1</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.α <span class="op">=</span> α</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ε <span class="op">=</span> ε</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> <span class="va">self</span>.Q1.copy()</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, bandit):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ε-greedy action selection</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.rng.random() <span class="op">&lt;</span> <span class="va">self</span>.ε:</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.rng.integers(<span class="va">self</span>.num_actions)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> np.argmax(<span class="va">self</span>.Q)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take action</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> bandit.pull_arm(action)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update value estimate</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[action] <span class="op">+=</span> <span class="va">self</span>.α <span class="op">*</span> (reward <span class="op">-</span> <span class="va">self</span>.Q[action])</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (action, reward)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="exr-2.4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.4</strong></span> If the step-size parameters, <span class="math inline">\(\alpha_n\)</span>, are not constant, then the estimate <span class="math inline">\(Q_n\)</span> is a weighted average of previously received rewards with a weighting different from that given by <a href="#eq-weighted-average" class="quarto-xref">Equation&nbsp;<span>2.4</span></a>. What is the weighting on each prior reward for the general case, analogous to <a href="#eq-weighted-average" class="quarto-xref">Equation&nbsp;<span>2.4</span></a>, in terms of the sequence of step-size parameters.</p>
</div>
<div id="sol-2.4" class="proof solution">
<p><span class="proof-title"><em>Solution 2.4</em>. </span>The update rule for non-constant step size has <span class="math inline">\(\alpha_n\)</span> depending on the step. <span id="eq-non-constant-stepsize"><span class="math display">\[
Q_{n+1} = Q_n + \alpha_n \Big[ R_n - Q_n \Big]
\tag{2.6}\]</span></span></p>
<p>In this case the weighted average is given by <span id="eq-general-weighted-average"><span class="math display">\[
Q_{n+1} = \left( \prod_{j=1}^n 1-\alpha_j \right) Q_1 + \sum_{i=1}^n \alpha_i \left( \prod_{j=i+1}^n 1 - \alpha_j \right) R_i
\tag{2.7}\]</span></span></p>
<p>This explicit form can be verified inductively. For <span class="math inline">\(n=0\)</span>, we get <span class="math inline">\(Q_1\)</span> on both sides.</p>
<p>For the induction step we have <span class="math display">\[
\begin{split}
Q_{n+1} &amp;= Q_n + \alpha_n \Big[ R_n - Q_n \Big] \\
&amp;= \alpha_n R_n + (1 - \alpha_n) Q_n \\
&amp;= \alpha_n R_n + (1 - \alpha_n) \Big[ \left( \prod_{j=1}^{n-1} 1-\alpha_j \right) Q_1 + \sum_{i=1}^{n-1} \alpha_i \left( \prod_{j=i+1}^{n-1} 1 - \alpha_j \right) R_i \Big] \\
&amp;= \left( \prod_{j=1}^n 1-\alpha_j \right) Q_1 + \sum_{i=1}^n \alpha_i \left( \prod_{j=i+1}^n 1 - \alpha_j \right) R_i
\end{split}
\]</span></p>
<p>We also note that in this general setting, <a href="#eq-general-weighted-average" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> is still a weighted average. We could prove it by induction or use a little trick. If we set <span class="math inline">\(Q_1 = 1\)</span> and <span class="math inline">\(R_n = 1\)</span> for all <span class="math inline">\(n\)</span> in the recurrence relation <a href="#eq-non-constant-stepsize" class="quarto-xref">Equation&nbsp;<span>2.6</span></a> we see that each <span class="math inline">\(Q_n = 1\)</span>. If we do the same in the explicit formula <a href="#eq-general-weighted-average" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> we see that each <span class="math inline">\(Q_n\)</span> is equal to the sum of the weights. Therefore, the weights sum up to <span class="math inline">\(1\)</span>.</p>
</div>
<div id="exr-2.5" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.5</strong></span> Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the <span class="math inline">\(q_\star(a)\)</span> start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the <span class="math inline">\(q_\star(a)\)</span> on each step). Prepare plots like Figure <a href="#fig-bandit-comparison-greediness" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, <span class="math inline">\(\alpha = 0.1\)</span>. Use <span class="math inline">\(\epsilon = 0.1\)</span> and longer runs, say of 10,000 steps</p>
</div>
<div id="sol-2.5" class="proof solution">
<p><span class="proof-title"><em>Solution 2.5</em>. </span>Alright, let’s do a little experiment, just as they told us.</p>
<div id="0f341e8a" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === battle between sample average and constant step ===</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    bandit_setup_mu<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    bandit_setup_sd<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    bandit_value_drift<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    bandit_value_drift_mu<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    bandit_value_drift_sd<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">10_000</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>agent0 <span class="op">=</span> SampleAverageBanditAgent(Q1<span class="op">=</span>np.zeros(config.bandit_num_arms, dtype<span class="op">=</span><span class="bu">float</span>), ε<span class="op">=</span><span class="fl">0.1</span>, seed<span class="op">=</span>config.exp_seed)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>agent1 <span class="op">=</span> ConstantStepBanditAgent(</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    Q1<span class="op">=</span>np.zeros(config.bandit_num_arms, dtype<span class="op">=</span><span class="bu">float</span>), α<span class="op">=</span><span class="fl">0.1</span>, ε<span class="op">=</span><span class="fl">0.1</span>, seed<span class="op">=</span>config.exp_seed</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [agent0, agent1]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">"sample averages (ε=0.1)"</span>, <span class="st">"constant step-size (α=0.1, ε=0.1)"</span>]</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>plot_average_reward(avg_rwd, labels<span class="op">=</span>labels)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>labels)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="02-multi-armed-bandits_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-11-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="02-multi-armed-bandits_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Not surprisingly, we can see how much the sample average agent struggles to keep up. For longer episode lengths, the problem only gets worse. Eventually, it will be completely out of touch with the world, like an old man unable to keep up with the times.</p>
<p>However, it remains unclear exactly how rapidly the sample average method will deteriorate to an unacceptable level. The results from the final exercise of this chapter (<a href="#exr-2.11" class="quarto-xref">Exercise&nbsp;<span>2.11</span></a>) indicate that this deterioration may take a significant number of steps.</p>
</div>
</section>
<section id="optimistic-initial-values" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="optimistic-initial-values"><span class="header-section-number">2.6</span> Optimistic Initial Values</h2>
<p>We later need the following figure comparing an optimistic greedy agent and a realistic <span class="math inline">\(\varepsilon\)</span>-greedy agent.</p>
<div id="cell-fig-bandit-comparison-optimism" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === realism vs optimism ===</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">1_000</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>agent_optimistic_greedy <span class="op">=</span> ConstantStepBanditAgent(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    Q1<span class="op">=</span>np.full(config.bandit_num_arms, <span class="fl">5.0</span>, dtype<span class="op">=</span><span class="bu">float</span>), α<span class="op">=</span><span class="fl">0.1</span>, ε<span class="op">=</span><span class="fl">0.0</span>, seed<span class="op">=</span>config.exp_seed</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>agent_realistic_ε_greedy <span class="op">=</span> ConstantStepBanditAgent(</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    Q1<span class="op">=</span>np.zeros(config.bandit_num_arms, dtype<span class="op">=</span><span class="bu">float</span>), α<span class="op">=</span><span class="fl">0.1</span>, ε<span class="op">=</span><span class="fl">0.1</span>, seed<span class="op">=</span>config.exp_seed</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [agent_optimistic_greedy, agent_realistic_ε_greedy]</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optimistic,greedy (Q1=5, ε=0, α=0.1)"</span>,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)"</span>,</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>labels)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-bandit-comparison-optimism" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bandit-comparison-optimism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="02-multi-armed-bandits_files/figure-html/fig-bandit-comparison-optimism-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;2.2: This is like Figure 2.3 [@sutton2018]: the effect of optimistic initial action-value estimates."><img src="02-multi-armed-bandits_files/figure-html/fig-bandit-comparison-optimism-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bandit-comparison-optimism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: This is like Figure 2.3 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>: the effect of optimistic initial action-value estimates.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exr-2.6" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.6 (Mysterious Spikes)</strong></span> The results shown in <a href="#fig-bandit-comparison-optimism" class="quarto-xref">Figure&nbsp;<span>2.2</span></a> should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?</p>
</div>
<div id="sol-2.6" class="proof solution">
<p><span class="proof-title"><em>Solution 2.6</em>. </span>We have the hard-earned luxury that we can zoom in on <a href="#fig-bandit-comparison-optimism" class="quarto-xref">Figure&nbsp;<span>2.2</span></a>:</p>
<div id="8ef29c11" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === realism vs optimism zoomed in ===</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">5_000</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>agent_optimistic_greedy <span class="op">=</span> ConstantStepBanditAgent(</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    Q1<span class="op">=</span>np.full(config.bandit_num_arms, <span class="fl">5.0</span>, dtype<span class="op">=</span><span class="bu">float</span>), α<span class="op">=</span><span class="fl">0.1</span>, ε<span class="op">=</span><span class="fl">0.0</span>, seed<span class="op">=</span>config.exp_seed</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>agent_realistic_ε_greedy <span class="op">=</span> ConstantStepBanditAgent(</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    Q1<span class="op">=</span>np.zeros(config.bandit_num_arms, dtype<span class="op">=</span><span class="bu">float</span>), α<span class="op">=</span><span class="fl">0.1</span>, ε<span class="op">=</span><span class="fl">0.1</span>, seed<span class="op">=</span>config.exp_seed</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [agent_optimistic_greedy, agent_realistic_ε_greedy]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optimistic,greedy (Q1=5, ε=0, α=0.1)"</span>,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)"</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>labels)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="02-multi-armed-bandits_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The spike occurs at step 11. Essentially, the optimistic method samples all actions once (poor performance), and then selects the action with the best result (good performance, with a success rate of over 40%). However, regardless of the outcome (which likely pales in comparison to the current Q-values, which are still likely greater than 4), the method returns to exploring all 10 actions again. This leads to poor performance once more. Around step 22, there is another spike for similar reasons, but this time smaller and more spread out.</p>
</div>
<div id="exr-2.7" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.7 (Unbiased Constant-Step-Size Trick)</strong></span> In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of onstant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of <span class="math display">\[
\beta_n := \alpha / \bar{o}_n
\]</span> to process the <span class="math inline">\(n\)</span>-th reward for a particular action, where <span class="math inline">\(\alpha &gt; 0\)</span> is a conventional constant step size, and <span class="math inline">\(\bar{o}_n\)</span> is a trace of one that starts at <span class="math inline">\(0\)</span>: <span class="math display">\[
\bar{o}_n := \bar{o}_{n-1} + \alpha (1 - \bar{o}_{n-1}), \text{ for } n \geq 1, \text{ with } \bar{o}_0 := 0
\]</span> Carry out an analyises like that in <a href="#eq-weighted-average" class="quarto-xref">Equation&nbsp;<span>2.4</span></a> to show that <span class="math inline">\(Q_n\)</span> is an exponential recency-weighted average without initial bias.</p>
</div>
<div id="sol-2.7" class="proof solution">
<p><span class="proof-title"><em>Solution 2.7</em>. </span>My first question when I saw this was, “What’s up with that strange name <span class="math inline">\(\bar{o}_n\)</span>?”” I guess it could be something like “the weighted average of ones”, maybe? Well, whatever. Let’s crack on.</p>
<p>When we rewrite <span class="math inline">\(\bar{o}_{n+1}\)</span> as a recurrence relation for <span class="math inline">\(\frac{\bar{o}_{n+1}}{\alpha}\)</span> <span class="math display">\[
\frac{\bar{o}_{n+1}}{\alpha} = 1 + (1 - \alpha) \frac{\bar{o}_n}{\alpha}
\]</span> we see that it is just the recurrence relation for a geometric series as in <a href="#eq-geometric-series-recurrence-relation" class="quarto-xref">Equation&nbsp;<span>2.13</span></a> for <span class="math inline">\(\gamma = 1-\alpha\)</span>. Thus we get <span class="math display">\[
\bar{o}_n = \alpha\sum_{i=0}^{n-1} (1 - \alpha)^{i} = 1 - (1-\alpha)^n
\]</span> and <span class="math inline">\(\beta_n = \frac{\alpha}{1-(1-\alpha)^n}\)</span></p>
<p>(btw. this is such a complicated way to define <span class="math inline">\(\beta_n\)</span> and I don’t understand why actually.)</p>
<p>In particular we have that <span class="math inline">\(\beta_1 = 1\)</span>, which makes the influence of <span class="math inline">\(Q_1\)</span> disappears after the first reward <span class="math inline">\(R_1\)</span> is received: <span class="math inline">\(Q_2 = Q_1 + 1 [ R_1 - Q_1] = R_1\)</span>. Great!</p>
<p>Scaling the <span class="math inline">\(\alpha\)</span> by the <span class="math inline">\(\bar{o}_n\)</span> has an additional nicer effect. I don’t quite understand how, but we can calculate it.</p>
<p>From <a href="#exr-2.4" class="quarto-xref">Exercise&nbsp;<span>2.4</span></a> we know <span class="math display">\[
Q_{n+1} = Q_1 \prod_{j=1}^n (1-\beta_j)
+ \sum_{i=1}^n  R_i \beta_i \prod_{j=i+1}^n (1- \beta_j )
\]</span></p>
<p>There is a nice form for these products <span class="math display">\[
\prod_{j=i}^n (1 - \beta_j) = (1-\alpha)^{n-j+1} \frac{\bar{o}_{i-1}}{\bar{o}_n}
\]</span></p>
<p>since they are telescoping using <span class="math display">\[
\begin{split}
1- \beta_j &amp;= 1 - \frac{\alpha}{\bar{o}_j} = \frac{\bar{o}_j - \alpha}{\bar{o}_j}\\
&amp;= \frac{\alpha + (1-\alpha) \bar{o}_{j-1}}{\bar{o}_j} = (1-\alpha)\frac{\bar{o}_{j-1}}{\bar{o}_j}.
\end{split}
\]</span></p>
<p>This gives the following closed form for <span class="math inline">\(Q_{n+1}\)</span> <span class="math display">\[
Q_{n+1} = \frac{\alpha}{1 - (1-\alpha)^n}\sum_{i=1}^n R_i (1-\alpha)^{n-i}
\]</span></p>
<p>We can see that the weight given to any reward <span class="math inline">\(R_i\)</span>​ decreases exponentially.</p>
</div>
</section>
<section id="upper-confidence-bound-action-selection" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="upper-confidence-bound-action-selection"><span class="header-section-number">2.7</span> Upper-Confidence-Bound Action Selection</h2>
<p>We have the opportunity to introduce a new agent here. The update rule remains the same (I assume sample average), but the action selection is more informed compared to <span class="math inline">\(\varepsilon\)</span>-greedy algorithms.</p>
<p><span class="math display">\[
A_t := \mathrm{argmax}_a \left[ Q_t(a) + c \sqrt{ \frac{\ln t}{N_t(a)} }\right]
\]</span></p>
<p>where <span class="math inline">\(N_t(a)\)</span> is the number of times that action has been selected, c &gt; 0 controls the exploration (similar to <span class="math inline">\(\varepsilon\)</span>).</p>
<p>If an action has not been selected even once, i.e., <span class="math inline">\(N_t(a)=0\)</span>, then <span class="math inline">\(a\)</span> is considered to be a maximizing action. (In our case, this means that in the first few steps, all actions have to be selected once, and only after that does the UCB-based action selection kick in.)</p>
<p>By the way, I have no idea where the UCB formulation comes from, but at least it looks fancy (and reasonable), and we can implement it:</p>
<div id="86aaec36" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the ucb bandit agent ===</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UcbBanditAgent:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_actions, c, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_actions <span class="op">=</span> num_actions</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c <span class="op">=</span> c  <span class="co"># exploration parameter</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.zeros(<span class="va">self</span>.num_actions, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts <span class="op">=</span> np.zeros(<span class="va">self</span>.num_actions, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, bandit):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># upper-Confidence-Bound Action Selection</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t <span class="op">&lt;=</span> <span class="va">self</span>.num_actions:</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if not all actions have been tried yet, select an untried action</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>._choose_untaken_action()</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate UCB values for each action</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            ucb_values <span class="op">=</span> <span class="va">self</span>.Q <span class="op">+</span> <span class="va">self</span>.c <span class="op">*</span> np.sqrt(np.log(<span class="va">self</span>.t) <span class="op">/</span> (<span class="va">self</span>.counts))</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># select the action with the highest UCB value</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> np.argmax(ucb_values)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take action and observe the reward</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> bandit.pull_arm(action)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update count and value estimate</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counts[action] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[action] <span class="op">+=</span> (reward <span class="op">-</span> <span class="va">self</span>.Q[action]) <span class="op">/</span> <span class="va">self</span>.counts[action]</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (action, reward)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _choose_untaken_action(<span class="va">self</span>):</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.rng.choice(np.where(<span class="va">self</span>.counts <span class="op">==</span> <span class="dv">0</span>)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s recreate the figure illustrating UCB action selection performance, which we’ll need for the next exercise.</p>
<div id="cell-fig-bandit-ucb-performance" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === ucb agent performance ===</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>agent_ucb <span class="op">=</span> UcbBanditAgent(num_actions<span class="op">=</span>config.bandit_num_arms, c<span class="op">=</span><span class="dv">2</span>, seed<span class="op">=</span>config.exp_seed)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>agent_ε_greedy <span class="op">=</span> SampleAverageBanditAgent(</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    Q1<span class="op">=</span>np.zeros(config.bandit_num_arms, dtype<span class="op">=</span><span class="bu">float</span>), ε<span class="op">=</span><span class="fl">0.1</span>, seed<span class="op">=</span>config.exp_seed</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [agent_ucb, agent_ε_greedy]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ucb (c=2, α=0.1)"</span>,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ε-greedy (ε=0.1, α=0.1)"</span>,</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>labels)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-bandit-ucb-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bandit-ucb-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="02-multi-armed-bandits_files/figure-html/fig-bandit-ucb-performance-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;2.3: This is like Figure 2.4 [@sutton2018]: average performance of UCB action selection."><img src="02-multi-armed-bandits_files/figure-html/fig-bandit-ucb-performance-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bandit-ucb-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: This is like Figure 2.4 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>: average performance of UCB action selection.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exr-2.8" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.8 (UCB Spikes)</strong></span> In <a href="#fig-bandit-ucb-performance" class="quarto-xref">Figure&nbsp;<span>2.3</span></a> the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if <span class="math inline">\(c = 1\)</span>, then the spike is less prominent.</p>
</div>
<div id="sol-2.8" class="proof solution">
<p><span class="proof-title"><em>Solution 2.8</em>. </span>I think the answer is similar to the answer to Exercise 2.6. The first <span class="math inline">\(10\)</span> steps the UCB algorithm tries out all actions. Then on step <span class="math inline">\(11\)</span> it will select the one that scored highest, which is quite a decent strategy. But then because of the <span class="math inline">\(N_t​(a)\)</span> in the denominator it is back to exploring.</p>
</div>
</section>
<section id="gradient-bandit-algorithms" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="gradient-bandit-algorithms"><span class="header-section-number">2.8</span> Gradient Bandit Algorithms</h2>
<p>This introduces a novel method that is not value-based; instead, it directly aims to select the best actions. The agent maintains numerical preferences, denoted by <span class="math inline">\(H_t​(a)\)</span>, rather than estimates of the action values.</p>
<p>For action selection, gradient bandit uses the softmax distribution: <span id="eq-soft-max-distribution"><span class="math display">\[
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}.
\tag{2.8}\]</span></span></p>
<p>Shifting all preferences by a constant <span class="math inline">\(C\)</span> doesn’t affect <span class="math inline">\(\pi_t(a)\)</span>: <span class="math display">\[
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \frac{e^Ce^{H_t(a)}}{\sum_{b=1}^k e^Ce^{H_t(b)}} = \frac{e^{H_t(a)+C}}{\sum_{b=1}^k e^{H_t(b)+C}}
\]</span></p>
<p>For learning, gradient bandit uses the following rule: <span id="eq-gradient-bandit-update"><span class="math display">\[
\begin{split}
H_{t+1}(a) &amp;:= H_t(a) + \alpha (R_t - \bar{R}_t) (\mathbb{I}_{a = A_t} - \pi_t(a))
\end{split}
\tag{2.9}\]</span></span></p>
<p>Now, let’s implement this gradient bandit algorithm:</p>
<div id="00af9bf8" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the gradient agent ===</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradientBanditAgent:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, H1, α, baseline<span class="op">=</span><span class="va">True</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_actions <span class="op">=</span> <span class="bu">len</span>(H1) </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.α <span class="op">=</span> α </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H1 <span class="op">=</span> np.asarray(H1, dtype<span class="op">=</span>np.float64)  <span class="co"># initial preferences</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.baseline <span class="op">=</span> baseline <span class="co"># apply average reward baseline</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">=</span> <span class="va">self</span>.H1.copy()  </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avg_reward <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">=</span> <span class="dv">0</span>  <span class="co"># step count</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, bandit):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># select action using softmax</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        action_probs <span class="op">=</span> GradientBanditAgent.softmax(<span class="va">self</span>.H)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> <span class="va">self</span>.rng.choice(<span class="va">self</span>.num_actions, p<span class="op">=</span>action_probs)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take action and observe the reward</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> bandit.pull_arm(action)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update average reward</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.baseline:</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.avg_reward <span class="op">+=</span> (reward <span class="op">-</span> <span class="va">self</span>.avg_reward) <span class="op">/</span> <span class="va">self</span>.t</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update action preferences</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        advantage <span class="op">=</span> reward <span class="op">-</span> <span class="va">self</span>.avg_reward <span class="co"># avg_reward = 0 if baseline = false</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        one_hot_action <span class="op">=</span> np.eye(<span class="va">self</span>.num_actions)[action]</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">+=</span> <span class="va">self</span>.α <span class="op">*</span> advantage <span class="op">*</span> (one_hot_action <span class="op">-</span> action_probs)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> action, reward</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(x):</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shift vector by max(x) to avoid hughe numbers.</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is basically using the fact that softmax(x) = softmax(x + C)</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>        exp_x <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x))</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> exp_x <span class="op">/</span> np.<span class="bu">sum</span>(exp_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Sutton and Barto <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018, 37</a>)</span> emphasize the importance of the baseline <span class="math inline">\(\bar{R}_t\)</span>​ in the update forumla and show that performance drops without it. In the derivation of the update as a form of stochastic gradient ascent, the baseline can be chosen arbitrarily (see <a href="#sec-derivation-bandit-gradient-update-formula" class="quarto-xref"><span>Section 2.8.1</span></a>). Whether or not a baseline is used, the resulting updates are unbiased estimators of the gradient. I assume, the baseline serves to reduce the variance of the estimator, although I have no idea about the maths behind it.</p>
<p>Here we recreat Figure 2.5 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>, which shows how drastically the running average baseline can improve performance.</p>
<div id="cell-fig-gradient-bandit-performance" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === gradient bandit performance ===</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> Config(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    exp_steps<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    exp_runs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    bandit_setup_mu<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.4</span>]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>agents <span class="op">=</span> [GradientBanditAgent(H1<span class="op">=</span>np.zeros(config.bandit_num_arms), α<span class="op">=</span>α, seed<span class="op">=</span>config.exp_seed) <span class="cf">for</span> α <span class="kw">in</span> alphas] <span class="op">+</span> [GradientBanditAgent(H1<span class="op">=</span>np.zeros(config.bandit_num_arms), α<span class="op">=</span>α, baseline<span class="op">=</span><span class="va">False</span>, seed<span class="op">=</span>config.exp_seed) <span class="cf">for</span> α <span class="kw">in</span> alphas]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>avg_rwd, opt_pct <span class="op">=</span> bandit_experiment(</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    agents,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    config</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="ss">f"α = </span><span class="sc">{α}</span><span class="ss">, with baseline"</span> <span class="cf">for</span> α <span class="kw">in</span> alphas] <span class="op">+</span> [<span class="ss">f"α = </span><span class="sc">{α}</span><span class="ss">, without baseline"</span> <span class="cf">for</span> α <span class="kw">in</span> alphas]</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>plot_optimal_action_percent(opt_pct, labels<span class="op">=</span>labels)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-gradient-bandit-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-bandit-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="02-multi-armed-bandits_files/figure-html/fig-gradient-bandit-performance-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;2.4: This is like Figure 2.5 [@sutton2018]: average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the q_*(a) are chosen to be near +4 rather than near zero. (we averaged over 50 runs because it gives this cool jaggedy looking graph)"><img src="02-multi-armed-bandits_files/figure-html/fig-gradient-bandit-performance-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-bandit-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: This is like Figure 2.5 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>: average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the <span class="math inline">\(q_*(a)\)</span> are chosen to be near <span class="math inline">\(+4\)</span> rather than near zero. (we averaged over 50 runs because it gives this cool jaggedy looking graph)
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exr-2.9" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.9</strong></span> Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.</p>
</div>
<div id="sol-2.9" class="proof solution">
<p><span class="proof-title"><em>Solution 2.9</em>. </span>The logistic function is defined as <span class="math display">\[
\sigma(x) := \frac{1}{1 + e^{-x}} = \frac{e^x}{1+e^x}.
\]</span> If we map the two preferences <span class="math inline">\(H(a_1), H(a_2)\)</span> to a single value <span class="math inline">\(\Delta = H(a_1) - H(a_2)\)</span> then <span class="math display">\[
\pi(a_1) = \frac{e^{H(a_1)}}{e^{H(a_1)} + e^{H(a_2)}} = \frac{e^{H(a_1)-H(a_2)}}{e^{H(a_1)-H(a_2)} + 1} = \sigma(\Delta)
\]</span> and similarly <span class="math display">\[
\pi(a_2) = \sigma(-\Delta).
\]</span></p>
</div>
<section id="sec-derivation-bandit-gradient-update-formula" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="sec-derivation-bandit-gradient-update-formula"><span class="header-section-number">2.8.1</span> the bandit gradient algorithm as stochastic gradient ascent</h3>
<p>This next subsection is devoted to the (I imagine) infamous brown box <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018, 38–40</a>)</span>, which marks a significant leap in theoretical complexity. It shows how the update rule arises as a form of stochastic gradient ascent.</p>
<p>We’ll retrace their steps as a self-contained argument and flag two subtle points: the role of the baseline and the randomness in the preference vector.</p>
<section id="quick-recap-of-gradient-ascent" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="quick-recap-of-gradient-ascent">1. quick recap of gradient ascent</h4>
<p>Let <span class="math inline">\(f \colon \mathbb{R}^n \to \mathbb{R}\)</span> be a differentiable function. We want to produce points <span class="math inline">\(\mathbf{x}_0, \mathbf{x}_1, \dots\)</span> that maximise <span class="math inline">\(f\)</span>. Gradient ascent updates the current point <span class="math inline">\(\mathbf{x}^{(t)}\)</span> in the direction of the gradient: <span class="math display">\[
\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} + \alpha \; \nabla f(\mathbf{x})\big|_{\mathbf{x}^{(t)}}
\]</span> where <span class="math inline">\(\alpha &gt; 0\)</span> is the step size.</p>
<p>In one dimension, this becomes: <span class="math display">\[
x_i^{(t+1)} = x_i^{(t)} + \alpha \; \frac{\partial f(\mathbf{x})}{\partial x_i}\bigg|_{x_i^{(t)}}
\]</span></p>
<p>In stochastic gradient ascent, we aim to maximise the expected value of a random vector <span class="math inline">\(\mathbf{R}\)</span> (a vector whose values are random variables), whose distribution depends on a parameter vector <span class="math inline">\(\mathbf{x}\)</span>. That is, the underlying probability space <span class="math inline">\((\Omega, \mathrm{Pr}_{\mathbf{x}})\)</span> is parameterised by <span class="math inline">\(\mathbf{x}\)</span>. Here <span class="math inline">\(f\)</span> is <span class="math inline">\(\mathbb{E}_{\mathbf{x}}[\mathbf{R}]\)</span> where we have explicitly indicated the dependence of the expected value on <span class="math inline">\(\mathbf{x}\)</span>. So this is still a deterministic gradient ascent step—although the true gradient is unknown to the algorithm and must later be estimated via sampling: <span class="math display">\[
\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} + \alpha \cdot \nabla \mathbb{E}_{\mathbf{x}}[\mathbf{R}]\big|_{\mathbf{x}^{(t)}}
\]</span></p>
<p>Our goal is to cast the gradient bandit update in this framework.</p>
</section>
<section id="setting-up-the-problem" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="setting-up-the-problem">2. setting up the problem</h4>
<p>In the gradient bandit algorithm, the parameters we adjust are the action preferences <span class="math inline">\(H_t​(a)\)</span>. These determine the policy via the softmax distribution: <span class="math display">\[
\pi_H(a) = \frac{e^{H(a)}}{\sum_{b \in \mathcal{A}} e^{H(b)}}
\]</span></p>
<p>This shows how our parameter <span class="math inline">\(H\)</span> determines the probability space: it determines the probability distribution for <span class="math inline">\(A_t\)</span> <span class="math inline">\(\pi_{H_t}(a) := \mathrm{Pr}_{H_t}(A_t = a)\)</span> the probabilities for rewards given an action are determined by the system and independent of the parameters <span class="math inline">\(q_*(a) := \mathbb{E}[R_t \mid A_t = a]\)</span>.</p>
<p>With this set-up, it is clear how <span class="math inline">\(\mathbb{E}_{H_t}[R_t]\)</span> is a function on <span class="math inline">\(H_t\)</span> <span class="math display">\[
\begin{split}
\mathbb{E}_{H_t}[R_t] &amp;= \sum_{b} \mathrm{Pr}_{H_t}(A_t = b) \cdot \mathbb{E}[R_t \mid A_t = b] \\
&amp;= \sum_{b} \pi_{H_t}(b) q_*(b)
\end{split}
\]</span> (if you are unsure about this, check <a href="#thm-law-of-total-expectation" class="quarto-xref">Theorem&nbsp;<span>2.4</span></a>).</p>
<p>In this context, each action <span class="math inline">\(a\in\mathcal{A}\)</span> corresponds to a coordinate in our parameter vector <span class="math inline">\(H\)</span>, so the gradient update in one dimension <span class="math inline">\(a \in \mathcal{A}\)</span> becomes: <span id="eq-gradient-bandit-problem-formulation"><span class="math display">\[
H_{t+1}(a) = H_t(a) + \alpha \frac{\partial \mathbf{E}[R_t]}{\partial H(a)}\bigg|_{H_t(a)}
\tag{2.10}\]</span></span></p>
</section>
<section id="calculating-the-gradient" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="calculating-the-gradient">3. calculating the gradient</h4>
<p>Now look at the row of the gradient for <span class="math inline">\(a \in \mathcal{A}\)</span>: <span class="math display">\[
\begin{split}
\frac{\partial \mathbb{E}_{H}[R_t]}{\partial H(a)}\Bigg|_{H_t(a)} &amp;=
\sum_{b} q_*(b) \cdot \frac{\partial \pi_{H}(b)}{\partial H(a)}\Bigg|_{H_t(a)} \\
&amp;= \sum_{b} (q_*(b) - B_t) \cdot \frac{\partial \pi_{H}(b)}{\partial H(a)}\Bigg|_{H_t(a)}
\end{split}
\]</span> We could add here any scalar (called the baseline) as <span class="math inline">\(\sum_b \pi_H(b) = 1\)</span> and thus <span class="math inline">\(\sum_{b} \frac{\partial \pi_{H}(b)}{\partial H(a)}\Big|_{H'(a)} = 0\)</span>. Note that for this argument to work <span class="math inline">\(B_t\)</span> cannot depend on <span class="math inline">\(b\)</span>.</p>
<p>To simplify that further we use the softmax derivative, (which is derived in Sutton and Barto): <span class="math display">\[
\frac{\partial \pi_{H}(b)}{\partial H(a)}\Bigg|_{H_t(a)}
= \pi_{H_t}(b) (\mathbb{I}_{a = b} - \pi_{H_t}(a))
\]</span></p>
<p>So we have <span class="math display">\[
\begin{split}
\frac{\partial \mathbb{E}_{H}[R_t]}{\partial H(a)}\Bigg|_{H_t(a)} &amp;=
\sum_{b} (q_*(b) - B_t) \cdot  (\mathbb{I}_{a = b} - \pi_{H_t}(a)) \pi_{H_t}(b) \\
&amp;= \mathbb{E}_{H_t}[(q_*(A_t)- B_t) (\mathbb{I}_{a = A_t} - \pi_{H_t(a)})] \\
&amp;= \mathbb{E}_{H_t}[ (R_t - B_t) (\mathbb{I}_{a = A_t} - \pi_{H_t(a)})].
\end{split}
\]</span> We get the second equality by applying the law of the unconscious statistician in reverse (<a href="#thm-law-of-the-unconscious-statistician" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a>), and yes the expression inside the expectation is all just a deterministic function of <span class="math inline">\(A_t\)</span>. In the final equality, we substituted <span class="math inline">\(R_t\)</span> for <span class="math inline">\(q_*(A_t)\)</span> using that <span class="math inline">\(q_*(A_t) = \mathbb{E}_{H_t}[R_t]\)</span>, and the law of iterated expectations justifies the substitution under the outer expectation.</p>
<p>Before going to the next step, you might want to check the plausibility of <span id="eq-bandit-gradient-explicit-form"><span class="math display">\[
\frac{\partial \mathbb{E}[R_t]}{\partial H(a)}\Bigg|_{H_t(a)}
= \mathbb{E}_{H_t}[ (R_t - B_t) (\mathbb{I}_{a = A_t} - \pi_{H_t(a)})].
\tag{2.11}\]</span></span> On the left we have basically a number, the value of the derivative at some point, and on the right we have an expected value with a parameter <span class="math inline">\(B_t\)</span>. So the parameter <span class="math inline">\(B_t\)</span> must cancel out somehow. Which it does indeed, you can check this for yourself.</p>
</section>
<section id="one-step-update-and-baseline-trade-off" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="one-step-update-and-baseline-trade-off">4. one-step update and baseline trade-off</h4>
<p>By plugging <a href="#eq-bandit-gradient-explicit-form" class="quarto-xref">Equation&nbsp;<span>2.11</span></a> into <a href="#eq-gradient-bandit-problem-formulation" class="quarto-xref">Equation&nbsp;<span>2.10</span></a> we get the deterministic gradient ascent update: <span class="math display">\[
H_{t+1}(a) = H_t(a) + \alpha \mathbb{E}[ (R_t - B_t) (\mathbb{I}_{a = A_t} - \pi_{H_t(a)})]
\]</span></p>
<p>Replacing the expectation by the single sample <span class="math inline">\(A_t, R_t\)</span> yields the stochastic gradient update: <span id="eq-gradient-bandit-update-with-constant-baseline"><span class="math display">\[
H_{t+1}(a) = H_t(a) + \alpha  (R_t - B_t) (\mathbb{I}_{a = A_t} - \pi_{H_t(a)})
\tag{2.12}\]</span></span></p>
<p>To get <a href="#eq-gradient-bandit-update" class="quarto-xref">Equation&nbsp;<span>2.9</span></a> we have to substitute <span class="math inline">\(\bar{R}_t\)</span> for <span class="math inline">\(B_t\)</span>. Which requires some discussion first.</p>
<p>Sutton and Barto make clear that <span class="math inline">\(\bar{R}_t\)</span> depend on <span class="math inline">\(R_t\)</span>:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\bar{R}_t\)</span> is the average of all the rewards up through and including time <span class="math inline">\(t\)</span> <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018, 37</a>)</span>.</p>
</blockquote>
<p>If we use that in <a href="#eq-gradient-bandit-update-with-constant-baseline" class="quarto-xref">Equation&nbsp;<span>2.12</span></a> for <span class="math inline">\(B_t\)</span> we are not using an unbiased estimator anymore. This is tightly coupled with the fact that when we introduced <span class="math inline">\(B_t\)</span> we required it not to depend on <span class="math inline">\(b\)</span> which does later play the role of <span class="math inline">\(A_t\)</span>, and <span class="math inline">\(\bar{R}_t\)</span> depends on <span class="math inline">\(R_t\)</span> which depends on <span class="math inline">\(A_t\)</span>.</p>
<p>Mostl likely there is a good reason for using <span class="math inline">\(\bar{R}_t\)</span>, but I don’t know the mathematical motivation. (As I said, maybe some variance reduction)</p>
<p>However I’m saying the derivation of their update formula is wrong<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> as they frame it as an unbiased estimator.</p>
</section>
<section id="comment-on-the-time-parameter" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="comment-on-the-time-parameter">comment on the time parameter</h4>
<p>We have keept the time parameter in the derivation to stick to the style of Sutton and Barto. We could have equally done the same derivation for <span class="math inline">\(H'\)</span> (new) and <span class="math inline">\(H\)</span> (old). However, conceptually keeping the time parameter is a bit shaky. Where does the <span class="math inline">\(H_t\)</span> come from? If we think this through then <span class="math inline">\(H_t\)</span> actually becomes part of the whole system (enviroment + agent) and thus is a random vector. And then it’s harder for me to think about how to analyse this to obtain the update formula. Once the update rule is fixed, we can then treat the entire system (agent and environment) as stochastic without any problems.</p>
<p>If correct or not Sutton-Barto derive this term for the gradient <span class="math display">\[
\mathbb{E}\big[ (R_t - \bar{R}_t) (\mathbb{I}_{a = A_t} - \pi_{H_t}(a)) \big].
\]</span> Which might look a bit fishy because maybe <span class="math inline">\(\mathbb{E}\big[ R_t - \bar{R}_t]\)</span> is always <span class="math inline">\(0\)</span>. But it is usually not, because the policy <span class="math inline">\(\pi\)</span> is not stationary, it get’s updated at every step and thus the policy’s evolution decouples the two expectations.</p>
</section>
</section>
</section>
<section id="associative-search-contextual-bandits" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="associative-search-contextual-bandits"><span class="header-section-number">2.9</span> Associative Search (Contextual Bandits)</h2>
<div id="exr-2.10" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.10</strong></span> Suppose you face a <span class="math inline">\(2\)</span>-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> are respectively <span class="math inline">\(0.1\)</span> and <span class="math inline">\(0.2\)</span> with probability <span class="math inline">\(0.5\)</span> (case A), and <span class="math inline">\(0.9\)</span> and <span class="math inline">\(0.8\)</span> with probability <span class="math inline">\(0.5\)</span> (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?</p>
</div>
<div id="sol-2.10" class="proof solution">
<p><span class="proof-title"><em>Solution 2.10</em>. </span>We are presented with two scenarios and the questions “What is the best strategy?” and “What is its expected reward?” for each scenario.</p>
<p>In the first scenario, we don’t know whether we are facing Case A or Case B at any given time step. The true values of the actions are as follows: <span class="math display">\[
\begin{split}
\mathbb{E}[R_t \mid A_t = 1] &amp;= \mathrm{Pr}(\text{Case A}) \cdot 0.1 + \mathrm{Pr}(\text{Case B}) \cdot 0.9\\
&amp;= 0.5 (0.1 + 0.9) = 0.5\\[3ex]
\mathbb{E}[R_t \mid A_t = 2] &amp;= \mathrm{Pr}(\text{Case A}) \cdot 0.2 + \mathrm{Pr}(\text{Case B}) \cdot 0.8\\
&amp;= 0.5 (0.2 + 0.8) = 0.5
\end{split}
\]</span></p>
<p>Since both actions have the same expected reward of 0.5, it does not matter which action is chosen. Thus, any algorithm is optimal and has an expected of 0.5.</p>
<p>In the second scenario, we know whether we are facing Case A or Case B. The expected reward under the optimal strategy, which always chooses the action with the highest expected value, is: <span class="math display">\[
\mathbb{E}_{\pi_*}[ R_t ] = \overbrace{0.5 \cdot 0.2}^{\text{case A}} + \overbrace{0.5 \cdot 0.9}^{\text{case B}} = 0.55
\]</span></p>
<p>To achieve this expected reward, we need to keep track of the two bandit problems separately and maximise their rewards. How to do this approximately is the topic of the whole chapter.</p>
</div>
</section>
<section id="summary" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.10</span> Summary</h2>
<div id="exr-2.11" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.11</strong></span> Make a figure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size <span class="math inline">\(\varepsilon\)</span>-greedy algorithm with <span class="math inline">\(\alpha\)</span>= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.</p>
</div>
<div id="sol-2.11" class="proof solution">
<p><span class="proof-title"><em>Solution 2.11</em>. </span>That last exercise is a banger to finish with. There’s a lot going on here. I’ll explain what was difficult for me at the end, but let’s start with what I actually did and what we can see in the graph.</p>
<p>I ran a parameter sweep for four different bandit agents over 200 episodes of 300,000 steps each. For each episode, I only looked at the average reward over the last 50,000 steps to measure steady-state performance.</p>
<div id="25e1aeaf" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === Parameter sweep for nonstationary bandit ===</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">We compare 4 bandit agents:</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">0 - ε-greedy sample-average, parameter = ε</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">1 - ε-greedy constant α = 0.1, parameter = ε</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">2 - Gradient ascent with baseline, parameter = α</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">3 - Gradient ascent no baseline, parameter = α</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.ticker <span class="im">import</span> FuncFormatter</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># custom import</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scripts.parameter_study.episode_mean <span class="im">import</span> episode_mean</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Global config</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>CONFIG <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span><span class="dv">1000000</span>,</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    num_arms<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    steps<span class="op">=</span><span class="dv">300_000</span>,</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    keep<span class="op">=</span><span class="dv">50_000</span>,</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    runs<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    q_0<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    drift_sd<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>AGENTS <span class="op">=</span> {</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ε in ε-greedy (sample-avg)"</span>: <span class="bu">dict</span>(</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">id</span><span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>        x_name<span class="op">=</span><span class="st">"ε"</span>,</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**-</span>k <span class="cf">for</span> k <span class="kw">in</span> (<span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>)],</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>        fixed<span class="op">=</span><span class="bu">dict</span>(),</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ε in ε-greedy (α = 0.1)"</span>: <span class="bu">dict</span>(</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">id</span><span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        x_name<span class="op">=</span><span class="st">"ε"</span>,</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**-</span>k <span class="cf">for</span> k <span class="kw">in</span> (<span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>)],</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>        fixed<span class="op">=</span><span class="bu">dict</span>(α<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"α in gradient ascent"</span>: <span class="bu">dict</span>(</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">id</span><span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        x_name<span class="op">=</span><span class="st">"α"</span>,</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**-</span>k <span class="cf">for</span> k <span class="kw">in</span> (<span class="dv">20</span>, <span class="dv">18</span>, <span class="dv">14</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">1</span>)],</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        fixed<span class="op">=</span><span class="bu">dict</span>(),</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">"α in gradient ascent (no base)"</span>: <span class="bu">dict</span>(</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">id</span><span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        x_name<span class="op">=</span><span class="st">"α"</span>,</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**-</span>k <span class="cf">for</span> k <span class="kw">in</span> (<span class="dv">20</span>, <span class="dv">18</span>, <span class="dv">14</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">1</span>)],</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>        fixed<span class="op">=</span><span class="bu">dict</span>(),</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Experiment helper</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(agent_type: <span class="bu">int</span>, <span class="op">**</span>kwargs) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Mean reward over the last *keep* steps averaged across *runs* runs."""</span></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> {<span class="op">**</span>CONFIG, <span class="op">**</span>kwargs}</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.default_rng(args[<span class="st">"seed"</span>])</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>    seeds <span class="op">=</span> rng.integers(<span class="dv">0</span>, <span class="dv">2_000_000</span>, size<span class="op">=</span>args[<span class="st">"runs"</span>])</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>    rewards <span class="op">=</span> [</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a>        episode_mean(</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>            agent_type,</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>            args[<span class="st">"num_arms"</span>],</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>            args[<span class="st">"steps"</span>],</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>            args[<span class="st">"keep"</span>],</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>            args[<span class="st">"q_0"</span>],</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>            <span class="dv">1</span>,  <span class="co"># bandit_action_sd</span></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="co"># drift_mu</span></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>            args[<span class="st">"drift_sd"</span>],</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>            seed,</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>            kwargs.get(<span class="st">"ε"</span>, <span class="fl">0.1</span>),</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a>            kwargs.get(<span class="st">"α"</span>, <span class="fl">0.1</span>),</span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> seed <span class="kw">in</span> seeds</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">float</span>(np.mean(rewards))</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a><span class="co"># --- run the sweeps</span></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, spec <span class="kw">in</span> AGENTS.items():</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>    run <span class="op">=</span> partial(evaluate, spec[<span class="st">"id"</span>], <span class="op">**</span>spec[<span class="st">"fixed"</span>])</span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>    results[label] <span class="op">=</span> [run(<span class="op">**</span>{spec[<span class="st">"x_name"</span>]: x}) <span class="cf">for</span> x <span class="kw">in</span> spec[<span class="st">"x_vals"</span>]]</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a><span class="co"># --- plot</span></span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>markers <span class="op">=</span> [<span class="st">"^"</span>, <span class="st">"o"</span>, <span class="st">"s"</span>, <span class="st">"*"</span>]  <span class="co"># one per agent</span></span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (label, spec), marker <span class="kw">in</span> <span class="bu">zip</span>(AGENTS.items(), markers):</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a>        spec[<span class="st">"x_vals"</span>],</span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a>        results[label],</span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span>marker,</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span>label,</span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a>ax.set_xscale(<span class="st">"log"</span>, base<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Exploration / step-size (log scale)"</span>)</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(</span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Average reward for the last </span><span class="sc">{</span>CONFIG[<span class="st">'keep'</span>]<span class="sc">:,}</span><span class="ss"> steps (</span><span class="sc">{</span>CONFIG[<span class="st">'steps'</span>]<span class="sc">:,}</span><span class="ss"> steps total)"</span></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f"Average reward (mean of </span><span class="sc">{</span>CONFIG[<span class="st">'runs'</span>]<span class="sc">}</span><span class="ss"> runs)"</span>)</span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, which<span class="op">=</span><span class="st">"both"</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a>ax.xaxis.set_major_formatter(</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a>    FuncFormatter(<span class="kw">lambda</span> x, _: <span class="ss">f"1/</span><span class="sc">{</span><span class="bu">int</span>(<span class="dv">1</span><span class="op">/</span>x)<span class="sc">}</span><span class="ss">"</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="bu">str</span>(<span class="bu">int</span>(x)))</span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="02-multi-armed-bandits_files/figure-html/cell-18-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Parameter sweep for a non-stationary bandit, similar to Figure 2.6 [@sutton2018]. The plot displays the average reward of the last 50,000 steps of a 300,000-step episode, averaged over 200 runs. The bandit was initialized with action-value means set to 10, which drifted according to a normal distribution with a standard deviation of 0.1 at each step. The standard deviation of the reward noise was 1."><img src="02-multi-armed-bandits_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img" alt="Parameter sweep for a non-stationary bandit, similar to Figure 2.6 (Sutton and Barto 2018). The plot displays the average reward of the last 50,000 steps of a 300,000-step episode, averaged over 200 runs. The bandit was initialized with action-value means set to 10, which drifted according to a normal distribution with a standard deviation of 0.1 at each step. The standard deviation of the reward noise was 1."></a></p>
<figcaption>Parameter sweep for a non-stationary bandit, similar to Figure 2.6 <span class="citation" data-cites="sutton2018">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton and Barto 2018</a>)</span>. The plot displays the average reward of the last 50,000 steps of a 300,000-step episode, averaged over 200 runs. The bandit was initialized with action-value means set to 10, which drifted according to a normal distribution with a standard deviation of 0.1 at each step. The standard deviation of the reward noise was 1.</figcaption>
</figure>
</div>
</div>
</div>
<p>The agents I tested were:</p>
<ul>
<li><span class="math inline">\(\varepsilon\)</span>-greedy with sample-average action values, sweeping over <span class="math inline">\(\varepsilon\)</span></li>
<li><span class="math inline">\(\varepsilon\)</span>-greedy with constant step-size (α = 0.1), also sweeping <span class="math inline">\(\varepsilon\)</span></li>
<li>gradient ascent with a sample-average baseline, sweeping <span class="math inline">\(\alpha\)</span></li>
<li>gradient ascent with no baseline, again sweeping <span class="math inline">\(\alpha\)</span></li>
</ul>
<p>To encourage adaptability, I changed the drift to 0.1.</p>
<p>Let’s go through some observations:</p>
<ol type="1">
<li>constant step-size ε-greedy outperforms the others. Not that surprising.</li>
<li>gradient ascent underperforms. That surprised me. I’d assumed the gradient method would be a good fit for a drifting environment. However, it does worse than the sample-average ε-greedy.</li>
<li>small α performs best for gradient ascent. This one I don’t have a great explanation for, as very small step sizes typically result in slow learning.</li>
<li>also, the baseline doesn’t matter for verly low α. Again no clue.</li>
<li>sample-average ε-greedy agent is better than I expected. I thought that, with that many steps, it wouldn’t be able to keep up with the changing environment.</li>
<li>random Behaviour for ε = 1. The ε-greedy agents with ε = 1 behaves randomly, as expected. Both agents achieve a reward of 10, which is consistent with random action selection.</li>
</ol>
<p>So, this figure raises quite a few questions I can’t yet answer. I think that makes this execrise really good. And maybe in the future I will be able to provide more insights about these findings.</p>
<p>This was a lot of work. Running 200 episodes at 300,000 steps each is way too slow in pure Python. I had to offload the inner loop into a separate file and used Numba to JIT-compile it, basically rewriting all the algorithms used in this experiment. That’s also why I don’t want to spend more time trying out gradient ascent with a constant-step baseline. Additionally, I can’t guarantee that there isn’t some subtle bug in the code for the main loop, as I didn’t really test all the algorithms.</p>
</div>
</section>
<section id="sec-appendix-multi-arm-bandits" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="sec-appendix-multi-arm-bandits"><span class="header-section-number">2.11</span> appendix</h2>
<p>Here are some more details on concepts that came up during this chapter.</p>
<section id="distribution" class="level3" data-number="2.11.1">
<h3 data-number="2.11.1" class="anchored" data-anchor-id="distribution"><span class="header-section-number">2.11.1</span> distribution</h3>
<p>Every random variable <span class="math inline">\(X\)</span> has a distribution, denoted <span class="math inline">\(p_X\)</span>, which maps each possible value to its probability: <span class="math display">\[
p_X(x) := \mathrm{Pr}(X = x).
\]</span></p>
<p>Often, we say <span class="math inline">\(X\)</span> is <em>distributed according to</em> <span class="math inline">\(f\)</span> for a function <span class="math inline">\(f \colon \mathcal{X} \to [0,1]\)</span>, which means that <span class="math inline">\(f(x) = \mathrm{Pr}(X = x)\)</span>. We write this as: <span class="math display">\[
X \sim f.
\]</span></p>
<p>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the same distribution if <span class="math inline">\(p_X = p_Y\)</span>.</p>
<p>The distribution of <span class="math inline">\(X\)</span> turns <span class="math inline">\((\mathcal{X}, p_X)\)</span> into a probability space, where <span class="math inline">\(p_X\)</span> is called the <em>pushforward measure</em>.</p>
</section>
<section id="sec-iid" class="level3" data-number="2.11.2">
<h3 data-number="2.11.2" class="anchored" data-anchor-id="sec-iid"><span class="header-section-number">2.11.2</span> independent and identically distributed random variables</h3>
<p>A very important concept: <strong>IID</strong>. A collection of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> is <em>independent and identically distributed</em> (IID) if all random variables have the same probability distribution, and all are mutually independent.</p>
<p>Formally, this means:</p>
<ul>
<li><span class="math inline">\(\mathrm{Pr}(X_i = x) = \mathrm{Pr}(X_j = x)\)</span></li>
<li><span class="math inline">\(\mathrm{Pr}(X_i = x,\, X_j = x') = \mathrm{Pr}(X_i = x) \cdot \mathrm{Pr}(X_j = x')\)</span></li>
</ul>
<p>for all distinct indices <span class="math inline">\(i \neq j\)</span>.</p>
</section>
<section id="lotus" class="level3" data-number="2.11.3">
<h3 data-number="2.11.3" class="anchored" data-anchor-id="lotus"><span class="header-section-number">2.11.3</span> lotus</h3>
<p>The following theorem is widely known as the “law of the unconscious statistician”. It is fundamental in many calculations as it allows us to compute the expected value of functions of random variables by only knowing the distributions of the random variables.</p>
<div id="thm-law-of-the-unconscious-statistician" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1</strong></span> Let <span class="math inline">\(X \colon \Omega \to \mathcal{X}\)</span> be a random variable, and let <span class="math inline">\(g \colon \mathcal{X} \to \mathbb{R}\)</span> be a real-valued function on the result space.<br>
Then the expected value of <span class="math inline">\(g\)</span> with respect to the pushforward distribution <span class="math inline">\(p_X\)</span> is the same as the expected value of the random variable <span class="math inline">\(g(X) := g \circ X\)</span> on <span class="math inline">\(\Omega\)</span>: <span class="math display">\[
\mathbb{E}[g(X)] = \sum_{x \in \mathcal{X}} g(x)\, p_X(x)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Pretty sure this proof could be beautifully visualised: summing over columns is the same as summing over rows. But indicator functions <span class="math inline">\(\mathbb{I}\)</span> do the trick too.</p>
<p><span class="math display">\[
\begin{split}
\sum_{x \in \mathcal{X}} g(x) p_X(x)
&amp;= \sum_{x \in \mathcal{X}} g(x) \mathrm{Pr}(X = x) \\
&amp;= \sum_{x \in \mathcal{X}} g(x) \left( \sum_{\omega \in \Omega} \mathrm{Pr}(\omega) \mathbb{I}_{X = x} \right) \\
&amp;= \sum_{x \in \mathcal{X}, \omega \in \Omega} g(x) \mathrm{Pr}(\omega) \mathbb{I}_{X = x} \\
&amp;= \sum_{\omega \in \Omega} \mathrm{Pr}(\omega) \left( \sum_{x \in \mathcal{X}} g(x) \mathbb{I}_{X = x} \right) \\
&amp;= \sum_{\omega \in \Omega} \mathrm{Pr}(\omega) g(X) \\
&amp;= \mathbb{E}[g(X)]
\end{split}
\]</span></p>
</div>
</section>
<section id="multiplication-rule-of-conditional-probabilities" class="level3" data-number="2.11.4">
<h3 data-number="2.11.4" class="anchored" data-anchor-id="multiplication-rule-of-conditional-probabilities"><span class="header-section-number">2.11.4</span> multiplication rule of conditional probabilities</h3>
<p>The multiplication rule of conditional probabilities is great for manipulating unknown distributions into known distributions.</p>
<div id="thm-multiplication-rule" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2</strong></span> Let <span class="math inline">\(A \colon \Omega \to \mathcal{A}\)</span> and <span class="math inline">\(R \colon \Omega \to \mathcal{R}\)</span> be random variables. Then <span class="math display">\[
\mathrm{Pr}[R = r, A = a] =  \mathrm{Pr}(A = a) \mathrm{Pr}[R = r \mid A = a]
\]</span> for <span class="math inline">\(a \in \mathcal{A}\)</span> and <span class="math inline">\(r \in \mathcal{R}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{split}
\mathrm{Pr}[R = r, A = a]
&amp;= \mathrm{Pr}(A = a) \frac{\mathrm{Pr}[R = r, A = a]}{\mathrm{Pr}(A = a)} \\
&amp;= \mathrm{Pr}(A = a) \mathrm{Pr}[R = r \mid A = a]
\end{split}
\]</span></p>
</div>
<div id="thm-law-of-total-probability" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.3</strong></span> Let <span class="math inline">\(A \colon \Omega \to \mathcal{A}\)</span> and <span class="math inline">\(R \colon \Omega \to \mathcal{R}\)</span> be random variables. Then <span class="math display">\[
\mathrm{Pr}[R =r] = \sum_{a \in \mathcal{A}} \mathrm{Pr}(A = a) \mathrm{Pr}[R = r \mid A = a]
\]</span> for <span class="math inline">\(r \in \mathcal{R}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{split}
\mathrm{Pr}[R =r]
&amp;= \sum_{a \in \mathcal{A}} \mathrm{Pr}[R = r, A = a] \\
&amp;= \sum_{a \in \mathcal{A}} \mathrm{Pr}(A = a) \mathrm{Pr}[R = r \mid A = a]
\end{split}
\]</span></p>
</div>
<div id="thm-law-of-total-expectation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.4</strong></span> Let <span class="math inline">\(A \colon \Omega \to \mathcal{A}\)</span> and <span class="math inline">\(R \colon \Omega \to \mathcal{R} \subseteq \mathbb{R}\)</span> be random variables. Then <span class="math display">\[
\mathbb{E}[R] = \sum_{a \in \mathcal{A}} \mathrm{Pr}(A = a) \mathbb{E}[R \mid A = a]
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{split}
\mathbb{E}[R]
&amp;= \sum_{r \in \mathcal{R}} r \mathrm{Pr}(R = r) \\
&amp;= \sum_{r \in \mathcal{R}} r \sum_{a \in \mathcal{A}} \mathrm{Pr}(A = a) \mathrm{Pr}[R = r \mid A = a] \\
&amp;= \sum_{a \in \mathcal{A}} \mathrm{Pr}(A = a) \sum_{r \in \mathcal{R}} r \mathrm{Pr}[R = r \mid A = a] \\
&amp;= \sum_{a \in \mathcal{A}} \mathrm{Pr}(A = a) \mathbb{E}[R \mid A = a]
\end{split}
\]</span></p>
</div>
</section>
<section id="variance" class="level3" data-number="2.11.5">
<h3 data-number="2.11.5" class="anchored" data-anchor-id="variance"><span class="header-section-number">2.11.5</span> variance</h3>
<p>The variance <span class="math inline">\(\mathrm{Var}(X)\)</span> of a random variable is defined as: <span class="math display">\[
\mathrm{Var}(X) := \mathbb{E}[(X-\mu)^2]
\]</span></p>
<p>where <span class="math inline">\(\mu = \mathbb{E}[X]\)</span> is the mean of <span class="math inline">\(X\)</span>.</p>
<p>It can be easily shown that <span class="math display">\[
\mathrm{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2.
\]</span></p>
</section>
<section id="independent-variables" class="level3" data-number="2.11.6">
<h3 data-number="2.11.6" class="anchored" data-anchor-id="independent-variables"><span class="header-section-number">2.11.6</span> independent variables</h3>
<p>Two random variables <span class="math inline">\(X,Y\)</span> are independent if <span class="math display">\[
\mathrm{Pr}(X = x, Y = y) = \mathrm{Pr}(X = x) \cdot \mathrm{Pr}(Y = y).
\]</span></p>
<p>In this case, the conditioned probabilities are equal to the ordinary probabilities</p>
<div id="lem-conditional-propability-of-independent-random-variables" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.1</strong></span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math display">\[
\mathrm{Pr}(X = x \mid Y = y) = \mathrm{Pr}(X = x).
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{split}
\mathrm{Pr}(X = x \mid Y = y) &amp;= \frac{\mathrm{Pr}(X = x, Y = y)}{\mathrm{Pr}(Y = y)} \\
&amp;= \frac{\mathrm{Pr}(X = x) \mathrm{Pr}(Y = y)}{\mathrm{Pr}(Y = y)} \\
&amp;= \mathrm{Pr}(X = x)
\end{split}
\]</span></p>
</div>
<div id="lem-expectation-of-product-of-independent-random-variables" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.2</strong></span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math display">\[
\mathbb{E}(XY) = \mathbb{E}(X) \cdot \mathbb{E}(Y)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We are cheating a bit here (but not doing anything wrong) and apply LOTUS on two random variables at once. <span class="math display">\[
\begin{split}
\mathbb{E}[XY] &amp;= \sum_{x,y} x\cdot y \; \mathrm{Pr}(X = x, Y = y) \\
&amp;= \left(\sum_{x} x \mathrm{Pr}(X = x)\right) \cdot \left(\sum_{y} y \mathrm{Pr}(Y = y)\right) \\
&amp;= \mathbb{E}[X] \cdot \mathbb{E}[Y]
\end{split}
\]</span></p>
</div>
<p>We can use this lemma to prove “linearity” for independent variables.</p>
<div id="lem-variance-of-sum-of-independent-random-variables" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.3</strong></span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math display">\[
\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{split}
\mathrm{Var}(X + Y) &amp;= \mathbb{E}[(X+Y)^2] - \mathbb{E}[X+Y]^2 \\
&amp;= (\mathbb{E}[X^2] + 2 \mathbb{E}[XY] + \mathbb{E}[Y^2]) - (\mathbb{E}[X]^2 + 2 \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y]^2) \\
&amp;= (\mathbb{E}[X^2] - \mathbb{E}[X]^2) + (\mathbb{E}[Y^2] - \mathbb{E}[Y]^2) \\
&amp;= \mathrm{Var}(X) + \mathrm{Var}(Y)
\end{split}
\]</span></p>
</div>
<div id="thm-variance-of-sum-of-independent-random-variables" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.5</strong></span> The population mean <span class="math inline">\(\bar{X}_n\)</span> of IID real-valued random variables <span class="math inline">\(X_1, \dots, X_n\)</span> has variance <span class="math display">\[
\mathrm{Var}(\bar{X}_n) = \frac{\sigma^2}{n},
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the variance of the <span class="math inline">\(X_i\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{split}
\mathrm{Var}(\bar{X}_n) &amp;= \mathrm{Var}(\frac{1}{n}\sum_{i=1}^n X_i) \\
&amp;= \frac{1}{n^2}\sum_{i=1}^n \mathrm{Var}(X_i)\\
&amp;= \frac{1}{n^2} n \sigma^2 \\
&amp;= \frac{\sigma^2}{n}
\end{split}
\]</span></p>
</div>
</section>
<section id="estimators" class="level3" data-number="2.11.7">
<h3 data-number="2.11.7" class="anchored" data-anchor-id="estimators"><span class="header-section-number">2.11.7</span> estimators</h3>
<p>Estimators are functions used to infer the value of a hidden parameter from observed data.</p>
<p>I don’t want to create too much theory for estimators. Let’s look at the <span class="math inline">\(Q_n\)</span> and <span class="math inline">\(R_i\)</span> from <a href="#sec-incremental-implementation" class="quarto-xref"><span>Section 2.4</span></a>.</p>
<p>The <span class="math inline">\(Q_n\)</span> are somehow based on the <span class="math inline">\(R_1, \dots, R_{n-1}\)</span> and called estimators for <span class="math inline">\(q_*\)</span>.</p>
<p>There are some common metrics for determining the quality of an estimator.</p>
<section id="bias" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bias">bias</h4>
<p>The bias of <span class="math inline">\(Q_n\)</span> is <span class="math display">\[
\mathrm{Bias}(Q_n) = \mathbb{E}[Q_n] - q_*.
\]</span></p>
<p>If this is <span class="math inline">\(0\)</span> then <span class="math inline">\(Q_n\)</span> is unbiased. If the bias disappears asymptotically, then <span class="math inline">\(Q_n\)</span>​ is asymptotically unbiased.</p>
</section>
<section id="mean-squared-error" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-squared-error">mean squared error</h4>
<p>It is used to indicate how far, on average, the collection of estimates are from the single parameter being estimated. <span class="math display">\[
\mathrm{MSE}(Q_n) = \mathbb{E}[ (Q_n - q_*)^2]
\]</span></p>
<p>The mean squared error can be expressed in terms of bias and variance. <span class="math display">\[
\mathrm{MSE}(Q_n) = \mathrm{Var}(Q_n) + \mathrm{Bias}(Q_n)^2
\]</span></p>
<p>In particular, for unbiased estimators, the mean squared error is just the variance.</p>
<div id="lem-mse-sample-average" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.4</strong></span> Let <span class="math inline">\(Q_n\)</span> be the sample average of <span class="math inline">\(R_1, \dots, R_n\)</span>. Then <span class="math display">\[
\mathrm{MSE}(Q_n) = \frac{\sigma^2}{n},
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the variance of the <span class="math inline">\(X_i\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The sample average is unbiased. Thus, its mean squared error is its variance given in <a href="#thm-variance-of-sum-of-independent-random-variables" class="quarto-xref">Theorem&nbsp;<span>2.5</span></a>.</p>
</div>
<div id="lem-mse-constant-step-size" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.5</strong></span> Let <span class="math inline">\(Q_n\)</span> be the constant step size weighted average of the <span class="math inline">\(R_1, \dots, R_n\)</span>. Then, the Mean Squared Error of <span class="math inline">\(Q_{n+1}\)</span> is given by: <span class="math display">\[
\mathrm{MSE}(Q_{n+1}) = \sigma^2 \frac{\alpha}{2-\alpha} + (1 - \alpha)^{2n} [(Q_n - q_*)^2 - \sigma^2\frac{\alpha}{2-\alpha}],
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the variance of the <span class="math inline">\(R_i\)</span>.</p>
<p>In particular, the MSE is bounded from below by <span class="math display">\[
\lim_{n\to\infty}\mathrm{MSE}(Q_n) = \sigma^2 \frac{\alpha}{2-\alpha}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The weighted average <span class="math inline">\(Q_{n+1}\)</span>​ is defined as: <span class="math display">\[
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1-\alpha)^{n-i} R_i
\]</span></p>
<section id="step-1-compute-the-expected-value" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="step-1-compute-the-expected-value">step 1: compute the expected value</h5>
<p>This has been done in <a href="#eq-mean-of-constant-step-size-average" class="quarto-xref">Equation&nbsp;<span>2.5</span></a> <span class="math display">\[
\mathbb{E}(Q_{n+1}) = (1-\alpha)Q_1 + (1 - (1-\alpha)^n)q_*
\]</span></p>
</section>
<section id="step-2-compute-the-bias-of" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="step-2-compute-the-bias-of">step 2: compute the bias of</h5>
<p>Using <span class="math inline">\(\mathrm{Bias}(Q_{n+1}) = \mathbb{E}[Q_{n+1}] - q_*\)</span> we get <span class="math display">\[
\begin{split}
\mathrm{Bias}(Q_{n+1}) &amp;= (1-\alpha)Q_1 + (1 - (1-\alpha)^n)q_* - q_* \\
&amp;= (1-\alpha)^n [Q_n - q_*].
\end{split}
\]</span></p>
</section>
<section id="step-3-compute-the-variance" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="step-3-compute-the-variance">step 3: compute the variance</h5>
<p><span class="math display">\[
\begin{split}
\mathrm{Var}(Q_{n+1}) &amp;= \mathrm{Var}\big((1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1-\alpha)^{n-i} R_i \big) \\
&amp;= \sum_{i=1}^n \alpha^2 \big((1-\alpha)^{2}\big)^{n-i} \sigma^2 \\
&amp;= \sigma^2\alpha^2 \sum_{i=0}^{n-1} \big((1-\alpha)^{2}\big)^{i} \\
&amp;= \sigma^2\alpha^2 \frac{1 - (1-\alpha)^{2n}}{1 - (1-\alpha)^2} \\
&amp;= \sigma^2 \frac{\alpha}{2-\alpha} \big(1 - (1-\alpha)^{2n}\big).
\end{split}
\]</span> Here it’s crucial that the <span class="math inline">\(R_i\)</span> are independent.</p>
</section>
<section id="step-4-compute-the-mean-squared-error" class="level5" data-number="2.11.7.0.1">
<h5 data-number="2.11.7.0.1" class="anchored" data-anchor-id="step-4-compute-the-mean-squared-error"><span class="header-section-number">2.11.7.0.1</span> step 4: compute the mean squared error</h5>
<p>Now we can use <span class="math inline">\(\mathrm{MSE}(Q_{n+1}) = \mathrm{Var}(Q_{n+1}) + \mathrm{Bias}(Q_{n+1})^2\)</span> <span class="math display">\[
\begin{split}
\mathrm{MSE}(Q_{n+1}) &amp;= \sigma^2 \frac{\alpha}{2-\alpha} \big(1 - (1-\alpha)^{2n}\big) + \Big( (1-\alpha)^n [Q_n - q_*] \Big)^2 \\
&amp;= \sigma^2 \frac{\alpha}{2-\alpha} + (1 - \alpha)^{2n} [(Q_n - q_*)^2 - \sigma^2\frac{\alpha}{2-\alpha}]
\end{split}
\]</span></p>
</section>
</div>
</section>
</section>
<section id="sec-geometric-series" class="level3" data-number="2.11.8">
<h3 data-number="2.11.8" class="anchored" data-anchor-id="sec-geometric-series"><span class="header-section-number">2.11.8</span> geometric series</h3>
<p>In the context of reinforcement learning, the concept of discounting naturally requires the notion of <em>geometric series</em>. This series is defined as, <span class="math display">\[
S(n+1) := \sum_{i=0}^n \gamma^i,
\]</span></p>
<p>where <span class="math inline">\(\gamma \in \mathbb{R}\)</span>. By convention, an empty sum is considered to be 0, thus <span class="math inline">\(S(0)=0\)</span>.</p>
<p>If <span class="math inline">\(\gamma = 1\)</span>, then the geometric series simplifies to <span class="math inline">\(S(n+1) = n+1\)</span>. So let’s assume <span class="math inline">\(\gamma \neq 1\)</span> from now on.</p>
<p>By pulling out the term for <span class="math inline">\(i=0\)</span> and factoring out a <span class="math inline">\(\gamma\)</span>, we can derive a recurrence relation for the geometric series <span id="eq-geometric-series-recurrence-relation"><span class="math display">\[
S(n+1) = 1 + \gamma S(n) \quad \text{and} \quad S(0) = 0
\tag{2.13}\]</span></span></p>
<p>When we even add a clever <span class="math inline">\(0 = \gamma^{n+1} - \gamma^{n+1}\)</span>, we get this equation for <span class="math inline">\(S(n)\)</span> <span class="math display">\[
S(n) = (1 - \gamma^{n+1}) + \gamma S(n).
\]</span></p>
<p>From this, we can deduce the closed-form expression for the geometric series: <span id="eq-finite-geometric-series"><span class="math display">\[
\sum_{i=0}^n \gamma^i  = \frac{1 - \gamma^{n+1}}{1 - \gamma}
\tag{2.14}\]</span></span></p>
<p>By omitting the first term (starting from <span class="math inline">\(i = 1\)</span>), we obtain: <span id="eq-finite-geometric-series-truncated"><span class="math display">\[
\sum_{i=1}^n \gamma^i =  \frac{\gamma - \gamma^{n+1}}{1 - \gamma}
\tag{2.15}\]</span></span></p>
<p>The infinite geometric series converges, if and only if, <span class="math inline">\(|\gamma| &lt; 1\)</span>. Using the previous formulas, we can derive their limits: <span id="eq-infinite-geometric-series"><span class="math display">\[
\sum_{i=0}^\infty \gamma^i = \frac{1}{1-\gamma}
\tag{2.16}\]</span></span> <span id="eq-infinite-geometric-series-truncated"><span class="math display">\[
\sum_{i=1}^\infty \gamma^i = \frac{\gamma}{1-\gamma}
\tag{2.17}\]</span></span></p>
</section>
<section id="sec-recurrence-relations" class="level3" data-number="2.11.9">
<h3 data-number="2.11.9" class="anchored" data-anchor-id="sec-recurrence-relations"><span class="header-section-number">2.11.9</span> recurrence relations</h3>
<p>As it turns out, the basic geometric series we’ve explored isn’t quite enough to handle discounting and cumulative discounted returns in reinforcement learning. While the geometric series solves the homogeneous linear recurrence relation given by <a href="#eq-geometric-series-recurrence-relation" class="quarto-xref">Equation&nbsp;<span>2.13</span></a>, dealing with cumulative discounted returns introduces a non-homogeneous variation, where the constants 1s are replaced by a some <span class="math inline">\(r_i\)</span>​, leading to the recurrence relation: <span id="eq-non-homogenous-geometric-series-recurrence-relation"><span class="math display">\[
Q(n+1) := r_n + \gamma Q(n) \quad \text{and} \quad Q(0) := 0
\tag{2.18}\]</span></span></p>
<p>We can also give an explicit formula for <span class="math inline">\(Q(n+1)\)</span>: <span id="eq-non-homogenous-geometric-series-convolution-form"><span class="math display">\[
Q(n+1) = \sum_{i=0}^n \gamma^{n-i} r_i.
\tag{2.19}\]</span></span></p>
<p>It’s easy to verify that this fulfils the recursive definition: <span class="math display">\[
\begin{split}
Q(n+1) &amp;= r_n + \sum_{i=0}^{n-1} \gamma^{n-i} r_i \\
&amp;= r_n + \gamma\sum_{i=0}^{n-1} \gamma^{n-1-i} r_{i} \\
&amp;= r_n + \gamma Q(n).
\end{split}
\]</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-sutton2018" class="csl-entry" role="listitem">
Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. <a href="https://mitpress.mit.edu/9780262039246/reinforcement-learning/">https://mitpress.mit.edu/9780262039246/reinforcement-learning/</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>I’m saying this not very loudly. Maybe I’m somewhere wrong.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/01-intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/03-finite-markov-decision-processes.html" class="pagination-link" aria-label="Finite Markov Decision Processes">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>