# Multi-arm Bandits

Some imports for python
```{python}
from dataclasses import dataclass, field
import numpy as np
import matplotlib.pyplot as plt
from typing import Sequence, Tuple
```

## A $k$-armed Bandit Problem

The true value of an action is defined as:
$$
q_*(a) := \mathbb{E}[ R_t \mid A_t = a].
$$

The time index here doesn't serve a special purpose, but it's a handy trick to have a temporal reference. You can think of it as "when $a$ is picked".

## Action-value Methods

This always trips me up, so let me clarify: $Q_t(a)$ is the estimate prior to time $t$, so $A_t$​ is not included. Then, $A_t$​ is based on the collection of $Q_t(a)$. For example, you could pick $A_t$​ greedily as $A_t:=\mathrm{argmax}_a Q_t(a)$, or $\varepsilon$-greedily.

### Exercise 2.1 {.unnumbered}

In $\varepsilon$-greedy action selection, for the case of two actions and $\varepsilon = 0.5$, what is the probability that the greedy action is selected?

#### Solution {.unnumbered}

The probability is 
$$
P(\text{greedy action}) + P(\text{explorative action}) \cdot \frac{1}{2} = 0.75
$$

## The 10-armed Testbed

The 10-armed testbed will accompany us through the rest of this chapter (I had to keep it variable in size just for the sake of generalization though).

```{python}
# | code-fold: false
# === the armed bandit ===
@dataclass
class ArmedBandit:
    """k-armed Gaussian bandit."""

    action_mu: np.ndarray  # mean reward of each arm
    action_sd: float  # standard deviation of reward noise
    seed: int | None = None
    rng: np.random.Generator = field(init=False)

    def __post_init__(self):
        self.action_mu = np.asarray(self.action_mu, dtype=np.float64).copy()
        self.rng = np.random.default_rng(self.seed)

    def pull_arm(self, action):
        return self.rng.normal(loc=self.action_mu[action], scale=self.action_sd)
```

And here's the code for the sample-average bandit algorithm. For clarity, I'll refer to this and upcoming algorithms as "agents", given their autonomous implementation. Note that we're also using the incremental implementation from section 2.4.

```{python}
# | code-fold: false
# === the simple average bandit agent ===
class SampleAverageBanditAgent:
    def __init__(self, Q1, ε, seed=None):
        self.rng = np.random.default_rng(seed)
        self.num_actions = len(Q1) 
        self.Q1 = np.asarray(Q1, dtype=np.float64)  # initial action-value estimates
        self.ε = ε  
        self.reset()

    def reset(self):
        self.Q = self.Q1.copy()
        self.counts = np.zeros(self.num_actions, dtype=int)

    def act(self, bandit):
        # ε-greedy action selection
        if self.rng.random() < self.ε:
            action = self.rng.integers(self.num_actions)
        else:
            action = np.argmax(self.Q)

        # take action and observe the reward
        reward = bandit.pull_arm(action)

        # update count and value estimate
        self.counts[action] += 1
        α = 1 / self.counts[action]
        self.Q[action] += α * (reward - self.Q[action])

        return (action, reward)
```    

Next, I'll define an experiment function that will be used throughout this chapter. To keep things simple, here's a summary: this function takes multiple agents, a specified number of steps (the length of each episode), and runs (the number of episodes), then repeatedly executes these agents and finally plots their average rewards and the percentage of optimal actions in these episodes. You don't have to read it though...

```{python}
# === the core bandit experiment ===


# -- core experiment
def bandit_experiment(
    agents: Sequence,
    *,
    bandit_setup_mu: float = 0.0,
    bandit_setup_sd: float = 1.0,
    bandit_action_sd: float = 1.0,
    bandit_value_drift: bool = False,
    bandit_value_drift_mu: float = 0.0,
    bandit_value_drift_sd: float = 0.0,
    bandit_num_arms: int = 10,
    exp_steps: int = 1_000,
    exp_runs: int = 200,
    exp_seed: int | None = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Run `exp_runs` × `exp_steps` episodes and return:

    average_rewards         shape = (len(agents), exp_steps)
    optimal_action_percent  shape = (len(agents), exp_steps)
    """
    rng = np.random.default_rng(exp_seed)
    num_agents = len(agents)
    average_rwds = np.zeros((num_agents, exp_steps))
    optimal_acts = np.zeros((num_agents, exp_steps))

    # allocate a single bandit and reuse its object shell
    bandit = ArmedBandit(
        action_mu=np.empty(bandit_num_arms),  # placeholder
        action_sd=bandit_action_sd,
        seed=exp_seed,
    )

    for run in range(exp_runs):
        # fresh true values for this run
        bandit.action_mu[:] = rng.normal(
            bandit_setup_mu, bandit_setup_sd, size=bandit_num_arms
        )
        best_action = np.argmax(bandit.action_mu)

        # reset all agents
        for agent in agents:
            agent.reset()

        # vectorised drift noise: shape = (exp_steps, bandit_num_arms)
        if bandit_value_drift:
            drift_noise = rng.normal(
                bandit_value_drift_mu,
                bandit_value_drift_sd,
                size=(exp_steps, bandit_num_arms),
            )

        # main loop
        for t in range(exp_steps):
            for i, agent in enumerate(agents):
                act, rwd = agent.act(bandit)
                average_rwds[i, t] += rwd
                optimal_acts[i, t] += act == best_action

            if bandit_value_drift:
                bandit.action_mu += drift_noise[t]
                best_action = np.argmax(bandit.action_mu)

    # mean over runs
    average_rwds /= exp_runs
    optimal_acts = 100 * optimal_acts / exp_runs
    return average_rwds, optimal_acts


# --- thin plotting helpers
def plot_average_reward(
    average_rewards: np.ndarray,
    *,
    labels: Sequence[str] | None = None,
    ax: plt.Axes | None = None,
) -> plt.Axes:
    """One line per agent: average reward versus step."""
    if ax is None:
        _, ax = plt.subplots(figsize=(8, 4))

    steps  = np.arange(1, average_rewards.shape[1] + 1)
    if labels is None:
        labels = [f"agent {i}" for i in range(average_rewards.shape[0])]

    for i, lbl in enumerate(labels):
        ax.plot(steps, average_rewards[i], label=lbl)

    ax.set_xlabel("Step")
    ax.set_ylabel("Average reward")
    ax.set_title("Average reward per step")
    ax.grid(alpha=0.3, linestyle=":")
    ax.legend()
    return ax


def plot_optimal_action_percent(
    optimal_action_percents: np.ndarray,
    *,
    labels: Sequence[str] | None = None,
    ax: plt.Axes | None = None,
) -> plt.Axes:
    """One line per agent: % optimal action versus step."""
    if ax is None:
        _, ax = plt.subplots(figsize=(8, 4))

    steps  = np.arange(1, optimal_action_percents.shape[1] + 1)
    if labels is None:
        labels = [f"agent {i}" for i in range(optimal_action_percents.shape[0])]

    for i, lbl in enumerate(labels):
        ax.plot(steps, optimal_action_percents[i], label=lbl)

    ax.set_xlabel("Step")
    ax.set_ylabel("% optimal action")
    ax.set_title("Optimal-action frequency")
    ax.grid(alpha=0.3, linestyle=":")
    ax.legend()
    return ax
```

...it's best to see such an experiment in action. We can, for example, recreate Figure 2.2 [@sutton2018]. It compares the performance of a greedy agent ($\varepsilon = 0$), and two $\varepsilon$-greedy agents with $\varepsilon =0.1$ and $\varepsilon=0.01$. We let them run for a couple of steps and then repeat this process for a couple of runs to get a smoother curve.

```{python}
# | code-fold: false
# | label: fig-bandit-comparison-greedyness
# | fig-cap: "This is like Figure 2.2 [@sutton2018]: the average performance of different ε-greedy sample average methods over 2000 runs. On the 10-armed testbed."
# | fig-subcap:
# |  - "The average reward."
# |  - "The percentage of optimal step selection"
# === comparison greedyness ===

# global parameters that will stay fixd in this chapter
seed = 0  # seed value for rng
num_arms = 10  # number of arms

# initialize agents with different epsilon values
epsilons = [0.0, 0.1, 0.01]
agents = [SampleAverageBanditAgent(Q1=np.zeros(10), ε=ε, seed=seed) for ε in epsilons]

# run bandit experiment
avg_rwd, opt_pct = bandit_experiment(
    agents,
    exp_steps=1_000,
    exp_runs=2_000,
    bandit_num_arms=num_arms,
    exp_seed=seed,
)

# plot average reward
plot_average_reward(avg_rwd, labels=[f"ε={e}" for e in epsilons])
plt.tight_layout()
plt.show()

# plot %optimal actions
plot_optimal_action_percent(opt_pct, labels=[f"ε={e}" for e in epsilons])
plt.tight_layout()
plt.show()
```

Just out of curiosity, let's see what happens when we have only one run (so it's not the average anymore but just the reward). Wow, it's a mess. Without averaging over a couple of runs, we can't make out anything.

```{python}
# === experiment with only one run ===
epsilons = [0.0, 0.1, 0.01]
agents = [SampleAverageBanditAgent(Q1=np.zeros(10), ε=ε, seed=seed) for ε in epsilons]
avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    exp_steps=1_000,
    exp_runs=1,
    exp_seed=seed,
)

plot_average_reward(avg_rwd, labels=[f"ε={e}" for e in epsilons])
plt.tight_layout()
plt.show()
```

### Exercise 2.2: _Bandit example_ {.unnumbered}

Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\varepsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all a. Suppose the initial sequence of actions and rewards is $A_1 = 1$, $R_1 = -1$, $A_2 = 2$, $R_2 = 1$, $A_3 = 2$, $R_3 = -2$, $A_4 = 2$, $R_4 = 2$,  $A_5 = 3$, $R_5 = 0$. On some of these time steps the $\varepsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?

#### Solution {.unnumbered}

Step 1 could have been exploratory, as all actions have the same estimates. After that, the value function is:
$$
Q_2(a) = \begin{cases}
			-1,& \text{if $a = 1$}\\
            0,& \text{otherwise}
		 \end{cases}
$$

Also, step 2 could have been exploratory. Now the value function is:
$$
Q_3(a) = \begin{cases}
			-1,& \text{if $a = 1$}\\
            1,& \text{if $a = 2$}\\
            0,& \text{otherwise}
		 \end{cases}
$$

In step 3, the action of the greedy action is taken. But it could also have been an exploratory action that selected 2. Now the value function is:
$$
Q_4(a) = \begin{cases}
			-1,& \text{if $a = 1$}\\
            -0.5,& \text{if $a = 2$}\\
            0,& \text{otherwise}
		 \end{cases}
$$

In step 4, a non-greedy action is taken, so this must have been an exploratory move. The value function is:
$$
Q_5(a) = \begin{cases}
			-1,& \text{if $a = 1$}\\
            0.33,& \text{if $a = 2$}\\
            0,& \text{otherwise}
		 \end{cases}
$$

In step 5, again a non-greedy action was taken, so this must have been an exploratory move as well.

### Exercise 2.3 {.unnumbered}

In the comparison shown in @fig-bandit-comparison-greedyness, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.

#### Solution {.unnumbered}

Obviously, we can disregard $\varepsilon = 0$. It's just bad.
Before we do the quantitative analysis, let's see what happens when we just crank up the number of steps (and reduce the runs even though now it's a bit noisier).

```{python}
# === battle betwee ε=0.1 and ε=0.01 ===
epsilons = [0.1, 0.01]
agents = [SampleAverageBanditAgent(Q1=np.zeros(10), ε=ε, seed=seed) for ε in epsilons]
avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    exp_steps=15_000,
    exp_runs=200,
    exp_seed=seed,
)

plot_average_reward(avg_rwd, labels=[f"ε={e}" for e in epsilons])
plt.tight_layout()
plt.show()

plot_optimal_action_percent(opt_pct, labels=[f"ε={e}" for e in epsilons])
plt.tight_layout()
plt.show()
```

We can see that $\varepsilon=0.01$ outperforms $\varepsilon=0.1$ in average reward around step $2000$. However, achieving a higher percentage of optimal actions takes more than $10,000$ steps. It's actually quite interesting that acchieving a higher percentage of optimal actions takes significantly longer.

Now, let's consider the long-term behavior. In the limit, we can assume both methods have near-perfect $Q$-values and the only reason they select non-optimal actions is due to their $\varepsilon$-softness.

This makes the calculation of the optimal action probability quite easy.
$$
P(\text{optimal action}) = (1-\varepsilon) + \varepsilon \frac{1}{10} = 1 - 0.9 \varepsilon
$$

So for $\varepsilon=0.1$ this probability is $0.91$, and for $\varepsilon=0.01$ this is $0.991$.

Now the average reward is trickier to compute. It can be done, but it's quite messy and we're here to learn reinforcement learning so we don't need to figure out perfect analytical solutions anymore. Luckily, we get this value directly from the book

> It [greedy algorithm] achieved a reward-per-step of only about $1$, compared with the best possible of about $1.55$ on this testbed [@sutton2018, p. 29].

Great, so selecting the optimal action gives an average reward of $1.55$. Selecting a random action has an average reward of $0$ because it's basically drawing a sample from a normal distribution with mean $0$. That gives:

$$
\mathbb{E}[R_t] = (1-\varepsilon) 1.55 + \varepsilon 0 = 1.55 (1-\varepsilon)
$$

This results in $1.40$ for $\varepsilon = 0.1$ and $1.53$ for $\varepsilon = 0.01$.

## Incremental Implementation

A common update rule we'll encounter a lot in reinforcement learning is:
$$
\text{NewEstimate} \gets \text{OldEstimate} + \text{StepSize}
\Big[\overbrace{
    \text{Target} - \text{OldEstimate}
    }^\text{error} \Big]
$$

I especially like that it is well-suited for programming. This typically translates to a central line of code in our value-based algorithms like so:
```{python}
#| code-fold: false
#| eval: false
Q[action] += α * (reward - Q[action])
```

## Tracking a Nonstationary Problem

To prevent the learning rate from slowing down (at the cost of convergence), we can use a constant step size $\alpha \in (0,1]$.
$$
Q_{n+1} := Q_n + \alpha \Big[ R_n - Q_n \Big]
$$

The closed form for $Q_{n+1}$​, given the initial value estimate $Q_1$​ and the received rewards $R_i$​, is:
$$
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} R_i
$$ {#eq-weighted-average}

This is a weighted average because the sum of the weights equals $1$, as shown here:
$$
\begin{split}
(1-\alpha)^n + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} &= (1-\alpha)^n + \alpha \sum_{i=0}^{n-1} (1-\alpha)^i \\
&= (1-\alpha)^n + \alpha \frac{1 - (1-\alpha)^n}{\alpha} = 1
\end{split}
$$
using the geometric series identiy $\sum_{i=0}^N a_i = \frac{1 - a^{N+1}}{1 - a}$.

Let's verify @eq-weighted-average inductively
$$
\begin{split}
Q_{n+1} &= Q_n + \alpha [R_n - Q_n] \\
&= \alpha R_n + (1-\alpha) Q_n \\
&= \alpha R_n + (1-\alpha) \Big[ (1-\alpha)^{n-1} Q_1 + \sum_{i=1}^{n-1} \alpha (1-\alpha)^{n-1-i} R_i \Big] \\
&= (1-\alpha)^n Q_1 + \alpha (1-\alpha)^0 R_n + \sum_{i=1}^{n-1} \alpha (1-\alpha)^{n-i} R_i \\
&= (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i
\end{split}
$$

The $Q_{n}$ values are estimators for the real value $q_*$. If $Q_n$​ is the sample average, the estimator is unbiased, i.e., $\mathbb{E}[Q_n] = q_*$ for all $n \in \mathbb{N}$.

In case they are given by a constant step size, they are biased (there is always a small $Q_1$ dependecy):
$$
\begin{split}
\mathbb{E}[Q_{n+1}] &=  (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} q  \\
&= (1-\alpha)^n Q_1 + q \alpha \sum_{i=1}^n (1 - \alpha)^{n-i} \\
&= (1-\alpha)^n Q_1 + q \alpha \sum_{i=0}^{n-1} (1 - \alpha)^{i} \\
&=  (1-\alpha)^n Q_1 + q \alpha \frac{1 - (1 - \alpha)^n}{\alpha} \\
&= (1-\alpha)^n Q_1 + q (1 - (1 - \alpha)^n)
\end{split}
$$

However, they are asymptotically unbiased, i.e., $\lim_{n\to\infty} \mathbb{E}[Q_{n}] = q$.

Let's define the constant step size agent to compare it with the sample average method later.
```{python}
# | code-fold: false
# === the constant step bandit agent ===
class ConstantStepBanditAgent:
    def __init__(self, Q1, α, ε, seed=None):
        self.rng = np.random.default_rng(seed)
        self.num_actions = len(Q1)
        self.Q1 = Q1
        self.α = α
        self.ε = ε
        self.reset()

    def reset(self):
        self.Q = self.Q1.copy()

    def act(self, bandit):
        # ε-greedy action selection
        if self.rng.random() < self.ε:
            action = self.rng.integers(self.num_actions)
        else:
            action = np.argmax(self.Q)

        # take action
        reward = bandit.pull_arm(action)

        # update value estimate
        self.Q[action] += self.α * (reward - self.Q[action])

        return (action, reward)
```  

### Exercise 2.4 {.unnumbered}

If the step-size parameters, $\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by @eq-weighted-average. What is the weighting on each prior reward for the general case, analogous to @eq-weighted-average, in terms of the sequence of step-size parameters.

#### Solution {.unnumbered}

The update rule for non-constant step size has $\alpha_n$ depending on the step.
$$
Q_{n+1} = Q_n + \alpha_n \Big[ R_n - Q_n \Big]
$${#eq-non-constant-stepsize}

In this case the weighted average is given by
$$
Q_{n+1} = \left( \prod_{j=1}^n 1-\alpha_j \right) Q_1 + \sum_{i=1}^n \alpha_i \left( \prod_{j=i+1}^n 1 - \alpha_j \right) R_i
$${#eq-general-weighted-average}

We can verify this formula inductively.
For $n=0$, we we get $Q_1$ on both sides.

For the induction step we have
$$
\begin{split}
Q_{n+1} &= Q_n + \alpha_n \Big[ R_n - Q_n \Big] \\
&= \alpha_n R_n + (1 - \alpha_n) Q_n \\
&= \alpha_n R_n + (1 - \alpha_n) \Big[ \left( \prod_{j=1}^{n-1} 1-\alpha_j \right) Q_1 + \sum_{i=1}^{n-1} \alpha_i \left( \prod_{j=i+1}^{n-1} 1 - \alpha_j \right) R_i \Big] \\
&= \left( \prod_{j=1}^n 1-\alpha_j \right) Q_1 + \sum_{i=1}^n \alpha_i \left( \prod_{j=i+1}^n 1 - \alpha_j \right) R_i
\end{split}
$$

We also note that in this general setting, @eq-general-weighted-average is still a weighted average. We could prove it by induction or use a little trick. If we set $Q_1 = 1$ and $R_n = 1$ for all $n$ in @eq-non-constant-stepsize we see that each $Q_n = 1$. If we do the same in @eq-general-weighted-average we see that each $Q_n$ is equal to the sum of the weights. Therefore, the weights sum up to $1$.

### Exercise 2.5 (programming) {.unnumbered}

Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $q_\star(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the $q_\star(a)$ on each step). Prepare plots like Figure @fig-bandit-comparison-greedyness for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\alpha = 0.1$. Use $\epsilon = 0.1$ and longer runs, say of 10,000 steps

#### Solution {.unnumbered}

Alright, let's do a little experiment, just as they told us.

```{python}
# === battle between sample average and constant step ===
agent0 = SampleAverageBanditAgent(Q1=np.zeros(num_arms, dtype=float), ε=0.1, seed=seed)
agent1 = ConstantStepBanditAgent(
    Q1=np.zeros(num_arms, dtype=float), α=0.1, ε=0.1, seed=seed
)
agents = [agent0, agent1]
avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    bandit_setup_mu=0,
    bandit_setup_sd=0,
    bandit_value_drift=True,
    bandit_value_drift_mu=0,
    bandit_value_drift_sd=0.01,
    exp_steps=10_000,
    exp_runs=100,
    exp_seed=seed,
)

labels = ["sample averages (ε=0.1)", "constant step-size (α=0.1, ε=0.1)"]
plot_average_reward(avg_rwd, labels=labels)
plt.tight_layout()
plt.show()

plot_optimal_action_percent(opt_pct, labels=labels)
plt.tight_layout()
plt.show()
```

Not surprisingly, we can see how much the sample average agent struggles to keep up. For longer episode lengths, the problem only gets worse. Eventually, it will be completely out of touch with the world, like an old man unable to keep up with the times.

However, it remains unclear exactly how rapidly the sample average method will deteriorate to an unacceptable level. The results from the final exercise of this chapter (exercise 2.11) indicate that this deterioration may take a significant number of steps.

## Optimistic Initial Values

We later need the following figure comparing an optimistic greedy agent and a realistic $\varepsilon$-greedy agent.
```{python}
# | label: fig-bandit-comparison-optimism
# | fig-cap: "TThis is like Figure 2.3 [@sutton2018]: the effect of optimistic initial action-value estimates."
# === realism vs optimism ===
agent_optimistic_greedy = ConstantStepBanditAgent(
    Q1=np.full(num_arms, 5.0, dtype=float), α=0.1, ε=0.0, seed=seed
)
agent_realistic_ε_greedy = ConstantStepBanditAgent(
    Q1=np.zeros(num_arms, dtype=float), α=0.1, ε=0.1, seed=seed
)

agents = [agent_optimistic_greedy, agent_realistic_ε_greedy]
avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    exp_steps=1_000,
    exp_runs=1_000,
    exp_seed=seed,
)

labels = [
    "optimistic,greedy (Q1=5, ε=0, α=0.1)",
    "realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)",
]
plot_optimal_action_percent(opt_pct, labels=labels)
plt.tight_layout()
plt.show()
```

### Exercise 2.6: Mysterious Spikes {.unnumbered}

The results shown in @fig-bandit-comparison-optimism should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?

#### Solution {.unnumbered}

We have the hard-earned luxury that we can zoom in on @fig-bandit-comparison-optimism:
```{python}
# === realism vs optimism zoomed in ===
agent_optimistic_greedy = ConstantStepBanditAgent(
    Q1=np.full(num_arms, 5.0, dtype=float), α=0.1, ε=0.0, seed=seed
)
agent_realistic_ε_greedy = ConstantStepBanditAgent(
    Q1=np.zeros(num_arms, dtype=float), α=0.1, ε=0.1, seed=seed
)

agents = [agent_optimistic_greedy, agent_realistic_ε_greedy]
avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    exp_steps=30,
    exp_runs=2_000,
    exp_seed=seed,
)

labels = [
    "optimistic,greedy (Q1=5, ε=0, α=0.1)",
    "realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)",
]
plot_optimal_action_percent(opt_pct, labels=labels)
plt.tight_layout()
plt.show()
```

The spike occurs at step 11. Essentially, the optimistic method samples all actions once (poor performance), and then selects the action with the best result (good performance, with a success rate of over 40%). However, regardless of the outcome (which likely pales in comparison to the current Q-values, which are still likely greater than 4), the method returns to exploring all 10 actions again. This leads to poor performance once more. Around step 22, there is another spike for similar reasons, but this time smaller and more spread out.

### Exercise 2.7: Unbiased Constant-Step-Size Trick {.unnumbered}

In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of  onstant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of
$$
\beta_n := \alpha / \bar{o}_n
$$
to process the $n$-th reward for a particular action, where $\alpha > 0$ is a conventional constant step size, and $\bar{o}_n$ is a trace of one that starts at $0$:
$$
\bar{o}_n := \bar{o}_{n-1} + \alpha (1 - \bar{o}_{n-1}), \text{ for } n \geq 0, \text{ with } \bar{o}_0 := 0
$$
Carry out an analyises like that in @eq-weighted-average to show that $Q_n$ is an exponential recency-weighted average without initial bias.

#### Solution {.unnumbered}

My first question when I saw this was, "What's up with that strange name $\bar{o}_n$?"" I guess it could be something like "the average of ones", which would make sense:
We obtain this sequence when we use $Q_{n+1} = Q_n + \alpha_n [R_n - Q_n]$ and set $Q_1 = \alpha$ (which is equivalent to $\bar{o}_0 = 0$) and $R_n = 1$.

Since $\beta_1 = 1$, the influence of $Q_1$ disappears after the first reward $R_1$ is received: $Q_2 = Q_1 + 1 [ R_1 - Q_1] = R_1$. Great!

Scaling the $\alpha$ by the $\bar{o}_n$ has an additional nicer effect. I don't quite understand how, but we can calculate it.

From Execrise 2.4 we know
$$
Q_{n+1} = Q_1 \prod_{j=1}^n (1-\beta_j)
+ \sum_{i=1}^n  R_i \beta_i \prod_{j=i+1}^n (1- \beta_j )
$$

There is a nice form for these products
$$
\prod_{j=i}^n (1 - \beta_j) = (1-\alpha)^{n-j+1} \frac{\bar{o}_{i-1}}{\bar{o}_n}
$$

since they are telescoping using
$$
1- \beta_j = 1 - \frac{\alpha}{\bar{o}_j} = \frac{\bar{o}_j - \alpha}{\bar{o}_j} = \frac{\alpha + (1-\alpha) \bar{o}_{j-1}}{\bar{o}_j} = (1-\alpha)\frac{\bar{o}_{j-1}}{\bar{o}_j}.
$$

This gives the following closed form for $Q_{n+1}$
$$
Q_{n+1} = \frac{\alpha}{\bar{o}_n}\sum_{i=1}^n R_i (1-\alpha)^{n-i}
$$

We can see that the weight given to any reward $R_i$​ decreases exponentially.

## Upper-Confidence-Bound Action Selection

We have the opportunity to introduce a new agent here. The update rule remains the same (I assume sample average), but the action selection is more informed compared to $\varepsilon$-greedy algorithms.

$$
A_t := \mathrm{argmax}_a \left[ Q_t(a) + c \sqrt{ \frac{\ln t}{N_t(a)} }\right]
$$

where $N_t(a)$ is the number of times that has been selected, c > 0 controls the exploration (similar to $\varepsilon$). 

If an action has not been selected even once, i.e., $N_t(a)=0$, then $a$ is considered to be a maximizing action. (In our case, this means that in the first few steps, all actions have to be selected once, and only after that does the UCB action selection kick in.) Btw I have no idea where this comes from, but at least it looks fancy (and reasonable), and we can implement it:

```{python}
# | code-fold: false
# === the ucb bandit agent ===
class UcbBanditAgent:
    def __init__(self, num_actions, c, seed=None):
        self.num_actions = num_actions
        self.c = c  # exploration parameter
        self.reset()
        self.rng = np.random.default_rng(seed)

    def reset(self):
        self.t = 0
        self.Q = np.zeros(self.num_actions, dtype=float)
        self.counts = np.zeros(self.num_actions, dtype=int)

    def act(self, bandit):
        self.t += 1

        # upper-Confidence-Bound Action Selection
        if self.t <= self.num_actions:
            # if not all actions have been tried yet, select an untried action
            action = self._choose_untaken_action()
        else:
            # calculate UCB values for each action
            ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / (self.counts))
            # select the action with the highest UCB value
            action = np.argmax(ucb_values)

        # take action and observe the reward
        reward = bandit.pull_arm(action)

        # update count and value estimate
        self.counts[action] += 1
        self.Q[action] += (reward - self.Q[action]) / self.counts[action]

        return (action, reward)

    def _choose_untaken_action(self):
        return self.rng.choice(np.where(self.counts == 0)[0])
```  

Let's recreate the UCB action selection figure, which we'll need for the next exercise.
```{python}
# | label: fig-bandit-ucb-performance
# | fig-cap: "This is like Figure 2.4 [@sutton2018]: average performance of UCB action selection."
# === ucb agent performance ===
agent_ucb = UcbBanditAgent(num_actions=num_arms, c=2, seed=seed)
agent_ε_greedy = SampleAverageBanditAgent(
    Q1=np.zeros(num_arms, dtype=float), ε=0.1, seed=None
)

agents = [agent_ucb, agent_ε_greedy]

avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    exp_steps=1_000,
    exp_runs=1_000,
    exp_seed=seed,
)

labels = [
    "ucb (c=2, α=0.1)",
    "ε-greedy (ε=0.1, α=0.1)",
]
plot_optimal_action_percent(opt_pct, labels=labels)
plt.tight_layout()
plt.show()
```

### Exercise 2.8: UCB Spikes {.unnumbered}

In @fig-bandit-ucb-performance the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward  ncreases on the 11th step and why it decreases on the subsequent steps. Hint: if $c = 1$, then the spike is less prominent.

#### Solution {.unnumbered}

I think the answer is similar to the answer to Exercise 2.6. The first $10$ steps the UCB algorithm tries out all actions. Then on step $11$ it will select the one that scored highest, which is quite a decent strategy. But then because of the $N_t​(a)$ in the denominator it is back to exploring.

## Gradient Bandit Algorithms

We get a novel method where we learn a numerical preference for each action, denoted as $H_t​(a)$, rather than learning their actual values.

To select an action based on these preferences, we use the softmax distribution:
$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}.
$${#eq-soft-max-distribution}

Shifting all preferences by a constant $C$ doesn't change the action selection:
$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \frac{e^Ce^{H_t(a)}}{\sum_{b=1}^k e^Ce^{H_t(b)}} = \frac{e^{H_t(a)+C}}{\sum_{b=1}^k e^{H_t(b)+C}}
$$

For the gradient bandit algorithm to learn, we update the preferences using the following rule:
$$
\begin{split}
H_{t+1}(a) &:= H_t(a) + \alpha (R_t - \bar{R}_t) (\mathbb{I}_{a = A_t} - \pi_t(a))
\end{split}
$${#eq-gradient-bandit-update}

Now, let's implement this gradient bandit algorithm:
```{python}
# | code-fold: false
# === the gradient agent ===
class GradientBanditAgent:
    def __init__(self, H1, α, seed=None):
        self.num_actions = len(H1) 
        self.α = α 
        self.H1 = np.asarray(H1, dtype=np.float64)  # initial preferences
        self.reset()
        self.rng = np.random.default_rng(seed)

    def reset(self):
        self.H = self.H1.copy()  
        self.avg_reward = 0  
        self.t = 0  # step count

    def act(self, bandit):
        self.t += 1

        # select action using softmax
        action_probs = GradientBanditAgent.softmax(self.H)
        action = self.rng.choice(self.num_actions, p=action_probs)

        # take action and observe the reward
        reward = bandit.pull_arm(action)

        # update average reward
        self.avg_reward += (reward - self.avg_reward) / self.t

        # update action preferences
        advantage = reward - self.avg_reward
        one_hot_action = np.eye(self.num_actions)[action]
        self.H += self.α * advantage * (one_hot_action - action_probs)

        return action, reward

    @staticmethod
    def softmax(x):
        # shift vector by max(x) to avoid hughe numbers.
        # This is basically using the fact that softmax(x) = softmax(x + C)
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
```  

Sutton and Barto [@sutton2018, p. 37] emphasize the importance of the baseline $\bar{R}_t$​ in the update forumla and show how performance drops without it. I guess that the baseline arises from some variance reduction for the estimator of the exact gradient ascent of the expected value. However, I need to verify this.

Here we recreat Figure 2.5 [@sutton2018], but only for the gradient bandit algorithm with baseline:
```{python}
# | label: fig-gardiet-bandit-performance
# | fig-cap: "This is like Figure 2.5 [@sutton2018]: average performance of the gradient bandit algorithm. Except that we only show the variants with baseline."
# === gradinet bandit performance ===
alphas = [0.1, 0.4]
agents = [GradientBanditAgent(H1=np.zeros(num_arms), α=α, seed=seed) for α in alphas]

avg_rwd, opt_pct = bandit_experiment(
    agents,
    bandit_num_arms=num_arms,
    exp_steps=1_000,
    exp_runs=1_000,
    exp_seed=seed,
)

labels = [f"α = {α}" for α in alphas]
plot_optimal_action_percent(opt_pct, labels=labels)
plt.tight_layout()
plt.show()
```

### Exercise 2.9 {.unnumbered}

Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.

#### Solution {.unnumbered}

The logistic function is $\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1+e^x}$.
Map the preferences $H(a_1), H(a_2)$ to $\Delta = H(a_1) - H(a_2)$ then
$$
\pi(a_1) = \frac{e^{H(a_1)}}{e^{H(a_1)} + e^{H(a_2)}} = \frac{e^{H(a_1)-H(a_2)}}{e^{H(a_1)-H(a_2)} + 1} = \sigma(\Delta)
$$
Similarly $\pi(a_2) = \sigma(-\Delta)$.

##  Associative Search (Contextual Bandits)

### Exercise 2.10 {.unnumbered}

Suppose you face a $2$-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions $1$ and $2$ are respectively $0.1$ and $0.2$ with probability $0.5$ (case A), and $0.9$ and $0.8$ with probability $0.5$ (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?

#### Solution {.unnumbered}

We are presented with two scenarios and the questions "What is the best strategy?" and "What is its expected reward?" for each scenario.

In the first scenario, we don't know whether we are facing Case A or Case B at any given time step. The true values of the actions are as follows:
$$
\begin{split}
\mathbb{E}[R_t \mid A_t = 1] &= P(\text{Case A}) \cdot 0.1 + P(\text{Case B}) 0.9\\
 &= 0.5 (0.1 + 0.9) = 0.5\\[3ex]
\mathbb{E}[R_t \mid A_t = 2] &= P(\text{Case A}) \cdot 0.2 + P(\text{Case B}) 0.8\\
&= 0.5 (0.2 + 0.8) = 0.5
\end{split}
$$

Since both actions have the same expected reward of 0.5, it does not matter which action is choosen. Thus any algorithm is optimal and has an expected of 0.5.

In the second scenario, we know whether we are facing Case A or Case B. The expected reward under the optimal strategy, which always chooses the action with the highest expected value, is:
$$
\mathbb{E}_{\pi_*}[ R_t ] = \overbrace{0.5 \cdot 0.2}^{\text{case A}} + \overbrace{0.5 \cdot 0.9}^{\text{case B}} = 0.55
$$

To achieve this expected reward, we need to keep track of the two bandit problems separately and maximise their rewards. How to apprximately do this is the topic of the whole chapter.

## Summary

### Exercise 2.11 (programming) {.unnumbered}

Make a figure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size $\varepsilon$-greedy algorithm with $\alpha$= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.

#### Solution {.unnumbered}

That last exercise is a banger to finish with. There's a lot going on here. I'll explain what was difficult for me at the end, but let’s start with what I actually did and what we can see in the graph.

I ran a parameter sweep for four different bandit agents over 200 episodes of 300,000 steps each. For each episode, I only looked at the average reward over the last 50,000 steps to measure steady-state performance.

```{python}
# | fig-cap: "Parameter sweep for a non-stationary bandit, similar to Figure 2.6 [@sutton2018]. The plot displays the average reward of the last 50,000 steps of a 300,000-step episode, averaged over 200 runs. The bandit was initialized with action-value means set to 10, which drifted according to a normal distribution with a standard deviation of 0.1 at each step. The standard deviation of the reward noise was 1."
# === Parameter sweep for nonstationary bandit ===
"""
We compare 4 bandit agents:
0 - ε-greedy sample-average, parameter = ε
1 - ε-greedy constant α = 0.1, parameter = ε
2 - Gradient ascent with baseline, parameter = α
3 - Gradient ascent no baseline, parameter = α
"""
from functools import partial
from matplotlib.ticker import FuncFormatter

import numpy as np
import matplotlib.pyplot as plt

# custom import
from scripts.parameter_study.episode_mean import episode_mean

# --- Global config
CONFIG = dict(
    seed=1000000,
    num_arms=10,
    steps=300_000,
    keep=50_000,
    runs=200,
    q_0 = 10,
    drift_sd=0.1,
)

AGENTS = {
    "ε in ε-greedy (sample-avg)": dict(
        id=0,
        x_name="ε",
        x_vals=[2**-k for k in (12, 9, 6, 3, 1, 0)],
        fixed=dict(),
    ),
    "ε in ε-greedy (α = 0.1)": dict(
        id=1,
        x_name="ε",
        x_vals=[2**-k for k in (12, 9, 6, 3, 1, 0)],
        fixed=dict(α=0.1),
    ),
    "α in gradient ascent": dict(
        id=2,
        x_name="α",
        x_vals=[2**-k for k in (20, 18, 14, 9, 6, 3, 1)],
        fixed=dict(),
    ),
    "α in gradient ascent (no base)": dict(
        id=3,
        x_name="α",
        x_vals=[2**-k for k in (20, 18, 14, 9, 6, 3, 1)],
        fixed=dict(),
    ),
}


# --- Experiment helper
def evaluate(agent_type: int, **kwargs) -> float:
    """Mean reward over the last *keep* steps averaged across *runs* runs."""
    args = {**CONFIG, **kwargs}
    rng = np.random.default_rng(args["seed"])
    seeds = rng.integers(0, 2_000_000, size=args["runs"])

    rewards = [
        episode_mean(
            agent_type,
            args["num_arms"],
            args["steps"],
            args["keep"],
            args["q_0"],
            1,  # bandit_action_sd
            0,  # drift_mu
            args["drift_sd"],
            seed,
            kwargs.get("ε", 0.1),
            kwargs.get("α", 0.1),
        )
        for seed in seeds
    ]
    return float(np.mean(rewards))


# --- run the sweeps
results = {}
for label, spec in AGENTS.items():
    run = partial(evaluate, spec["id"], **spec["fixed"])
    results[label] = [run(**{spec["x_name"]: x}) for x in spec["x_vals"]]

# --- plot
fig, ax = plt.subplots(figsize=(10, 6))

markers = ["^", "o", "s", "*"]  # one per agent
for (label, spec), marker in zip(AGENTS.items(), markers):
    ax.plot(
        spec["x_vals"],
        results[label],
        marker=marker,
        label=label,
    )

ax.set_xscale("log", base=2)
ax.set_xlabel("Exploration / step-size (log scale)")
ax.set_ylabel(
    f"Average reward for the last {CONFIG['keep']:,} steps ({CONFIG['steps']:,} steps total)"
)
ax.set_title(f"Average reward (mean of {CONFIG['runs']} runs)")
ax.grid(True, which="both", linewidth=0.5)

ax.xaxis.set_major_formatter(
    FuncFormatter(lambda x, _: f"1/{int(1/x)}" if x < 1 else str(int(x)))
)

ax.legend()
fig.tight_layout()

plt.show()
```

The agents I tested were:

- $\varepsilon$-greedy with sample-average action values, sweeping over $\varepsilon$
- $\varepsilon$-greedy with constant step-size (α = 0.1), also sweeping $\varepsilon$
- gradient ascent with a sample-average baseline, sweeping $\alpha$
- gradient ascent with no baseline, again sweeping $\alpha$

To encourage adaptability, changed the drift to 0.1.

Let's go through some observations:

1. constant step-size ε-greedy outperforms the others. Not that surprising.
2. gradient ascent underperforms. That surprised me. I'd assumed the gradient method would be a good fit for a drifting environment. However, it does worse than the sample-average ε-greedy.
3. small α performs best for gradient ascent. This one I don’t have a great explanation for, as very small step sizes typically result in slow learning.
4. also the baseline doesn't matter for verly low α. Again no clue.
5. sample-average ε-greedy agent is better than I expected. I thougt for those stepnumbers it wouldn't be able to keep up with the changing environment anymore.
6. random Behavior for ε = 1. The ε-greedy agents with ε = 1 behaves randomly, as expected. Both agents achieve a reward of 10, which is consistent with random action selection.

So this figure raises quite a bunch of questions I can't answer. I think that makes this execrise really good. And maybe in the future I will be able to provide more insights about these findings.

This was a lot of work. Running 200 episodes at 300,000 steps each is way too slow in pure Python. I had to offload the inner loop into a separate file and used Numba to JIT-compile it, basically rewriting all the algorithms used in this experiment. That's also why I don't want to spend more time trying out gradient ascent with a baseline and constant step-size. Additionally, I can't guarantee that there isn't some subtle bug in the code for the main loop, as I didn't really test all the algorithms.