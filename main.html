<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Notes on Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="main_files/libs/clipboard/clipboard.min.js"></script>
<script src="main_files/libs/quarto-html/quarto.js"></script>
<script src="main_files/libs/quarto-html/popper.min.js"></script>
<script src="main_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="main_files/libs/quarto-html/anchor.min.js"></script>
<link href="main_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="main_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="main_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="main_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="main_files/libs/bootstrap/bootstrap-1bc8a17f135ab3d594c857e9f48e611b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="main_files/libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="main_files/libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Notes on Reinforcement Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="foundations" class="level1 unnumbered">
<h1 class="unnumbered">Foundations</h1>
<p>Given a discrete probability space <span class="math inline">\((\Omega, p)\)</span> and a ranom variable, it’s expectation is <span class="math display">\[
\mathbb{E}[X] := \sum_{\omega \in \Omega} X(\omega) p(\omega)
\]</span></p>
<div id="thm-this-needs-a-name" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (This needs a proper name)</strong></span> The expected value of <span class="math inline">\(X\)</span> can be computed from its probability mass function <span class="math inline">\(p_X(x) := p(X = x)\)</span> via <span class="math display">\[
\mathbb{E}[X] = \sum_{x \in \mathbb{R}} x \cdot p_X(x)
\]</span> (note that <span class="math inline">\(p_X(x) &gt; 0\)</span> for only countabley many <span class="math inline">\(x\)</span>)</p>
</div>
<p>When applying a real valued function <span class="math inline">\(f\)</span> to a random variable we get another random variable and we can compute its expected value by <span class="math inline">\(\mathbb{E}[f(X)] = \sum_{x \in \mathbb{R}} f(x) \cdot p_X(x)\)</span> This is true because <span class="math inline">\(p (f(X) = x) = \sum_{y \in \mathbb{R}} 1_{f(y) = x} \cdot p(X = y)\)</span> and thus <span class="math display">\[
\begin{split}
\mathbb{E}[f(X)] &amp;= \sum_{x \in \mathbb{R}} x \cdot p(f(X) = x) \\
&amp;= \sum_{x \in \mathbb{R}} x \cdot \sum_{y \in \mathbb{R}} 1_{f(y) = x} \cdot p(X = y) \\
&amp;= \sum_{y \in \mathbb{R}} p(X = y) \cdot \sum_{x \in \mathbb{R}} x \cdot 1_{f(y) = x} \\
&amp;= \sum_{y \in \mathbb{R}} p_X(y) \cdot f(y)
\end{split}
\]</span></p>
<section id="strong-law-of-lange-numbers" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="strong-law-of-lange-numbers">Strong law of lange numbers</h2>
<p><span class="math display">\[
\mathrm{Pr}\left( \lim_{n \to \infty} \bar{X}_n = \mu \right) = 1
\]</span></p>
</section>
</section>
<section id="the-reinforcement-learning-problem" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> The Reinforcement Learning Problem</h1>
<section id="reinforcement-learning" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="reinforcement-learning"><span class="header-section-number">1.1</span> Reinforcement Learning</h2>
</section>
<section id="examples" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="examples"><span class="header-section-number">1.2</span> Examples</h2>
</section>
<section id="elements-of-reinforcement-learning" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="elements-of-reinforcement-learning"><span class="header-section-number">1.3</span> Elements of Reinforcement Learning</h2>
</section>
<section id="limitations-and-scope" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="limitations-and-scope"><span class="header-section-number">1.4</span> Limitations and Scope</h2>
</section>
<section id="an-extended-example-tic-tac-toe" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="an-extended-example-tic-tac-toe"><span class="header-section-number">1.5</span> An Extended Example Tic-Tac-Toe</h2>
<section id="exercise-1.1-sef-play" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-1.1-sef-play">Exercise 1.1: Sef-Play</h3>
<p>Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a di↵erent policy for selecting moves?</p>
</section>
</section>
</section>
<section id="multi-arm-bandits" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Multi-arm Bandits</h1>
<section id="a-k-armed-bandit-problem" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-k-armed-bandit-problem"><span class="header-section-number">2.1</span> A <span class="math inline">\(k\)</span>-armed Bandit Problem</h2>
<p>I would like to have an interactive bandit problem. Like 4 buttons and when you click them you can see your reward.</p>
<div id="291359af" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ipywidgets <span class="im">as</span> widgets</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the hidden "success" probabilities for the four arms</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>arm_probabilities <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Label widget to show your reward messages</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>message_label <span class="op">=</span> widgets.Label(value<span class="op">=</span><span class="st">"Pull an arm to see your reward!"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create four buttons, one for each arm</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>buttons <span class="op">=</span> [widgets.Button(description<span class="op">=</span><span class="ss">f"Arm </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> widgets.Text(</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span><span class="st">"Hello World"</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    placeholder<span class="op">=</span><span class="st">"Type something"</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    description<span class="op">=</span><span class="st">"String:"</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    disabled<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the button click handler</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> on_button_click(b):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    arm_index <span class="op">=</span> buttons.index(b)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    reward <span class="op">=</span> np.random.binomial(n<span class="op">=</span><span class="dv">1</span>, p<span class="op">=</span>arm_probabilities[arm_index])</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the label's text instead of printing</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    text.value <span class="op">=</span> <span class="ss">f"You pulled Arm </span><span class="sc">{</span>arm_index<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> and got a reward of </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">."</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Attach the click event to each button</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> btn <span class="kw">in</span> buttons:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    btn.on_click(on_button_click)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the buttons and the label</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>display(widgets.VBox([widgets.HBox(buttons), text]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8e511f6e4a3a4dc89540cb22e6aac333","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="f8aaa52a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ipywidgets <span class="im">as</span> widgets</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>test_label <span class="op">=</span> widgets.Label(<span class="st">"Hello from a label!"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>test_label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"28d45f1b70e84b078a5f15d1726b1cf2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="introduction-to-importance-sampling" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduction-to-importance-sampling">Introduction to importance sampling</h2>
<p>Later for monte carlo methods we need importance sampling. For me the idea is best introduced a bit earlier in the multi-armed bandit scenario.</p>
<p>We have a policy <span class="math inline">\(\pi\)</span> for selecting our action and want to know the expectation of its value <span class="math display">\[v_\pi
= \mathbb{E}[r(A) \mid A \sim \pi]
= \sum_{a \in \mathcal{A}} r(a) \cdot \pi(a)
\]</span> We can get an estimate for <span class="math inline">\(v_\pi\)</span> by sampling a bunch of times according to <span class="math inline">\(\pi\)</span> and get retunrs <span class="math inline">\(R_1,\dots,R_N\)</span> and estimate <span class="math display">\[v_\pi \approx \frac{\sum_i R_i}{N}\]</span></p>
<p>Now assume we are forced to follow a different behaviour policy <span class="math inline">\(b\)</span> but still want to obtain an estimate for <span class="math inline">\(v_\pi\)</span>. If we sample according to <span class="math inline">\(b\)</span> and take the mean we get an estimate for <span class="math inline">\(v_b\)</span>. But we can turn those samples into an estimate for <span class="math inline">\(v_\pi\)</span>. We can express <span class="math inline">\(v_\pi\)</span> as <span class="math display">\[
\begin{align*}
v_\pi &amp;= \sum_{a \in \mathcal{A}} r(a) \cdot \pi(a)  \\
&amp;= \sum_{a \in \mathcal{A}} r(a) \frac{\pi(a)}{b(a)} \cdot b(a) \\
&amp;= \mathbb{E}[r(A) \frac{\pi(A)}{b(A)} \mid A \sim b]
\end{align*}
\]</span> So we can sample according to <span class="math inline">\(b\)</span> list the choosen actions <span class="math inline">\(A_1, \dots, A_n\)</span> and the reward <span class="math inline">\(R_1, \dots, R_n\)</span> and estimate <span class="math display">\[
v_\pi \approx (\sum_i R_i \frac{\pi(A_i)}{b(A_i)}) : N
\]</span></p>
</section>
</section>
<section id="finite-markov-decision-processes" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Finite Markov Decision Processes</h1>
</section>
<section id="dynamic-programming" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Dynamic Programming</h1>
<section id="policy-evaluation" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="policy-evaluation"><span class="header-section-number">4.1</span> Policy Evaluation</h2>
<p>How to get from <span class="math inline">\(\pi\)</span> to <span class="math inline">\(v_\pi\)</span>? Use the Bellman equation to approximate <span class="math inline">\(v_\pi\)</span> from a random guess <span class="math inline">\(v_0\)</span> with the update rule: <span class="math display">\[
v_{k+1}(s) = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \left[ r + \gamma v_k(s')\right]
\]</span></p>
<p>As pseudocode it would look like this</p>
<div id="fig-iterative-policy-evaluation" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-comment-delimiter="//" data-pseudocode-number="1" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{Iterative policy evaluation} \begin{algorithmic} \State Input $\pi$, the policy to be evaluated \State Initialse an array $V(s) = 0$ for all $s \in S$ \REPEAT \state $\Delta \gets 0$ \for{$s \in S$} \state $v \gets V(s)$ \state $V(s) \gets \sum_a \pi(a | s) \sum_{s',r}p(s',r|s,a) [r + \gamma V(s')]$ \state $\Delta \gets \max(\Delta, |v - V(s)|)$ \endfor \UNTIL{$\Delta &lt; \theta$} \RETURN $V \approx v_\pi$ \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="policy-improvement" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="policy-improvement"><span class="header-section-number">4.2</span> Policy Improvement</h2>
<p>Given policy <span class="math inline">\(\pi\)</span> we can create an improved policy <span class="math inline">\(\pi'\)</span> (i.e.&nbsp;<span class="math inline">\(v_\pi(s) \leq v_{\pi'}(s)\)</span>) via <span class="math display">\[\pi'(s) = \mathrm{argmax}_a q_\pi(s,a).\]</span></p>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="cell-fig-gridworld" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">12</span>, <span class="dv">13</span>, <span class="dv">14</span>, <span class="dv">0</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [<span class="dv">0</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">22</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">18</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">18</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">22</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">14</span>, <span class="dv">0</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_grid(ax, labels, title):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Draws a 4×4 grid with the given labels."""</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        ax.plot([<span class="dv">0</span>, <span class="dv">4</span>], [x, x], <span class="st">"k-"</span>)  <span class="co"># Horizontal lines</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        ax.plot([x, x], [<span class="dv">0</span>, <span class="dv">4</span>], <span class="st">"k-"</span>)  <span class="co"># Vertical lines</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(labels):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        row, col <span class="op">=</span> <span class="bu">divmod</span>(i, <span class="dv">4</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        ax.text(col <span class="op">+</span> <span class="fl">0.5</span>, <span class="fl">3.5</span> <span class="op">-</span> row, <span class="bu">str</span>(label), ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_gridworld():</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    draw_grid(axes[<span class="dv">0</span>], states, <span class="st">"States"</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    draw_grid(axes[<span class="dv">1</span>], values, <span class="st">"Values"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>draw_gridworld()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-gridworld" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gridworld-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_files/figure-html/fig-gridworld-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gridworld-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: left 4×4 Gridworld states, right values for the equiprobable random policy
</figcaption>
</figure>
</div>
</div>
</div>
<section id="exercise-4.1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-4.1">Exercise 4.1</h3>
<p>If <span class="math inline">\(\pi\)</span> is the equiprobable random policy, what is <span class="math inline">\(q_\pi(11, \mathrm{down})\)</span> (see <a href="#fig-gridworld" class="quarto-xref">Figure&nbsp;1</a>)? What is <span class="math inline">\(q_\pi(7, \mathrm{down})\)</span>?</p>
<section id="solution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution">Solution</h4>
<p>Clearly <span class="math inline">\(q_\pi(11, \mathrm{down}) = -1\)</span> for any policy, since this action terminates the episode.</p>
<p>For <span class="math inline">\(q_\pi(7, \mathrm{down})\)</span> we’ll use the fact that <span class="math inline">\(v_\pi(11, \mathrm{down}) = -14\)</span> (see Figure 4.2 in the book). So <span class="math inline">\(q_\pi(7, \mathrm{down}) = -1 + -14 = -15\)</span>.</p>
</section>
</section>
<section id="exercise-4.2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-4.2">Exercise 4.2</h3>
<p>Suppose a new state 15 is added to the gridworld (<a href="#fig-gridworld" class="quarto-xref">Figure&nbsp;1</a>) just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is <span class="math inline">\(v_\pi(15)\)</span> for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is <span class="math inline">\(v_π(15)\)</span> for the equiprobable random policy in this case?</p>
<section id="solution-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-1">Solution</h4>
<p>For the first case the transitions of the old cells don’t change so their values also stay the same. So we can solve this equation for the value of cell number <span class="math inline">\(15\)</span>. <span class="math display">\[
\begin{split}
v_\pi(15) &amp;= \sum_a \pi(a\mid s) \sum_{s',r} p(s',r \mid s,a) [r + \gamma v_\pi(s')]\\
&amp;= \frac{1}{4} [-1 + v_\pi(12)] + \frac{1}{4} [-1 + v_\pi(13)] \frac{1}{4} [-1 + v_\pi(14)] + \frac{1}{4} [-1 + v_\pi(15)]\\
&amp;= \frac{1}{4} [-1 + -22] + \frac{1}{4} [-1 + -20] \frac{1}{4} [-1 + -14] + \frac{1}{4} [-1 + v_\pi(15)]
\end{split}
\]</span> Thus <span class="math inline">\(v_{\pi}(15) = (-1 -22 - 1 - 20 - 1 - 14 - 1) : 3\)</span> which is exactly 20.</p>
<p>In the second case transition of state <span class="math inline">\(13\)</span> changes. This usually changes the value and infact it changes the behaviour, e.g.&nbsp;starting at <span class="math inline">\(13\)</span> the sequen down, up, up, up, left resulted in the terminal state in the original grid world but in the new one it results in state 4.</p>
<p>What I did first, was to write a full policy evaluation. Use the values we had for the states so far (the first case) and see what happens.</p>
<p>However, there is a clever trick that let’s you skip most of the work. When we do policy evaluation we update the approximate values of all states towards their true values. If we take the value function from the first case <span class="math inline">\(v_{\mathrm{old}}\)</span>, we actually know that policy evaluation will only change 13 in the first sweep (only state whose transitions has changed), (and then these changes would spread to other states in further sweeps, but this doesn’t happpen as we will see.)</p>
<p>Let’s compute it then: <span class="math display">\[
v_{\mathrm{new}}(13) = -1 + \frac{1}{4} [v_\mathrm{old}(9) + v_\mathrm{old}(12) + v_\mathrm{old}(15) + v_\mathrm{old}(14)]
\]</span> This is just <span class="math inline">\(v_{\mathrm{old}}(15)\)</span> as (coincidently) <span class="math inline">\(v_{\mathrm{old}}(13) = v_{\mathrm{old}}(15)\)</span> and <span class="math display">\[
v_{\mathrm{old}}(13) = -1 + \frac{1}{4} [v_\mathrm{old}(9) + v_\mathrm{old}(12) + v_\mathrm{old}(13) + v_\mathrm{old}(14)]
\]</span></p>
<p>So the the value function for the first case is already the value function for the second case.</p>
<p>I didn’t realize that and thought because the dynamics of the grid world has changed my only option is to do a full policy evaluation. I don’t want to throw away all that work so here is the code for this</p>
<div id="d59138c1" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">12</span>, <span class="dv">13</span>, <span class="dv">14</span>, <span class="dv">15</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [<span class="dv">0</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">22</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">18</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">18</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">22</span>, <span class="op">-</span><span class="dv">20</span>, <span class="op">-</span><span class="dv">14</span>, <span class="op">-</span><span class="dv">20</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neighbours(state):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># up</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        up <span class="op">=</span> state</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> state <span class="op">==</span> <span class="dv">15</span>:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        up <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        up <span class="op">=</span> state <span class="op">-</span> <span class="dv">4</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># down</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="kw">in</span> [<span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">15</span>]:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        down <span class="op">=</span> state</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> state <span class="op">==</span> <span class="dv">11</span>:</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        down <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> state <span class="op">==</span> <span class="dv">13</span>:</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        down <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        down <span class="op">=</span> state <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># left</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="kw">in</span> [<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">12</span>]:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        left <span class="op">=</span> state</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> state <span class="op">==</span> <span class="dv">15</span>:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        left <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        left <span class="op">=</span> state <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># right</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="kw">in</span> [<span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">11</span>]:</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> state</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> state <span class="op">==</span> <span class="dv">15</span>:</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> state <span class="op">==</span> <span class="dv">14</span>:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> state <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> up, down, left, right</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterative value update process</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>sweeps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Track the largest change</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    sweeps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">16</span>):  <span class="co"># Skip state 0</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        old_value <span class="op">=</span> values[state]</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># full backup</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        up, down, left, right <span class="op">=</span> neighbours(state)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        values[state] <span class="op">=</span> (</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>            values[up] <span class="op">+</span> values[down] <span class="op">+</span> values[left] <span class="op">+</span> values[right]</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        ) <span class="op">/</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> <span class="bu">max</span>(delta, <span class="bu">abs</span>(old_value <span class="op">-</span> values[state]))</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> delta <span class="op">&lt;</span> <span class="fl">0.01</span>:  <span class="co"># Convergence criterion</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"new values after sweeps"</span>, sweeps)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>new values after sweeps 1
[0, -14.0, -20.0, -22.0, -14.0, -18.0, -20.0, -20.0, -20.0, -20.0, -18.0, -14.0, -22.0, -20.0, -14.0, -20.0]</code></pre>
</div>
</div>
<p>So alone the fact that we only needed one sweeps means that the initial value function was already correct.</p>
</section>
</section>
<section id="exercise-4.3" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-4.3">Exercise 4.3</h3>
<p>This exercise references these equations: Formulas for policy evaluation <span id="eq-bellman-expectation-for-value-function"><span class="math display">\[
v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{\pi}(S_t+1) \mid S_t = s]
\tag{1}\]</span></span></p>
<p><span id="eq-bellman-equation-for-value-function"><span class="math display">\[
v_\pi(s) = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) [r + \gamma v_\pi(s')]
\tag{2}\]</span></span></p>
<p><span id="eq-update-rule-for-value-function"><span class="math display">\[
\begin{split}
v_{k+1}(s) &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]\\
    &amp;= \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) [r + \gamma v_k(s')]
\end{split}
\tag{3}\]</span></span></p>
<p>What are the equations analogous to <a href="#eq-bellman-expectation-for-value-function" class="quarto-xref">Equation&nbsp;1</a>, <a href="#eq-bellman-equation-for-value-function" class="quarto-xref">Equation&nbsp;2</a>, and <a href="#eq-update-rule-for-value-function" class="quarto-xref">Equation&nbsp;3</a> for the action-value function qπ and its successive approximation by a sequence of functions q0, q1, q2, . . . ?</p>
<section id="solution-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-2">Solution</h4>
<p><span class="math display">\[
\begin{split}
q_\pi(s,a) &amp;= \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \;\middle|\; S_t = s, A_t = a \right] \\
&amp;= \mathbb{E}_\pi\left[ R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k R_{t+k+2} \;\middle|\; S_t = s, A_t = a \right] \\
&amp;= \mathbb{E}_\pi\left[ R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \;\middle|\; S_t = s, A_t = a \right] \\
&amp;= \sum_{s',r} p(s',r|s,a) [r + \gamma\sum_{a'} \pi(a'|s') q_\pi(s',a')]
\end{split}
\]</span> And for the update rule: <span class="math display">\[
\begin{split}
q_{k+1}(s,a) &amp;= \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma q_k(S_{t+1}, A_{t+1}) \;\middle|\; S_t = s, A_t = a \right] \\
&amp;= \sum_{s',r} p(s',r|s,a) [r + \gamma\sum_{a'} \pi(a'|s') q_k(s',a')]
\end{split}
\]</span></p>
</section>
</section>
<section id="exercise-4.4" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-4.4">Exercise 4.4</h3>
<p>In some undiscounted episodic tasks there may be policies for which eventual termination is not guaranteed. For example, in the grid problem above it is possible to go back and forth between two states forever. In a task that is otherwise perfectly sensible, <span class="math inline">\(v_\pi(s)\)</span> may be negative infinity for some policies and states, in which case the algorithm for iterative policy evaluation given in Figure 4.1 will not terminate. As a purely practical matter, how might we amend this algorithm to assure termination even in this case? Assume that eventual termination is guaranteed under the optimal policy.</p>
<section id="solution-3" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-3">Solution</h4>
<p>I really don’t know how to solve this exercise. I think we can’t really solve this for iterative policy evaluation. And what would it help for us that the eventual termination is guarnateed under the optimal policy, that doesn’t affect the current non-optimal policy.</p>
<p>Actually I can add another nearly bug. Imagine in state <span class="math inline">\(s\)</span> the policy says deterministically stay in <span class="math inline">\(s\)</span> and that the reward for this action would be <span class="math inline">\(0\)</span>. The value of this state under this policy should be <span class="math inline">\(0\)</span>. The iterative policy evaluation will leave the starting <span class="math inline">\(v(s)\)</span> unchanged, so it’s important that it is initialized as <span class="math inline">\(0\)</span>.</p>
</section>
</section>
<section id="exercise-4.5" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-4.5">Exercise 4.5</h3>
<p>Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem. If your computer is too slow for the full problem, cut all the numbers of cars in half.</p>
<section id="solution-4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-4">Solution</h4>
<p>There is a lot to unpack here. Let’s write the constraints down again:</p>
<ul>
<li>two locations A and B</li>
<li>each day Pois(3) cars are requested and Pois(3) cars are returned at A</li>
<li>each day Pois(4) cars are requested and Pois(2) are returned at B</li>
<li>if more than 10 cars are at a location it costs $4</li>
<li>there can be at most 20 cars at a location</li>
<li>any additional cars disappear from the problem</li>
<li>a maximum of 5 cars can be moved over night</li>
<li>this cost $2 per car (except for one car from A to B)</li>
<li>this is a continuing finite MDP with <span class="math inline">\(\gamma = 0.9\)</span></li>
<li>x number of customers arrive at A and B</li>
<li>if there is a car they give 10$</li>
<li>cars become available the day after they are returned</li>
<li>cars can be moved overnight at a cost of $2 per car</li>
</ul>
<p>instead of <span class="math display">\[
\sum_{s',r} p(s',r|s,a) [r + \gamma v(s')]
\]</span> we’ll be using <span class="math display">\[
\sum_{\omega \in \Omega} p(\omega) [r(s,a,\omega) + \gamma v(t(s,a,\omega))]
\]</span></p>
</section>
</section>
<section id="exercise-4.6" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-4.6">Exercise 4.6</h3>
<p>How would policy iteration be defined for action values? Give a complete algorithm for computing <span class="math inline">\(q_*\)</span>, analogous to Figure 4.3 for computing <span class="math inline">\(v_*\)</span>. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.</p>
<section id="solution-5" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-5">Solution</h4>
<p>Alright. Here is some more text now Zla bla</p>
</section>
</section>
</section>
</section>
<section id="monte-clarlo-methods" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Monte Clarlo Methods</h1>
<section id="monte-carlo-prediction" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="monte-carlo-prediction"><span class="header-section-number">5.1</span> Monte Carlo Prediction</h2>
<section id="exercise-5.1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.1">Exercise 5.1</h3>
<p>Consider the diagrams on the right in Figure 5.1. Why does the estimated value function jump up for the last two rows in the rear? Why does it drop off for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?</p>
<section id="solution-6" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-6">Solution</h4>
<p>They jump up because the last two rows are 20, 21 which are stick for that policy. Apparently the probability of the dealer beating those numbers are pretty slim.</p>
<p>The last row on the left is a dealer ace which is a versatile card to have.</p>
<p>Equally having a usable ace is pretty useful as a player which gives better probabilities for the front rows of the upper diagram.</p>
</section>
</section>
<section id="exercise-5.2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.2">Exercise 5.2</h3>
<p>Suppose every-visit MC was used instead of first-visit MC on the blackjack task. Would you expect the results to be very different? Why or why not?</p>
<section id="solution-7" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-7">Solution</h4>
<p>Is it possible to have cycles in blackjack? I think it’s impossible, so both methods behave the same.</p>
</section>
</section>
</section>
<section id="monte-carlo-estimation-of-action-values" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="monte-carlo-estimation-of-action-values"><span class="header-section-number">5.2</span> Monte Carlo Estimation of Action Values</h2>
<section id="exercise-5.3" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.3">Exercise 5.3</h3>
<p>What is the backup diagram for Monte Carlo estimation of <span class="math inline">\(q_\pi\)</span>?</p>
<section id="solution-8" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-8">Solution</h4>
<p>Pretty much the same as for <span class="math inline">\(v_\pi\)</span> but it starts at an state value pair <span class="math inline">\(s,a\)</span>.</p>
</section>
</section>
</section>
<section id="monte-carlo-control" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="monte-carlo-control"><span class="header-section-number">5.3</span> Monte Carlo Control</h2>
<section id="exercise-5.4" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.4">Exercise 5.4</h3>
<p>The pseudocode for Monte Carlo ES (exproling starts) is inefficient because, for each state–action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more efficient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state–action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this</p>
<section id="solution-9" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-9">Solution</h4>
<p>That’s a throwback to 2.4. So we basically just do what the exercise asks us to. For each state-action pair <span class="math inline">\((s,a)\)</span> we maintain a count <span class="math inline">\(N(s,a)\)</span> and <span class="math inline">\(Q(s,a)\)</span>. Where <span class="math inline">\(Q(s,a) = \frac{\sum_i R_i}{N}\)</span> is the current mean. We update them when for <span class="math inline">\(G\)</span> as follows <span class="math display">\[Q(s,a) \gets (N(s,a)Q(s,a) + G):(N(s,a)+1) = \]</span> and <span class="math inline">\(N(s,a) \gets N(s,a) + 1\)</span>. …Hm. Something about the indices here isn’t quite right.</p>
</section>
</section>
</section>
<section id="monte-carlo-control-without-exploring-starts" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="monte-carlo-control-without-exploring-starts"><span class="header-section-number">5.4</span> Monte Carlo Control without Exploring Starts</h2>
</section>
<section id="off-policy-prediction-via-importance-sampling" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="off-policy-prediction-via-importance-sampling"><span class="header-section-number">5.5</span> Off-policy Prediction via Importance Sampling</h2>
<section id="a-refresher-on-measure-theory" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="a-refresher-on-measure-theory"><span class="header-section-number">5.5.1</span> A refresher on measure theory</h3>
<p>The expectation of <span class="math inline">\(X\)</span> is <span class="math display">\[
\mathbb{E}[X]  =  \int_\Omega (X(ω)) \;\mathrm{d}P(ω).
\]</span> Define a new measure <span class="math inline">\(\mu_X\)</span>​ on the real line <span class="math inline">\(\mathbb{R}\)</span> by <span class="math display">\[ \mu_X(A)  :=  P(\{X(ω) \in A\}) \]</span> for any Borel set <span class="math inline">\(A \subseteq \mathbb{R}\)</span>.</p>
<p>This measure <span class="math inline">\(\mu_X\)</span>​ is called the distribution measure or pushforward measure of <span class="math inline">\(X\)</span>.</p>
<p>A fundamental result in measure theory states that</p>
<p><span class="math display">\[
\int_\Omega X(w) \mathrm{d}P(w) = \int_{\mathbb{R}} X \mathrm{d}\mu_X(x).
\]</span></p>
</section>
<section id="a-refresher-on-estimators." class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="a-refresher-on-estimators."><span class="header-section-number">5.5.2</span> A refresher on estimators.</h3>
</section>
<section id="introducing-importance-sampling" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="introducing-importance-sampling"><span class="header-section-number">5.5.3</span> Introducing Importance Sampling</h3>
<p>Suppose we want to evaluate a policy <span class="math inline">\(\pi\)</span> for a bandit. The exact value of <span class="math inline">\(\pi\)</span> is <span class="math display">\[v_\pi = \mathbb{E}_\pi[R] = \sum_{a \in \mathcal{A}} \pi(a)r(a)\]</span> where <span class="math inline">\(r(a)\)</span> is the expected value of picking <span class="math inline">\(a\)</span>. We can get an estimate for <span class="math inline">\(v_\pi\)</span> by sampling some rewards <span class="math inline">\(R_1, \dots, R_n\)</span> and we get <span class="math inline">\(v_\pi \approx \frac{\sum_i R_i}{N}\)</span>.</p>
<p>Now we want to extract the give an estimate for <span class="math inline">\(v_\pi\)</span> using a different behaviour policy <span class="math inline">\(b\)</span> for the samples.</p>
<p>Using <span class="math inline">\(b\)</span> we can get samples <span class="math inline">\(A_1,R_1, \dots , A_n, R_n\)</span> of actions and rewards. Now to get an estimator for <span class="math inline">\(v_\pi\)</span> we can use the following <span class="math display">\[
\begin{split}
\mathbb{E}_\pi[R] &amp;= \sum_{a \in \mathcal{A}} \pi(a) r(a) \\
&amp;= \sum_{a \in \mathcal{A}} \frac{\pi(a)}{b(a)} r(a) b(a) \\
&amp;= \mathbb{E}[\frac{\pi(A)}{b(A)} R \mid A \sim b]
\end{split}
\]</span> This relative probability <span class="math inline">\(\rho_t := \pi(A_t): b(A_t)\)</span> of the target and behaviour policies is called the importance-sampling ratio. Now we can simply estimate <span class="math inline">\(v_\pi\)</span> by ordinary importance sampling</p>
<p><span class="math display">\[
v_\pi \approx \frac{\sum_{t}\rho_t R_t}{N}
\]</span> or by the weighted importance sampling <span class="math display">\[
v_\pi \approx \frac{\sum_{t}\rho_t R_t}{\sum \rho_t}.
\]</span></p>
<p>Ordinary importance sampling is unbiased, since <span class="math display">\[
\mathbb{E}_b[\rho_t R_t] = \mathbb{E}_b [\frac{\pi(A_t)}{b(A_t)}R_t] = \sum_{a \in \mathcal{A}}\frac{\pi(a)}{b(a)}r(a) b(a) = \sum_{\mathcal{A}} \pi(a)r(a) = v_\pi
\]</span></p>
<p>Weighted importance sampling is biased but mean converges to <span class="math inline">\(v_\pi\)</span>. We note that <span class="math inline">\(\mathbb{E}_b\left[ \frac{1}{N}\sum_t \rho_t R_t \mid A_t \sim b \right] = v_\pi\)</span> and <span class="math inline">\(\mathbb{E}_b\left[ \frac{1}{N}\sum_t \rho_t \mid A_t \sim b \right] = 1\)</span> Thus <span class="math display">\[
\frac{ \frac{1}{N}\sum_t \rho_t R_t}{\frac{1}{N}\sum_t \rho_t} \mid A_t \sim b  \overset{a.s.}{\to} v_\pi
\]</span> And from that we get <span class="math display">\[
\lim_{N \to \infty}  \mathbb{E}_b\left[ \frac{ \frac{1}{N}\sum_t \rho_t R_t}{\frac{1}{N}\sum_t \rho_t} \mid A_t \sim b \right]
=   \mathbb{E}_b\left[  \lim_{N \to \infty}\frac{ \frac{1}{N}\sum_t \rho_t R_t}{\frac{1}{N}\sum_t \rho_t} \mid A_t \sim b \right]
= v_\pi
\]</span></p>
</section>
<section id="exercise-5.5" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.5">Exercise 5.5</h3>
<p>Consider an MDP with a single nonterminal state and a single action that transitions back to the nonterminal state with probability <span class="math inline">\(p\)</span> and transitions to the terminal state with probability <span class="math inline">\(1-p\)</span>. Let the reward be +1 on all transitions, and let <span class="math inline">\(\gamma = 1\)</span>. Suppose you observe one episode that lasts 10 steps, with a return of 10. What are the first-visit and every-visit estimators of the value of the nonterminal state?</p>
<section id="solution-10" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-10">Solution</h4>
<p>The estimators It should be <span class="math inline">\(10\)</span> for first visit as the final reward was 10 after seeing <span class="math inline">\(s\)</span> for the first time. And 5.5 for every-visit because the estimate is <span class="math inline">\((10 + 9 + \dots + 1): 10 = \frac{11\cdot10}{2 \cdot 10} = 5.5\)</span>.</p>
<p>First let’s try to figure out some exact values. The exact value of <span class="math inline">\(s\)</span> is <span class="math display">\[
v(s) = 1 \cdot p + 2 \cdot (1-p) \cdot p + \dots = p \sum_{k=0} (k+1)(1-p)^k
= \dots = \frac{1}{p}
\]</span> Using first visit estimator for one observation has the same expected value. And using every-visit we should get <span class="math display">\[
\mathbb{E}(X) = 1 \cdot p + \frac{1 + 2}{2} (1-p) \cdot p + \dots
= p \sum_{k=0} \frac{k \cdot (k+1)}{2 \cdot k} (1-p)^k
= \frac{1}{2p}
\]</span></p>
</section>
</section>
<section id="example-5.5-infinite-variance" class="level3" data-number="5.5.4">
<h3 data-number="5.5.4" class="anchored" data-anchor-id="example-5.5-infinite-variance"><span class="header-section-number">5.5.4</span> Example 5.5: Infinite Variance</h3>
<p><span class="math display">\[
\begin{split}
\mathcal{S} = \{s, T\}, \mathcal{A}(s) = \{\mathrm{left}, \mathrm{right}\} \\
p(s, 0 | s, left) = 0.9, p(T,1 | s, left ) = 0.1, p(T,0,| s, \mathrm{right}) = 1
\end{split}
\]</span> Target policy: always selects left Behaviour policy: equiprobable policy</p>
<p>Let’s run one simulation as Sutton-Barto did</p>
<div id="a2952c6e" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define actions and policies</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="st">"left"</span>, <span class="st">"right"</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>target_policy <span class="op">=</span> {<span class="st">"left"</span>: <span class="fl">1.0</span>, <span class="st">"right"</span>: <span class="fl">0.0</span>}</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>behaviour_policy <span class="op">=</span> {<span class="st">"left"</span>: <span class="fl">0.5</span>, <span class="st">"right"</span>: <span class="fl">0.5</span>}</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate sampling ratios for ordinary importance sampling</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>sampling_ratio <span class="op">=</span> {</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    action: target_policy[action] <span class="op">/</span> behaviour_policy[action] <span class="cf">for</span> action <span class="kw">in</span> actions</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of episodes</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>num_episodes <span class="op">=</span> <span class="dv">1_000_000</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>estimated_value <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>estimates <span class="op">=</span> []</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_episodes <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Importance sampling ratio for this episode</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    imp_samp_ratio <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> behaviour_policy[<span class="st">"left"</span>]:</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>            imp_samp_ratio <span class="op">*=</span> sampling_ratio[<span class="st">"left"</span>]</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="fl">0.1</span>:</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>                reward, episode_over <span class="op">=</span> (<span class="dv">1</span>, <span class="va">True</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>                reward, episode_over <span class="op">=</span> (<span class="dv">0</span>, <span class="va">False</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>            imp_samp_ratio <span class="op">*=</span> sampling_ratio[<span class="st">"right"</span>]</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>            reward, episode_over <span class="op">=</span> (<span class="dv">0</span>, <span class="va">False</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> episode_over:</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            scaled_return <span class="op">=</span> reward <span class="op">*</span> imp_samp_ratio</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>            estimated_value <span class="op">+=</span> (scaled_return <span class="op">-</span> estimated_value) <span class="op">/</span> i</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>            estimates.append(estimated_value)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, num_episodes <span class="op">+</span> <span class="dv">1</span>), estimates)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.ylim(0, 2)</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="fl">1.0</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">"log"</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes (log scale)"</span>)</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Estimated Value"</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Ordinary Importance Sampling Estimates"</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So this estimator is just bad (very slowely converging). I don’t quite understand what infinite variance means practically. My thought about this problem is the following:</p>
<ul>
<li>the denominator grows even when the episode doesn’t count for the numerator -&gt; this pulls the curve down</li>
<li>for long episodes where <span class="math inline">\(\rho\)</span> grows exponentially big -&gt; this makes the curve jump up</li>
</ul>
</section>
<section id="exercise-5.6" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.6">Exercise 5.6</h3>
<p>What is the equation analogous to <span class="math display">\[
V(s) := \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(T)-1} G_t}{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(T)-1}}
\]</span> for action values <span class="math inline">\(Q(s, a)\)</span> instead of state values <span class="math inline">\(V(s)\)</span>, again given returns generated using <span class="math inline">\(b\)</span>?</p>
<section id="solution-11" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-11">Solution</h4>
<p>If we would do normal on-policy sampling we had <span class="math display">\[
Q(s,a) = \frac{\sum_{t \in \mathcal{T}(s,a)} G_t}{|\mathcal{T}(s,a)|}
\]</span> where <span class="math inline">\(\mathcal{T}(s,a)\)</span> is all the (first-visit) times s.t. <span class="math inline">\(S_t = s, A_t = a\)</span>.</p>
<p>Now for off-policy sampling we can just use the original formula and replace <span class="math inline">\(\mathcal{T}(s)\)</span> with <span class="math inline">\(\mathcal{T}(s,a)\)</span>. Nearly, we should also exclude the first action of <span class="math inline">\(\rho\)</span> because we don’t want to scale by the probability ratio for <span class="math inline">\(a\)</span>: <span class="math display">\[
Q(s,a) = \frac{\sum_{t \in \mathcal{T}(s,a)} \rho_{t+1:T(t)}G_t}{\sum_{t \in \mathcal{T}(s,a)} \rho_{t+1:T(t)}}
\]</span> I’m pretty sure it should be <span class="math inline">\(\rho_{t+1:T(t)-1}\)</span> instead of <span class="math inline">\(\rho_{t:T(t)-1}\)</span>. Imagine that <span class="math inline">\(a\)</span> deterministically leads to the terminal state. Then We don’t want any scaling but we would get it when <span class="math inline">\(t\)</span> was included in <span class="math inline">\(\rho\)</span>.</p>
</section>
</section>
<section id="exercise-5.7" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.7">Exercise 5.7</h3>
<p>In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?</p>
<section id="solution-12" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-12">Solution</h4>
<p>Could be that because weighted importance-sampling is biased. But I don’t know.</p>
</section>
</section>
<section id="exercise-5.8" class="level3" data-number="5.5.5">
<h3 data-number="5.5.5" class="anchored" data-anchor-id="exercise-5.8"><span class="header-section-number">5.5.5</span> Exercise 5.8</h3>
<p>The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?</p>
<section id="solution-13" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-13">Solution</h4>
<p>If we use every-visit MC we can recalculate <span class="math display">\[
\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t
\]</span> <span class="math display">\[
\mathbb{E}_b\left[ \left(\prod_{t=0}^{T-1}\frac{\pi(A_t\mid S_t)}{b(A_t \mid S_t)} G_0 \right)^2 \right]
\]</span> and see if it’s finite.</p>
<p>We also break it down in episodes. Let’s note that the ratio for left is <span class="math inline">\(\frac{1}{0.5} = 2\)</span> and for right <span class="math inline">\(\frac{0}{0.5} = 0\)</span> Episodes of length <span class="math inline">\(1\)</span> have a probability of <span class="math inline">\(0.5 \cdot 0.1\)</span> and we get as scaled reward <span class="math inline">\(2\)</span>.</p>
<p>For Episodes of length <span class="math inline">\(2\)</span> we get a probability of <span class="math inline">\((0.5 \cdot 0.9) \cdot (0.5 \cdot 0.1)\)</span> and we get scaled rewards of <span class="math inline">\(2*2\)</span> and <span class="math inline">\(2\)</span>.</p>
<p>I think in general we have <span class="math inline">\((0.5 \cdot 0.9)^k \cdot (0.5 \cdot 0.1)\)</span> and we get rewards <span class="math inline">\(\sum_{i=1}^k 2^i = 2^{k+1}-1\)</span></p>
<p>So we should get <span class="math display">\[
\begin{split}
\mathbb{E}_b\left[ \left(
    \sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t
\right)^2 \right] &amp;= \sum_{k=1}^\infty 0.05 \cdot 0.45^k  (2^{k+1}-1)^2 \\
&amp;&gt; 0.05 \sum 0.45^k \cdot 4^{k}  \\
&amp;= 0.05 \sum 1.8^k = \infty
\end{split}
\]</span> So it’s still infitine. We should try it out though :D</p>
</section>
</section>
<section id="exercise-5.9" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.9">Exercise 5.9</h3>
<p>Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample averages described in Section 2.4.</p>
</section>
<section id="exercise-5.10" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.10">Exercise 5.10</h3>
<p>Derive the weighted-average update rule (5.8) from (5.7). Follow the pattern of the derivation of the unweighted rule (2.3)</p>
<p><span class="math display">\[
\begin{split}
V_{n+1} &amp;= \frac{\sum_{k=1}^{n} W_k G_k}{\sum_{k=1}^{n}{W_k}} \\
&amp;= \frac{W_n G_n + (\sum_{k=1}^{n}W_k - W_n) V_n}{\sum_{k=1}^{n}{W_k}} \\
&amp;= V_n + \frac{W_n}{\sum_{k=1}^n W_k} [G_n - V_n]
\end{split}
\]</span></p>
</section>
<section id="exercise-5.12-racetrack-programming" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-5.12-racetrack-programming">Exercise 5.12: Racetrack (programming)</h3>
<p>Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track.</p>
<p>In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, 0, -1 in each step, for a total of nine (<span class="math inline">\(3 \times 3\)</span>) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line.</p>
<p>Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are <span class="math inline">\(-1\)</span> for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability <span class="math inline">\(0.1\)</span> at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).</p>
<section id="solution-14" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-14">Solution</h4>
<p>I think that the easiest and quickest way of solving this exercise is to use Mone Carlo ES (Exploring starts). It might be more instructive to use <span class="math inline">\(\epsilon\)</span>-soft policies or even Off-policy MC control. But <span class="math inline">\(\epsilon\)</span>-soft policies are kind of undesirable and Off-policy MC control looks kind of bad for long episodes like here.</p>
<p>However, it states in the exercise that each episode begins in one of the randomly selected start states. So maybe I shouldn’t use expolring starts.</p>
<p>So I implementet some solution in exercise 6.12 but no proper analysis</p>
<p>This has actually turned out to be quite a challenging exercise. Making a the learning algorithm work is tricky because every minor bug can screw up your learning in weird ways and finding the root cause is not that straight forward.</p>
<p>I found Monte Carlo with exploring start to be really inefficient. I think the problem here is that the space of all states is managable memory vise but not computationally vise. Exploring states that crash with highest velocity into a wall is not actually needed.</p>
<p>Also once the policy has converged on some solution it just changes it’s behaviour if a reachable state gets randomly selected.</p>
<p>Also I think the average return is bad for very long learning. When the learning is rather slow averaging makes the learning even slower.</p>
<p>So, do not recommend.</p>
</section>
</section>
</section>
</section>
<section id="temporal-difference-learning" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Temporal-Difference Learning</h1>
<section id="td-prediction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="td-prediction"><span class="header-section-number">6.1</span> TD prediction</h2>
<section id="exercise-6.1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.1">Exercise 6.1</h3>
<p>If <span class="math inline">\(V\)</span> changes during the episode, then (6.6) only holds approximately; what would the difference be between the two sides? Let <span class="math inline">\(V_t\)</span> denote the array of state values used at time t in the TD error (6.5) <span class="math display">\[
\delta_t := R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)
\]</span> and in the TD update (6.2) <span class="math display">\[
V_{t+1}(S_t) \gets V_t(S_t) + \alpha  \left[ R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\right].
\]</span> Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error <span class="math display">\[
\begin{split}
G_t - V_t(S_t) &amp;= R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_t(S_{t+1}) - \gamma V_t(S_{t+1}) \\
&amp;= \delta_t + \gamma (G_{t+1} -  V_t(S_{t+1})) \\
&amp;= \delta_t + \gamma (G_{t+1} - V_{t+1}(S_{t+1}) + V_{t+1}(S_{t+1}) - V_t(S_{t+1})) \\
&amp;= \delta_t + \gamma (G_{t+1} - V_{t+1}(S_{t+1}) + \alpha \delta_t)
\end{split}
\]</span></p>
</section>
</section>
<section id="advantages-of-td-prediction-methods" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="advantages-of-td-prediction-methods"><span class="header-section-number">6.2</span> Advantages of TD prediction methods</h2>
<p>We define RandomWalk, TemporalDifference0 and Monte Carlo</p>
<div id="58fc943a" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statistics</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ----- Environment -----</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomWalk:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, seed  <span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        random.seed(seed)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.terminal_left <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.terminal_right <span class="op">=</span> size <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.starting_state <span class="op">=</span> math.ceil(size <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.true_values <span class="op">=</span> <span class="va">self</span>.compute_true_values()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_nonterminal_states(<span class="va">self</span>):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="va">self</span>.size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_init_eval(<span class="va">self</span>):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Return an initial dictionary of state-value estimates,</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">        with terminal states = 0 and all others = 0.5</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        eval_dict <span class="op">=</span> {}</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        eval_dict[<span class="va">self</span>.terminal_left] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        eval_dict[<span class="va">self</span>.terminal_right] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="va">self</span>.get_nonterminal_states():</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>            eval_dict[i] <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> eval_dict</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> is_terminal(<span class="va">self</span>, state):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (state <span class="op">==</span> <span class="va">self</span>.terminal_left) <span class="kw">or</span> (state <span class="op">==</span> <span class="va">self</span>.terminal_right)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_true_values(<span class="va">self</span>)        :</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {i:i<span class="op">/</span>(<span class="va">self</span>.size<span class="op">+</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="va">self</span>.get_nonterminal_states()}</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> true_value(<span class="va">self</span>, state):</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.true_values[state]</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> take_step(<span class="va">self</span>, state):</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Randomly step left or right from the current state.</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="co">        If we step into the terminal_right, reward=1, else 0.</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> state <span class="op">+</span> random.choice([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> next_state <span class="op">==</span> <span class="va">self</span>.terminal_right <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> next_state, reward</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="co"># ----- Training Algorithms -----</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_td0_one_episode(evaluation, random_walk, alpha<span class="op">=</span><span class="fl">0.1</span>, gamma<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="co">    Run one episode of TD(0) starting from the random walk's starting state.</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="co">    Update 'evaluation' in-place.</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> random_walk.starting_state</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> random_walk.is_terminal(state):</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        next_state, reward <span class="op">=</span> random_walk.take_step(state)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        evaluation[state] <span class="op">+=</span> alpha <span class="op">*</span> (reward <span class="op">+</span> gamma <span class="op">*</span> evaluation[next_state] <span class="op">-</span> evaluation[state])</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_mc_one_episode(evaluation, random_walk, alpha<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="co">    Run one episode of Monte Carlo control starting from the random walk's</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a><span class="co">    starting state. Update 'evaluation' in-place using an incremental MC update.</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="al">TODO</span><span class="co">: Actually I don't quite understand this MC implementation but for the random walk example it doesn't matter as γ=1 and there is only at most one reward</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the episode</span></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    episode_states <span class="op">=</span> []</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    episode_rewards <span class="op">=</span> []</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> random_walk.starting_state</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> random_walk.is_terminal(state):</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>        episode_states.append(state)</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>        next_state, reward <span class="op">=</span> random_walk.take_step(state)</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>        episode_rewards.append(reward)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> <span class="bu">sum</span>(r <span class="cf">for</span> r <span class="kw">in</span> episode_rewards)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update each visited state with incremental MC</span></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> episode_states:</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>        evaluation[s] <span class="op">+=</span> alpha <span class="op">*</span> (G <span class="op">-</span> evaluation[s])</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="co"># ----- Evaluation -----</span></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_rms_error(evaluation, true_values):</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the RMS error between the current evaluation and</span></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a><span class="co">    the true values. We assume that states are labeled from 1 to n</span></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    squared_errors <span class="op">=</span> []</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> state <span class="kw">in</span> true_values:</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>        squared_errors.append((evaluation[state] <span class="op">-</span> true_values[state]) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    mean_sq_error <span class="op">=</span> <span class="bu">sum</span>(squared_errors) <span class="op">/</span> <span class="bu">len</span>(squared_errors)</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> math.sqrt(mean_sq_error)</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="co"># ----- Experiment Runner -----</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_experiment(random_walk_size, alpha, episodes, runs, method<span class="op">=</span><span class="st">"TD"</span>, gamma<span class="op">=</span><span class="fl">1.0</span>, seed<span class="op">=</span><span class="dv">1</span>, start_values<span class="op">=</span><span class="va">None</span>, true_values <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="co">    Run multiple independent trials of either TD(0) or MC for the specified</span></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="co">    number of episodes and alpha. Collect the RMS error after each episode.</span></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a><span class="co">    :param random_walk_size: size of the random walk (number of nonterminal states)</span></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a><span class="co">    :param alpha: step-size parameter</span></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a><span class="co">    :param episodes: how many episodes to run</span></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a><span class="co">    :param runs: how many independent runs to average over</span></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a><span class="co">    :param method: "TD" or "MC"</span></span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a><span class="co">    :param gamma: discount factor</span></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a><span class="co">    :return: a list of mean RMS errors (across 'runs') for each episode index</span></span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>    random_walk <span class="op">=</span> RandomWalk(random_walk_size, seed<span class="op">=</span>seed)</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>    rms_errors_across_episodes <span class="op">=</span> [<span class="fl">0.0</span>] <span class="op">*</span> episodes</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(runs):</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialise an evaluation dict for each run</span></span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> start_values <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>            evaluation <span class="op">=</span> random_walk.get_init_eval()</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>            evaluation <span class="op">=</span> start_values.copy()</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ep <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> method <span class="op">==</span> <span class="st">"TD"</span>:</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>                train_td0_one_episode(evaluation, random_walk, alpha, gamma)</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> method <span class="op">==</span> <span class="st">"MC"</span>:</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>                train_mc_one_episode(evaluation, random_walk, alpha)</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute RMS after this episode</span></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span>(true_values <span class="op">==</span> <span class="va">None</span>):</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>                true_values <span class="op">=</span> random_walk.true_values</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>            rms <span class="op">=</span> compute_rms_error(evaluation, true_values)</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>            rms_errors_across_episodes[ep] <span class="op">+=</span> rms</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average the RMS by number of runs</span></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>    rms_errors_across_episodes <span class="op">=</span> [r <span class="op">/</span> runs <span class="cf">for</span> r <span class="kw">in</span> rms_errors_across_episodes]</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rms_errors_across_episodes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we can see how quickly random walk 0 approaches the true values (<span class="math inline">\(\alpha = 0.5\)</span>)</p>
<div id="f77a8004" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>random_walk_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>random_walk <span class="op">=</span> RandomWalk(random_walk_size, seed <span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>evaluation <span class="op">=</span> random_walk.get_init_eval()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(random_walk.true_values)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> state <span class="kw">in</span> random_walk.get_nonterminal_states():</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(state)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(random_walk.true_value(state))</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Run evaluations for different episode counts</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>episode_counts <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>,<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">max</span>(episode_counts)<span class="op">+</span><span class="dv">2</span>):</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="kw">in</span> episode_counts:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> evaluation.copy()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    train_td0_one_episode(evaluation, random_walk)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="bu">range</span>(random_walk_size)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co"># True values</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>true_vals <span class="op">=</span> [random_walk.true_value(state) <span class="cf">for</span> state <span class="kw">in</span> random_walk.get_nonterminal_states()]</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.plot(x, true_vals, label<span class="op">=</span><span class="st">"True Values"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co"># TD(0) estimates</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> count <span class="kw">in</span> episode_counts:</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [results[count][state] <span class="cf">for</span> state <span class="kw">in</span> random_walk.get_nonterminal_states()]</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, y, label<span class="op">=</span><span class="ss">f"TD(0), </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> episodes"</span>, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>plt.xticks(x, random_walk.get_nonterminal_states())</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"State"</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Estimated Value"</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"TD(0) Evaluation vs True Values"</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>{1: 0.16666666666666666, 2: 0.3333333333333333, 3: 0.5, 4: 0.6666666666666666, 5: 0.8333333333333334}
1
0.16666666666666666
2
0.3333333333333333
3
0.5
4
0.6666666666666666
5
0.8333333333333334</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="exercise-6.3" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.3">Exercise 6.3</h3>
<p>From the results shown in the graph of the random walk example it appears that the first episode results in a change in only <span class="math inline">\(V(1)\)</span>. What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed?</p>
<section id="solution-15" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-15">Solution</h4>
<p>In TD the update rule is <span class="math display">\[
V(S_t) \gets V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
\]</span></p>
<p>At the biggingi of the trainging for any state <span class="math inline">\(i\)</span> that cannot reach a terminal state we don’t get any reward, <span class="math inline">\(\gamma = 1\)</span> in this example and also not any update as we have: <span class="math display">\[
V(S_t) \gets V(S_t) + \alpha \left[ 0 + 1 \cdot 0.5 - 0.5 \right] = V(S_t)
\]</span></p>
<p>But for <span class="math inline">\(V(1)\)</span> when the random walks terminates we get: <span class="math display">\[
V(0) \gets V(0) + 0.1 \left[ 0 + 1 \cdot 0 - 0.5 \right] = V(0) - 0.05
\]</span> So it got changed by <span class="math inline">\(-0.05\)</span>.</p>
<p>Maybe we can also look at the convergence with a discounting factor <span class="math inline">\(\gamma = 0.9 \neq 1\)</span>. I also go down with <span class="math inline">\(\alpha = 0.01\)</span>. We can see that the values go in an arch below the real values. I don’t know why they have the values they have. I would have thought they should approximate the solution to <span class="math display">\[
\begin{split}
v_1 &amp;= 0.5 \times 0.9 \times v_2 \\
v_i &amp;= 0.5 \times 0.9 \times (v_{i-1} + v_{i+1}) \\
v_5 &amp;= 0.5 + 0.5 \times 0.9 \times v_4
\end{split}
\]</span></p>
<div id="e39a04ca" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>random_walk_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>random_walk <span class="op">=</span> RandomWalk(random_walk_size)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>evaluation <span class="op">=</span> random_walk.get_init_eval()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Run evaluations for different episode counts</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>episode_counts <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10_000</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">max</span>(episode_counts)<span class="op">+</span><span class="dv">2</span>):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="kw">in</span> episode_counts:</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        results[i] <span class="op">=</span> evaluation.copy()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    train_td0_one_episode(evaluation, random_walk, alpha<span class="op">=</span><span class="fl">0.01</span>, gamma<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="bu">range</span>(random_walk_size)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># True values</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>true_vals <span class="op">=</span> [random_walk.true_value(state) <span class="cf">for</span> state <span class="kw">in</span> random_walk.get_nonterminal_states()]</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.plot(x, true_vals, label<span class="op">=</span><span class="st">"True Values"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># TD(0) estimates</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> count <span class="kw">in</span> episode_counts:</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [results[count][state] <span class="cf">for</span> state <span class="kw">in</span> random_walk.get_nonterminal_states()]</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, y, label<span class="op">=</span><span class="ss">f"TD(0), </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> episodes"</span>, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.xticks(x, random_walk.get_nonterminal_states())</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"State"</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Estimated Value"</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"TD(0) Evaluation vs True Values"</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>That’s actually true. We can calculate the exact values by inverting <span class="math display">\[
\begin{pmatrix}
  1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
  -0.45 &amp; 1 &amp; -0.45 &amp; 0 &amp; 0 &amp;0 &amp;0 \\
  0 &amp;-0.45 &amp; 1 &amp; -0.45 &amp; 0 &amp; 0 &amp;0 \\
  0 &amp; 0 &amp;-0.45 &amp; 1 &amp; -0.45 &amp; 0 &amp; 0  \\
  0&amp; 0 &amp; 0 &amp;-0.45 &amp; 1 &amp; -0.45 &amp; 0  \\
  0&amp; 0&amp; 0 &amp; 0 &amp;-0.45 &amp; 1 &amp; -0.45  \\
  0&amp; 0&amp; 0 &amp; 0 &amp;0 &amp; 0 &amp; 1  \\
\end{pmatrix}
\]</span> Which is the specification where we’ve included the terminal states And the last column of the inverted matrix has the answers. <span class="math display">\[
\begin{split}
v_1 = 0.0655013 \\
v_2 = 0.145558 \\
v_3 = 0.257962 \\
v_4 = 0.42769 \\
v_5 = 0.692461
\end{split}
\]</span> which seem to fit pretty well.</p>
<p>Now can can compare the empirical RMS errors also to Monte Carlo: <span class="math display">\[
\mathrm{RMSD} = \left( \frac{1}{n} \sum_{i=1}^n \| X_i - x_0 \| \right)^{1/2}
\]</span></p>
<div id="0d553a93" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>random_walk_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>runs <span class="op">=</span> <span class="dv">500</span> </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>alphas_td <span class="op">=</span> [<span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.15</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>alphas_mc <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.02</span>, <span class="fl">0.03</span>, <span class="fl">0.04</span>]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Compare RMS error over episodes for different alpha (TD)</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_td:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    td_rms <span class="op">=</span> run_experiment(random_walk_size, alpha, episodes, runs, method<span class="op">=</span><span class="st">"TD"</span>, gamma<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    plt.plot(td_rms, label<span class="op">=</span><span class="ss">f"TD(0), alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_mc:</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    td_rms <span class="op">=</span> run_experiment(random_walk_size, alpha, episodes, runs, method<span class="op">=</span><span class="st">"MC"</span>, gamma <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    plt.plot(td_rms, label<span class="op">=</span><span class="ss">f"MC, alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episode"</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RMS Error"</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"TD(0): RMS Error vs Episodes for Various Alphas"</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-6.4" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.4">Exercise 6.4</h3>
<p>The specific results shown in the graph of the random walk example are dependent on the value of the step-size parameter, <span class="math inline">\(\alpha\)</span>. Do you think the conclusions about which algorithm is better would be affected if a wider range of <span class="math inline">\(\alpha\)</span> values were used? Is there a different, fixed value of <span class="math inline">\(\alpha\)</span> at which either algorithm would have performed significantly better than shown? Why or why not?</p>
<section id="solution-16" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-16">Solution</h4>
<p>I think the trend of the <span class="math inline">\(\alpha\)</span> can be seen quite well. Higher <span class="math inline">\(\alpha\)</span> faster stabilization and higher uncertainty. So I would say basically no. MC is already jiggly enough (this is averaged over 500 runs). So I say without any further elaboration. There is now value <span class="math inline">\(\alpha\)</span> for which MC comes close to TD in speed and precision (of course you can sacrifice one for the other but I mean the combination of both).</p>
</section>
</section>
<section id="exercise-6.5" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.5">Exercise 6.5</h3>
<p>In the graph of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high <span class="math inline">\(\alpha\)</span>. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized?</p>
<section id="solution-17" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-17">Solution</h4>
<p>Oh yes. That stupid bump. What happens to the bump with discounting <span class="math inline">\(\gamma = 0.9\)</span>?</p>
<div id="8fd75182" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>random_walk_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>runs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>true_values <span class="op">=</span> {<span class="dv">1</span>: <span class="fl">0.0655013</span>, <span class="dv">2</span>:<span class="fl">0.145558</span>, <span class="dv">3</span>:<span class="fl">0.257962</span>, <span class="dv">4</span>:<span class="fl">0.42769</span>, <span class="dv">5</span>:<span class="fl">0.692461</span>}</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>alphas_td <span class="op">=</span> [<span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.15</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>alphas_mc <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.02</span>, <span class="fl">0.03</span>, <span class="fl">0.04</span>]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Compare RMS error over episodes for different alpha (TD)</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_td:</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    td_rms <span class="op">=</span> run_experiment(random_walk_size, alpha, episodes, runs, method<span class="op">=</span><span class="st">"TD"</span>, gamma<span class="op">=</span>gamma, true_values<span class="op">=</span>true_values)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    plt.plot(td_rms, label<span class="op">=</span><span class="ss">f"TD(0), alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episode"</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RMS Error"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"TD(0): RMS Error vs Episodes for Various Alphas"</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Oh it seems to have gone. So it looks like an artifact of the training.</p>
<p>I don’t quite know where this drop of the green or organe line come from but I suspect that’s due to the way how the individual values approach the true values and that for low states the average seems to under estimate the true value and thus comming from above goes past the correct value.</p>
<p>Let’s see if we start with <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> everywhere:</p>
<div id="c19555bd" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>random_walk_size <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>runs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>start_values_1 <span class="op">=</span> {<span class="dv">0</span> : <span class="fl">0.0</span>, <span class="dv">1</span> : <span class="fl">1.0</span>, <span class="dv">2</span> : <span class="fl">1.0</span>, <span class="dv">3</span> : <span class="fl">1.0</span>, <span class="dv">4</span> : <span class="fl">1.0</span>, <span class="dv">5</span> : <span class="fl">1.0</span>, <span class="dv">6</span> : <span class="fl">0.0</span>}</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>start_values_0 <span class="op">=</span> {<span class="dv">0</span> : <span class="fl">0.0</span>, <span class="dv">1</span> : <span class="fl">0.0</span>, <span class="dv">2</span> : <span class="fl">0.0</span>, <span class="dv">3</span> : <span class="fl">0.0</span>, <span class="dv">4</span> : <span class="fl">0.0</span>, <span class="dv">5</span> : <span class="fl">0.0</span>, <span class="dv">6</span> : <span class="fl">0.0</span>}</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>alphas_td <span class="op">=</span> [<span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.15</span>]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>alphas_mc <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.02</span>, <span class="fl">0.03</span>, <span class="fl">0.04</span>]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Compare RMS error over episodes for different alpha (TD)</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_td:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    td_rms <span class="op">=</span> run_experiment(random_walk_size, alpha, episodes, runs, method<span class="op">=</span><span class="st">"TD"</span>, gamma<span class="op">=</span>gamma, start_values<span class="op">=</span>start_values_0)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    plt.plot(td_rms, label<span class="op">=</span><span class="ss">f"TD, start=0, alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_td:</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    td_rms <span class="op">=</span> run_experiment(random_walk_size, alpha, episodes, runs, method<span class="op">=</span><span class="st">"TD"</span>, gamma<span class="op">=</span>gamma, start_values<span class="op">=</span>start_values_1)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    plt.plot(td_rms, label<span class="op">=</span><span class="ss">f"TD, start=1, alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">"</span>)    </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episode"</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RMS Error"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"TD(0): RMS Error vs Episodes for Various Alphas"</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>No bumps anymore. So it’s just part of the intialization. Maybe it’s also because the middle state in the original problem get’s initialized to the exact value.</p>
</section>
</section>
<section id="exercise-6.6" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.6">Exercise 6.6</h3>
<p>We stated that the true values for the random walk example are <span class="math inline">\(1/6\)</span>, <span class="math inline">\(2/6\)</span>, <span class="math inline">\(3/6\)</span>, <span class="math inline">\(4/6\)</span>, <span class="math inline">\(5/6\)</span> for states <span class="math inline">\(1\)</span> through <span class="math inline">\(5\)</span>. Describe at least two different ways that these could have been computed. Which would you guess we actually used? Why?</p>
<section id="solution-18" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-18">Solution</h4>
<p>I have no idea which solution had been favoured by the authors. But I think we can calculate the exact values in two ways that are kind of reminiscent of MC and TD.</p>
<p>I’ve already talked about the “TD-method”. We have to solve the system of linear equatios from above.</p>
<p>The other way is to think of the value as expected value (the “MC-metho”). We have <span class="math display">\[
v_i = 0 \cdot P(\text{random walk ends left}) 1 \cdot P(\text{random walk ends right})
\]</span> A general fact which I won’t proof is when we have a discrete 1D random walk and a state <span class="math inline">\(C\)</span> and two states <span class="math inline">\(L\)</span> (left of <span class="math inline">\(C\)</span>) and <span class="math inline">\(R\)</span> (right of <span class="math inline">\(C\)</span>). Then we have to mutual exclusive events <span class="math inline">\(E_L = \text{ random walk starting at C reaches L first }\)</span> <span class="math inline">\(E_R = \text{ random walk starting at C reaches R first }\)</span> The probabilites of these are <span class="math display">\[
P(E_L) = \frac{r}{\ell + r}, P(E_R) = \frac{\ell}{\ell + r}
\]</span> where <span class="math inline">\(\ell\)</span> and <span class="math inline">\(r\)</span> are the distances to <span class="math inline">\(L\)</span> and <span class="math inline">\(R\)</span>.</p>
<p>So we get <span class="math display">\[
v_i = 1 \cdot P(\text{random walk ends right}) = \frac{i}{(i - 0) + (n+1-i)} = \frac{i}{n+1}
\]</span></p>
</section>
</section>
</section>
<section id="optimality-of-td0" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="optimality-of-td0"><span class="header-section-number">6.3</span> Optimality of TD(0)</h2>
<section id="exercise-6.7" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.7">Exercise 6.7</h3>
<p>Design an on-policy version of the TD(0) update that can be used with arbitrary target policy <span class="math inline">\(\pi\)</span> and covering behavior policy <span class="math inline">\(b\)</span>, using at each step <span class="math inline">\(t\)</span> the importance sampling ratio <span class="math inline">\(\rho_{t:t}\)</span> (5.3).</p>
<section id="solution-19" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-19">Solution</h4>
<p>We have this litte calculation: <span class="math display">\[
\begin{split}
v_\pi(s) &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})  \mid S_t = s] \\
&amp;= \sum_a \pi(a) \sum_{s',r} p(s',r \mid s,a) [r + \gamma v_\pi(s')] \\
&amp;= \sum_a b(a) \sum_{s',r} p(s',r \mid s,a) \frac{\pi(a)}{b(a)} [r + \gamma v_\pi(s')] \\
&amp;= \mathbb{E}_b[\frac{\pi(A)}{b(A)}(R + \gamma v_\pi(S_{t+1}))]
\end{split}
\]</span> How can we turn this into an update rule though? The simple approach <span class="math inline">\(x = \mathbb{E}_\pi[f(r)]\)</span> thus we create the update rule <span class="math inline">\(x \gets x + \alpha (f(R) - x)\)</span> is not applicable here because we are sampling according to <span class="math inline">\(b\)</span> and not according to <span class="math inline">\(\pi\)</span>. And if we would use it anyway we would end up at <span class="math display">\[
V(S) \gets V(S) + \alpha  [ \frac{\pi(A)}{b(A)}(R + \gamma V(S')) - V(S)]
\]</span> which is wrong.</p>
<p>How can I derive the the correct formula <span class="math display">\[
V(S) \gets V(S) + \alpha  \frac{\pi(A)}{b(A)}[ R + \gamma V(S') - V(S)]
\]</span> then?</p>
<p>So in normal sampling we would have take samples in state <span class="math inline">\(S\)</span> <span class="math display">\[
v_\pi(S) \approx \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:t} [R_t + \gamma v_\pi(S_{t+1})]}{| results |}
\]</span></p>
<p>So in the offline algorithm we have to shift our estimate <span class="math inline">\(V(S)\)</span> towards <span class="math inline">\(\frac{\pi(A)}{b(A)} [ R + \gamma V(S')]\)</span>. In detail this means when we are in state <span class="math inline">\(S\)</span> and <span class="math inline">\(b\)</span> takes action <span class="math inline">\(A\)</span> with reward <span class="math inline">\(R\)</span> and resulting state <span class="math inline">\(S'\)</span> we have to move <span class="math inline">\(V(S)\)</span> more towards <span class="math inline">\(\frac{\pi(A)}{b(A)}[R + \gamma V(S')]\)</span> which results in the update formula <span class="math inline">\(V(S) \gets V(S) + \alpha  [ \frac{\pi(A)}{b(A)}(R + \gamma V(S')) - V(S)]\)</span></p>
<p>which suggests this algorithm</p>
<div class="pseudocode-container quarto-float" data-comment-delimiter="//" data-caption-prefix="Algorithm" data-pseudocode-number="2" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{offline tabular TD(0) for estimating $v_\pi$} \begin{algorithmic} \State Input $\pi$, the policy to be evaluated \State Initialse $V(s)$ for all $s \in \mathcal{S}^+$, arbitrarily except that V(terminal) = 0 \for{for each episode} \state initialzie S \repeat \state $A \gets \pi(S)$ \state take action $A$ according to $b$, observe $R,S'$ \state $V(S) \gets V(S) + \alpha \frac{\pi(A)}{b(A)} [ R + \gamma V(S') - V(S)]$ \state $S \gets S'$ \until{S is terminal} \endfor \end{algorithmic} \end{algorithm}
</div>
</div>
<p>Is it really good. I don’t know</p>
</section>
</section>
</section>
<section id="sarsa-on-policy-td-control" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sarsa-on-policy-td-control"><span class="header-section-number">6.4</span> Sarsa: On-policy TD Control</h2>
<p>Sarsa update rule <span class="math display">\[
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \big[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \big]
\]</span></p>
<section id="exercise-6.8" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="exercise-6.8"><span class="header-section-number">6.4.1</span> Exercise 6.8</h3>
<p>Show that an action-value version of (6.6) holds for the action-value form of the TD error <span class="math display">\[
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t),
\]</span> again assuming that the values don’t change from step to step.</p>
<section id="solution-20" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-20">Solution</h4>
<p>Proof by induction that <span class="math inline">\(G_t - Q(S_t, A_t) = \sum_{k=t}^{T-1} \gamma^{k-t} \delta_t\)</span>. If <span class="math inline">\(t = T\)</span> then (by definition) <span class="math inline">\(G_T - Q(S_T, A_T) = 0 - 0\)</span></p>
<p>Induction step <span class="math display">\[
\begin{split}
G_t - Q(S_t, A_t) &amp;= R_{t+1} + \gamma G_{t+1} - Q(S_t, A_t) \\
&amp;= R_{t+1} - Q(S_t, A_t) + \gamma Q(S_{t+1}, A_{t+1}) + \gamma (G_{t+1} - Q(S_{t+1}, A_{t+1})) \\
&amp;= \delta_t + \gamma (G_{t+1} - Q(S_{t+1}, A_{t+1})) \\
&amp;= \delta_t + \gamma (\sum_{k=t+1}^{T-1} \gamma^{k-(t+1)} \delta_k) \\
&amp;= \delta_t + \sum_{k=t+1}^{T-1} \gamma^{k-t} \delta_k \\
&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k
\end{split}
\]</span></p>
</section>
</section>
<section id="exercise-6.9-windy-gridworld-with-kings-moves-programming" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="exercise-6.9-windy-gridworld-with-kings-moves-programming"><span class="header-section-number">6.4.2</span> Exercise 6.9: Windy Gridworld with King’s Moves (programming)</h3>
<p>Re-solve the windy gridworld assuming eight possible actions, including the diagonal moves, rather than the usual four. How much better can you do with the extra actions? Can you do even better by including a ninth action that causes no movement at all other than that caused by the wind?</p>
<section id="solution-21" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-21">Solution</h4>
<p>See windy_gridworld</p>
</section>
</section>
</section>
<section id="q-learning-off-policy-td-control" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="q-learning-off-policy-td-control"><span class="header-section-number">6.5</span> Q-learning: Off-policy TD control</h2>
<p>update formula: <span class="math display">\[
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \max_a Q(S_t+1,a) - Q(S_t, A_t) \Big]
\]</span></p>
<section id="exercise-6.11" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.11">Exercise 6.11</h3>
<p>Why is Q-learning considered an off-policy control method?</p>
<section id="solution-22" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-22">Solution</h4>
<p>I think the relevant part is in this part of the algorithm</p>
<pre><code>Choose A from S using policy derived from Q (e.g., ε-greedy)</code></pre>
<p>it could have been any othe policy too I think. We can also see it visibly in the cliff walk example. Q-learning learns the optimal path even though the average return is lower than sarsa. So it’s learning an optimal strategy from suboptimal behaviour.</p>
</section>
</section>
<section id="exercise-6.12" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.12">Exercise 6.12</h3>
<p>Suppose action selection is greedy. Is Q-learning then exactly the same algorithm as Sarsa? Will they make exactly the same action selections and weight updates?</p>
<section id="solution-23" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-23">Solution</h4>
<p>Let’s compare the two algorithms with greedy action selection.</p>
<div class="pseudocode-container quarto-float" data-comment-delimiter="//" data-caption-prefix="Algorithm" data-pseudocode-number="2" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{Greedy-Sarsa (on-policy TD control) for estimating $Q \approx q_*$} \begin{algorithmic} \State Algorithm parameters: step size $\alpha \in (0, 1]$, small $\epsilon &gt; 0$ \State Initialize $Q(s, a)$, for all $s \in \mathcal{S}^+$, $a \in \mathcal{A}(s)$, arbitrarily except that $Q(\mathrm{terminal}, \cdot)=0$ \for{each episode} \state Initialize S \state Choose $A = \max_a Q(S,a)$ \repeat \state Take action $A$, observe $R$, $S'$ \state Choose $A' = \max_a Q(S'a)$ \state $Q(S,A) \gets Q(S,A) + \alpha \Big[ R + \gamma Q(S', A') - Q(S,A) \Big]$ \state $S \gets S'$, $A \gets A'$ \until{S is terminal} \endfor \end{algorithmic} \end{algorithm}
</div>
</div>
<div class="pseudocode-container quarto-float" data-comment-delimiter="//" data-caption-prefix="Algorithm" data-pseudocode-number="2" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{Greedy-Q-learning (on-policy TD control) for estimating $Q \approx q_*$} \begin{algorithmic} \State Algorithm parameters: step size $\alpha \in (0, 1]$, small $\epsilon &gt; 0$ \State Initialize $Q(s, a)$, for all $s \in \mathcal{S}^+$, $a \in \mathcal{A}(s)$, arbitrarily except that $Q(\mathrm{terminal}, \cdot)=0$ \for{each episode} \state Initialize S \repeat \state Choose $A = \max_a Q(S,a)$ \state Take action $A$, observe $R$, $S'$ \state $Q(S,A) \gets Q(S,A) + \alpha \Big[ R + \gamma \max_a Q(S', a) - Q(S,A) \Big]$ \state $S \gets S'$, $A \gets A'$ \until{S is terminal} \endfor \end{algorithmic} \end{algorithm}
</div>
</div>
<p>Yes, with greedy action selection, Q-learning and Sarsa become the same.</p>
<p>In standard Sarsa, the update is based on the action actually taken in the next state (A’). But if action selection is greedy, then A’ = argmax_a Q(S’, a)—exactly the same choice that Q-learning makes for its update. So both algorithms use the same action to calculate the update.</p>
<p>Moreover, if we rewrite the Sarsa algorithm by moving the initial action selection (A = argmax Q(S, a)) inside the loop, it becomes structurally identical to Q-learning. The only subtle difference is that Sarsa still follows its action selection, while Q-learning uses it purely for the update, but under greedy selection, these behaviours coincide.</p>
</section>
</section>
</section>
<section id="expected-sarsa" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="expected-sarsa"><span class="header-section-number">6.6</span> Expected Sarsa</h2>
<p>The update formula iiiis:</p>
<p><span class="math display">\[
\begin{split}
Q(S_t, A_t)
&amp;\gets Q(S_t, A_t) + \alpha \Big[ R_{t+1} +
\gamma \mathbb{E}_\pi[Q(S_{t+1}, A_{t+1}) \mid S_{t+1}] - Q(S_t, A_t) \Big] \\
&amp;\gets Q(S_t, A_t) + \alpha \Big[ R_{t+1} +
\gamma \sum_{a} \pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t)\Big]
\end{split}
\]</span></p>
</section>
<section id="maximization-bias-and-double-learning" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="maximization-bias-and-double-learning"><span class="header-section-number">6.7</span> Maximization Bias and Double Learning</h2>
<section id="exercise-6.13" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.13">Exercise 6.13</h3>
<p>What are the update equations for Double Expected Sarsa with an <span class="math inline">\(\epsilon\)</span>-greedy target policy?</p>
<section id="solution-24" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-24">Solution</h4>
<p><span class="math display">\[
Q_1(S_t, A_t) \gets Q_1(S_t,A_t) + \alpha \Big[ R_{t+1} +
\gamma \sum_a \pi_1(a|S_{t+1}) Q_2(S_{t+1}, a) - Q_1(S_t,A_t) \Big]
\]</span></p>
</section>
</section>
</section>
<section id="games-afterstates-and-other-special-cases" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="games-afterstates-and-other-special-cases"><span class="header-section-number">6.8</span> Games, Afterstates, and Other Special Cases</h2>
<section id="exercise-6.14" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-6.14">Exercise 6.14</h3>
<p>Describe how the task of Jack’s Car Rental (Example 4.2) could be reformulated in terms of afterstates. Why, in terms of this specific task, would such a reformulation be likely to speed convergence?</p>
<section id="solution-25" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-25">Solution</h4>
<p>Didn’t think it through but afterstates could be the amount of cars in the morning, i.e., the cars after the swap.</p>
<p>This speeds up the process as described in the tic-tac-toe example there a multiple state-actions that correspond to the same action-afterstate</p>
</section>
</section>
</section>
</section>
<section id="n-step-bootstrapping" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> n-step Bootstrapping</h1>
<section id="n-step-td-prediction" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="n-step-td-prediction"><span class="header-section-number">7.1</span> n-step TD Prediction</h2>
<p>Complete return used for MC: <span class="math display">\[
G_t := R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t-1} R_T
= \sum_{k = 1}^{T-t} \gamma^{k-1} R_{t+k}
\]</span></p>
<p>one-step return used for TD(0): <span class="math display">\[
G_{t:t+1} := R_{t+1} + \gamma V_t(S_{t+1})
\]</span></p>
<p>genereal n-step return <span class="math display">\[
\begin{split}
G_{t:t+n} :=&amp; R_{t+1} + \gamma R_{t+2} + \dots \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n}) \\
=&amp; \sum_{k=1}^{n} \gamma^{k-1} R_{t+k} + \gamma^n V_{t+n+1}(S_{t+n})
\end{split}
\]</span></p>
<section id="exercise-7.1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exercise-7.1">Exercise 7.1</h3>
<p>In Chapter 6 we noted that the Monte Carlo error can be written as the sum of TD errors (6.6) if the value estimates don’t change from step to step. Show that the n-step error used in (7.2) can also be written as a sum TD errors (again if the value estimates don’t change) generalizing the earlier result.</p>
<section id="solution-26" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-26">Solution</h4>
<p>The TD error is <span class="math inline">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\)</span></p>
<p>And (6.6): <span class="math inline">\(G_t - V(S_t) = \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k\)</span></p>
<p>And (7.2): <span class="math inline">\(V_{t+n}(S_t) = V_{t+n-1}(S_t) + \alpha \Big[ G_{t:t+n} - V_{t+n-1}(S_t) \Big]\)</span></p>
<p>So let’s start the calculation: <span class="math display">\[
G_t - V(S_t) = G_t - V(S_t) + \alpha[G_{t:t+n} - V_t]
\]</span></p>
</section>
</section>
<section id="exercise-7.2-programming" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="exercise-7.2-programming"><span class="header-section-number">7.1.1</span> Exercise 7.2 (programming)</h3>
<p>With an n-step method, the value estimates do change from step to step, so an algorithm that used the sum of TD errors (see previous exercise) in place of the error in (7.2) would actually be a slightly different algorithm. Would it be a better algorithm or a worse one? Devise and program a small experiment to answer this question empirically</p>
<section id="solution-27" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-27">Solution</h4>
</section>
</section>
<section id="the-error-reduction-property" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="the-error-reduction-property"><span class="header-section-number">7.1.2</span> The error reduction property</h3>
<p>The equation for <em>error reduction property</em> is <span class="math display">\[
\max_s \Big| \mathbb{E}_\pi \big[ G_{t:t+n} \mid S_t = s \big] - v_\pi(s) \Big|
\leq
\gamma^n \max_s \Big| V_{t+n-1}(s) - v_\pi(s) \Big|
\]</span></p>
<p>Again <span class="math display">\[
G_{t:t+n} = R_{t+1} +  \dots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n})
\]</span></p>
<p>Let’s go through some calculation… well, I don’t know <span class="math display">\[
\max_s \Big| \mathbb{E}_\pi \big[
R_{t+1} +  \dots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n})
\mid S_t = s \big] - v_\pi(s) \Big|
\]</span></p>
</section>
</section>
<section id="going-through-example-7.1-n-step-td-methods-on-the-random-walk" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="going-through-example-7.1-n-step-td-methods-on-the-random-walk"><span class="header-section-number">7.2</span> Going through Example 7.1: n-step TD Methods on the Random Walk</h2>
<div id="329e4071" class="cell" data-auto-fold="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># class RandomWalk:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># def __init__(self, size, seed  =1):</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># def get_nonterminal_states(self):</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># def get_init_eval(self):</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># def is_terminal(self, state):</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># def compute_true_values(self)        :</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># def true_value(self, state):</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># def take_step(self, state):</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># goal alpha x Average RMS error</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># my pseudocode</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># for each alpha:</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">#   for each n:</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co">#       train n-DT for 10 episodes</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">#       compute rms error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"07b0d62e958a4c2ab8a7c85654f0bec0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b063e446e641b1b943dd0c9c90885a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"TextModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"TextView","continuous_update":true,"description":"String:","description_allow_html":false,"disabled":false,"layout":"IPY_MODEL_59da19fdb4f34f459f96ba095e8c0182","placeholder":"Type something","style":"IPY_MODEL_fec3ff9f5fc34abd8b681fc6a641da7d","tabbable":null,"tooltip":null,"value":"Hello World"}},"0a90300ba21e4a5dbef0c38e0b4c3f00":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"LabelStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"LabelStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_family":null,"font_size":null,"font_style":null,"font_variant":null,"font_weight":null,"text_color":null,"text_decoration":null}},"0f180c7e23bf4d2d821eebac186baf9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"LabelStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"LabelStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_family":null,"font_size":null,"font_style":null,"font_variant":null,"font_weight":null,"text_color":null,"text_decoration":null}},"1d738d5b602a4c3abce2f52187cbd9d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ButtonView","button_style":"","description":"Arm 4","disabled":false,"icon":"","layout":"IPY_MODEL_e85be954d76a4cd49568420b125e83b2","style":"IPY_MODEL_eb1e2033137d43f1b8e273e6714b3754","tabbable":null,"tooltip":null}},"1f65e194286245abbee63813c43cf924":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","button_color":null,"font_family":null,"font_size":null,"font_style":null,"font_variant":null,"font_weight":null,"text_color":null,"text_decoration":null}},"22c9f6dac0394653945e4324e069124b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ButtonView","button_style":"","description":"Arm 1","disabled":false,"icon":"","layout":"IPY_MODEL_d5730cfd08194862b9837a69bad2b994","style":"IPY_MODEL_1f65e194286245abbee63813c43cf924","tabbable":null,"tooltip":null}},"28d45f1b70e84b078a5f15d1726b1cf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"LabelView","description":"","description_allow_html":false,"layout":"IPY_MODEL_686afba828304dc9a15cb283f0605c68","placeholder":"​","style":"IPY_MODEL_0a90300ba21e4a5dbef0c38e0b4c3f00","tabbable":null,"tooltip":null,"value":"Hello from a label!"}},"44a6a5ab2d0d47dfa13d439e4d926c69":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"LabelView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5874403db40347e7b65172dcc3758044","placeholder":"​","style":"IPY_MODEL_0f180c7e23bf4d2d821eebac186baf9a","tabbable":null,"tooltip":null,"value":"Pull an arm to see your reward!"}},"5874403db40347e7b65172dcc3758044":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59da19fdb4f34f459f96ba095e8c0182":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64964c54622d46f1a8ff8ceb63c07362":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"686afba828304dc9a15cb283f0605c68":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bdb5320ea4646a9a99dec6201716d51":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","button_color":null,"font_family":null,"font_size":null,"font_style":null,"font_variant":null,"font_weight":null,"text_color":null,"text_decoration":null}},"76acfd98e8694ccd87d62395257e711d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ButtonView","button_style":"","description":"Arm 2","disabled":false,"icon":"","layout":"IPY_MODEL_07b0d62e958a4c2ab8a7c85654f0bec0","style":"IPY_MODEL_ee1fa3120ecc46c99567280a6e40e3b4","tabbable":null,"tooltip":null}},"83b4471437df4495941f1a0da66847bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22c9f6dac0394653945e4324e069124b","IPY_MODEL_76acfd98e8694ccd87d62395257e711d","IPY_MODEL_a9253f7146894e70bbd0d9b71d25a9d7","IPY_MODEL_1d738d5b602a4c3abce2f52187cbd9d4"],"layout":"IPY_MODEL_64964c54622d46f1a8ff8ceb63c07362","tabbable":null,"tooltip":null}},"8e511f6e4a3a4dc89540cb22e6aac333":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_83b4471437df4495941f1a0da66847bb","IPY_MODEL_08b063e446e641b1b943dd0c9c90885a"],"layout":"IPY_MODEL_d55b72dfc7d646a2aacfd6d58bc306b3","tabbable":null,"tooltip":null}},"a9253f7146894e70bbd0d9b71d25a9d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ButtonView","button_style":"","description":"Arm 3","disabled":false,"icon":"","layout":"IPY_MODEL_e36595e1a4de45f6b87e5af737939161","style":"IPY_MODEL_6bdb5320ea4646a9a99dec6201716d51","tabbable":null,"tooltip":null}},"d55b72dfc7d646a2aacfd6d58bc306b3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5730cfd08194862b9837a69bad2b994":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e36595e1a4de45f6b87e5af737939161":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e85be954d76a4cd49568420b125e83b2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb1e2033137d43f1b8e273e6714b3754":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","button_color":null,"font_family":null,"font_size":null,"font_style":null,"font_variant":null,"font_weight":null,"text_color":null,"text_decoration":null}},"ee1fa3120ecc46c99567280a6e40e3b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","button_color":null,"font_family":null,"font_size":null,"font_style":null,"font_variant":null,"font_weight":null,"text_color":null,"text_decoration":null}},"fec3ff9f5fc34abd8b681fc6a641da7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"TextStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"TextStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>