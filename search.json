[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Sutton & Barto",
    "section": "",
    "text": "Preface\nWelcome to my study notes on “Reinforcement Learning: An Introduction” by Richard S. Sutton and Andrew G. Barto (Sutton and Barto 2018). These notes primarily focus on my solutions and insights while working through the exercises in the book.\nI copy to the chapter and section structure of the book, presenting and solving the exercises within this framework. Additionally, sections with titles in lowercase are my own additions.\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-intro.html",
    "href": "chapters/01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Exercise 1.1 (Self-Play) Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n\n\nSolution 1.1. The algorithm would learn the true game-theoretic values of each board state because, in the long run, both sides would learn the optimal strategy against their opponent.\n\n\nExercise 1.2 (Symmetries) Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n\n\nSolution 1.2. We could alter the learning process by using a canonical representative for each board state instead of the board state itself. This would speed up learning (the algorithm would generalise for symmetric states) and require less memory.\nIf the opponent does not respect the board symmetries, then the environment (board state plus opponent) should be treated as having no symmetries.\n\n\nExercise 1.3 (Greedy Play) Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a non-greedy player? What problems might occur?\n\n\nSolution 1.3. It could potentially learn to play better or worse. The greedy player has the advantage of always exploiting its knowledge. However, it has the significant disadvantage of never exploring. It could end up valuing a position as a draw that is actually a win as it never explores subsequent positions that would lead to win.\n\n\nExercise 1.4 (Learning from Exploration) Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n\n\nSolution 1.4. Without exploratory moves we learn the values of the states according to the optimal policy. With exploratory moves we learn the values of the states according to the \\(\\varepsilon\\)-optimal policy.\nIf we continue playing with \\(\\varepsilon\\)-soft, the latter values are preferable.\n\n\nExercise 1.5 (Other Improvements) Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?\n\n\nSolution 1.5. Surely there are many ways. And I think this book will discuss many of them. Just to give one example we could use \\(n\\)-step temporal difference (update a state’s value only after \\(n\\) moves).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html",
    "href": "chapters/02-multi-armed-bandits.html",
    "title": "2  Multi-armed Bandits",
    "section": "",
    "text": "2.1 A k-armed Bandit Problem\n(Some imports for python, can be ignored)\nThe k-armed bandit problem is a very simple example that already introduces a key structure of reinforcement learning: the reward depends on the action taken. It’s technically not a full Markov chain (more on that in Section 3.1)—since there are no states or transitions—but we still get a sequence of dependent random variables \\(A_1, R_1, A_2, \\dots\\), where each reward \\(R_t\\) depends on the corresponding action \\(A_t\\).\nThe true value of an action is defined as: \\[\nq_*(a) := \\mathbb{E}[ R_t \\mid A_t = a].\n\\]\nThe time index here doesn’t play a special role as the action-reward probabilities in the armed bandit are stationary. You can think of it as “when \\(a\\) is picked”—that is, the expected reward when action \\(a\\) is chosen.\nIf you’re feeling queasy about the conditional expected value, here’s a quick refresher on the relevant notation and concepts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#a-k-armed-bandit-problem",
    "href": "chapters/02-multi-armed-bandits.html#a-k-armed-bandit-problem",
    "title": "2  Multi-armed Bandits",
    "section": "",
    "text": "random variables and probability\nThe foundations of all processes we discuss here are discrete probability spaces \\((\\Omega, \\mathrm{Pr})\\).  \\(\\Omega\\) is the set of all possible trajectories—that is, complete sequences of outcomes for the random variables in a single run of the process—and \\(\\mathrm{Pr}\\) assigns a probability to each trajectory. That is, \\[\n\\mathrm{Pr}\\colon \\Omega \\to [0,1] \\quad \\text{with} \\quad\n\\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) = 1.\n\\]\nThe random variables \\(X\\) are simply functions \\(X\\colon \\Omega \\to \\mathcal{X}\\) from \\(\\Omega\\) to a result space \\(\\mathcal{X}\\).\nWe follow the convention of Sutton and Barto (2018): random variables are written in capital letters, and their possible values are in lowercase.\nIf we want to refer to the concrete outcome of a single trajectory (which we actually don’t often do in theory crafting), we evaluate random variables on a specific \\(\\omega \\in \\Omega\\), which fixes their values.  So an arbitrary trajectory looks like this \\(A_1(\\omega), R_1(\\omega), A_2(\\omega), R_2(\\omega) \\dots\\)\nLet’s bring up two common conventions we can find in \\(\\mathbb{E}[ R_t \\mid A_t = a]\\):\n\nWe usually omit the argument \\(\\omega\\) when referring to the value of a random variable. This is what makes the randomness implicit in a random variable \\(X\\). This mathematically conflates the function \\(X ⁣\\colon\\Omega\\to \\mathcal{X}\\) with the value \\(X(\\omega)\\), but context sorts that out.\nWhen writing functions of sets, we abbreviate expressions like \\(F({\\omega \\in \\Omega : \\text{statement true in }\\omega})\\) to simply \\(F(\\text{statement})\\).\n\nWith both conventions in play, we can see that \\(\\mathrm{Pr}(X = x)\\) is just shorthand for \\(\\mathrm{Pr}(\\omega \\in \\Omega : X(\\omega) = x)\\)\n\n\nconditional probability\nThe reward \\(R_t\\)​ depends on the action \\(A_t\\)​ taken. If we know the value of \\(A_t\\), then the conditional probability that \\(R_t=r\\) given \\(A_t = a\\) is: \\[\n\\mathrm{Pr}(R_t = r \\mid A_t = a) = \\frac{\\mathrm{Pr}(R_t = r, A_t = a)}{\\mathrm{Pr}(A_t = a)},\n\\]\nwhere the comma denotes conjunction of the statements.\nThis is only well-defined if \\(\\mathrm{Pr}(A_t = a) &gt; 0\\) but that’s a technicality we won’t worry too much about—it won’t bite us.\n\n\nexpected value\nReal-valued random variables like \\(R_t\\colon \\Omega \\to \\mathbb{R}\\) have an expected value—also called the mean—\\(\\mathbb{E}[R_t]\\), defined as: \\[\n\\mathbb{E}[ R_t ] := \\sum_{\\omega \\in \\Omega} R_t(\\omega) \\mathrm{Pr}(\\omega)\n\\]\nA more commonly used form—sometimes called the “law of the unconscious statistician” (see appendix Theorem 2.1)—is: \\[\n\\mathbb{E}[R_t] = \\sum_{r \\in \\mathcal{R}} r \\; \\mathrm{Pr}(R_t = r),\n\\]\nTo compute a conditional expectation, we just switch probabilities with conditional probabilities: \\[\n\\mathbb{E}[R_t \\mid A_t = a] = \\sum_{\\omega \\in \\Omega} R_t(\\omega) \\mathrm{Pr}(\\omega \\mid A_t = a).\n\\]\nOr, using the more practical LOTUS form: \\[\n\\mathbb{E}[R_t \\mid A_t = a] = \\sum_{r \\in \\mathcal{R}} r \\; \\mathrm{Pr}(R_t = r \\mid A_t = a).\n\\]\nTo close the loop, a more explicit formulation of the true value of an action is: \\[\nq_*(a) = \\sum_{r \\in \\mathcal{R}} r \\; \\frac{\\mathrm{Pr}(R_t = r, A_t = a)}{\\mathrm{Pr}(A_t = a)}\n\\] (This form is arguably less intuitive, but it’s included here to jog our memory.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#action-value-methods",
    "href": "chapters/02-multi-armed-bandits.html#action-value-methods",
    "title": "2  Multi-armed Bandits",
    "section": "2.2 Action-value Methods",
    "text": "2.2 Action-value Methods\nThis part always trips me up, so let me clarify it for myself: \\(Q_t(a)\\) is the estimated value of action \\(a\\) prior to time \\(t\\), so not included are \\(A_t\\) and it’s corresponding reward \\(R_t\\).\nInstead, \\(A_t\\) is selected based on the current estimates \\(\\{Q_{t}(a):a \\in \\mathcal{A}\\}\\). For example, our algorithm could pick \\(A_t\\)​ greedily as \\(A_t:=\\mathrm{argmax}_{a \\in \\mathcal{A}} Q_t(a)\\), or \\(\\varepsilon\\)-greedily.\n\nExercise 2.1 In \\(\\varepsilon\\)-greedy action selection, for the case of two actions and \\(\\varepsilon = 0.5\\), what is the probability that the greedy action is selected?\n\n\nSolution 2.1. The total probability of selecting the greedy action is: \\[\n\\mathrm{Pr}(\\text{greedy action}) + \\mathrm{Pr}(\\text{exploratory action}) \\cdot \\frac{1}{2} = 0.75\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#the-10-armed-testbed",
    "href": "chapters/02-multi-armed-bandits.html#the-10-armed-testbed",
    "title": "2  Multi-armed Bandits",
    "section": "2.3 The 10-armed Testbed",
    "text": "2.3 The 10-armed Testbed\nThe 10-armed testbed will accompany us through the rest of this chapter (I had to keep it variable in size just for the sake of generalization though).\n\n# === the armed bandit ===\nclass ArmedBandit:\n    \"\"\"k-armed Gaussian bandit.\"\"\"\n    def __init__(self, action_mu, action_sd, seed):\n        self.action_mu = np.asarray(action_mu, dtype=np.float64)\n        self.action_sd = np.asarray(action_sd, dtype=np.float64)\n        self.seed = seed\n        self.rng = np.random.default_rng(self.seed)\n\n    def pull_arm(self, action):\n        return self.rng.normal(loc=self.action_mu[action], scale=self.action_sd)\n\nAnd here’s the code for the sample-average bandit algorithm. For clarity, I’ll refer to this and upcoming algorithms as ‘agents’, given their autonomous implementation. Note that we’re also using the incremental implementation from section 2.4.\n\n# === the simple average bandit agent ===\nclass SampleAverageBanditAgent:\n    def __init__(self, Q1, ε, seed=None):\n        self.rng = np.random.default_rng(seed)\n        self.num_actions = len(Q1) \n        self.Q1 = np.asarray(Q1, dtype=np.float64)  # initial action-value estimates\n        self.ε = ε  \n        self.reset()\n\n    def reset(self):\n        self.Q = self.Q1.copy()\n        self.counts = np.zeros(self.num_actions, dtype=int)\n\n    def act(self, bandit):\n        # ε-greedy action selection\n        if self.rng.random() &lt; self.ε:\n            action = self.rng.integers(self.num_actions)\n        else:\n            action = np.argmax(self.Q)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update count and value estimate\n        self.counts[action] += 1\n        α = 1 / self.counts[action]\n        self.Q[action] += α * (reward - self.Q[action])\n\n        return (action, reward)\n\nNext, I’ll define an experiment function bandit_experiment that we’ll use throughout this chapter. In short, it takes multiple agents, length of each episode, and the number of episodes, then executes these agents repeatedly in a shared bandit environment, which gets reset after each episode. It returns two arrays: the average reward per step and the percentage of optimal actions taken per step—both averaged over all runs. These results can then be visualised using the plotting functions also provided. You don’t have to read the code unless you’re curious…\n\n\nCode\n# === the core bandit experiment ===\n\n# --- config\n@dataclass\nclass Config:\n    bandit_num_arms: int = 10\n    bandit_setup_mu: float = 0.0\n    bandit_setup_sd: float = 1.0\n    bandit_action_sd: float = 1.0\n    bandit_value_drift: bool = False\n    bandit_value_drift_mu: float = 0.0\n    bandit_value_drift_sd: float = 0.0\n    exp_steps: int = 1_000\n    exp_runs: int = 200\n    exp_seed: int = 0\n\n\n# -- core experiment\ndef bandit_experiment(agents, config: Config) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run `exp_runs` × `exp_steps` episodes and return:\n\n    average_rewards         shape = (len(agents), exp_steps)\n    optimal_action_percent  shape = (len(agents), exp_steps)\n    \"\"\"\n    rng = np.random.default_rng(config.exp_seed)\n    num_agents = len(agents)\n    average_rwds = np.zeros((num_agents, config.exp_steps))\n    optimal_acts = np.zeros((num_agents, config.exp_steps))\n\n    # allocate a single bandit and reuse its object shell\n    bandit = ArmedBandit(\n        action_mu=np.empty(config.bandit_num_arms),  # placeholder\n        action_sd=config.bandit_action_sd,\n        seed=config.exp_seed,\n    )\n\n    for run in range(config.exp_runs):\n        # fresh true values for this run\n        bandit.action_mu[:] = rng.normal(\n            config.bandit_setup_mu, config.bandit_setup_sd, size=config.bandit_num_arms\n        )\n        best_action = np.argmax(bandit.action_mu)\n\n        # reset all agents\n        for agent in agents:\n            agent.reset()\n\n        # vectorised drift noise: shape = (exp_steps, bandit_num_arms)\n        if config.bandit_value_drift:\n            drift_noise = rng.normal(\n                config.bandit_value_drift_mu,\n                config.bandit_value_drift_sd,\n                size=(config.exp_steps, config.bandit_num_arms),\n            )\n\n        # main loop\n        for t in range(config.exp_steps):\n            for i, agent in enumerate(agents):\n                act, rwd = agent.act(bandit)\n                average_rwds[i, t] += rwd\n                optimal_acts[i, t] += act == best_action\n\n            if config.bandit_value_drift:\n                bandit.action_mu += drift_noise[t]\n                best_action = np.argmax(bandit.action_mu)\n\n    # mean over runs\n    average_rwds /= config.exp_runs\n    optimal_acts = 100 * optimal_acts / config.exp_runs\n    return average_rwds, optimal_acts\n\n\n# --- thin plotting helpers\ndef plot_average_reward(\n    average_rewards: np.ndarray,\n    *,\n    labels: Sequence[str] | None = None,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"One line per agent: average reward versus step.\"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(8, 4))\n\n    steps  = np.arange(1, average_rewards.shape[1] + 1)\n    if labels is None:\n        labels = [f\"agent {i}\" for i in range(average_rewards.shape[0])]\n\n    for i, lbl in enumerate(labels):\n        ax.plot(steps, average_rewards[i], label=lbl)\n\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Average reward\")\n    ax.set_title(\"Average reward per step\")\n    ax.grid(alpha=0.3, linestyle=\":\")\n    ax.legend()\n    return ax\n\n\ndef plot_optimal_action_percent(\n    optimal_action_percents: np.ndarray,\n    *,\n    labels: Sequence[str] | None = None,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"One line per agent: % optimal action versus step.\"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(8, 4))\n\n    steps  = np.arange(1, optimal_action_percents.shape[1] + 1)\n    if labels is None:\n        labels = [f\"agent {i}\" for i in range(optimal_action_percents.shape[0])]\n\n    for i, lbl in enumerate(labels):\n        ax.plot(steps, optimal_action_percents[i], label=lbl)\n\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"% optimal action\")\n    ax.set_title(\"Optimal-action frequency\")\n    ax.grid(alpha=0.3, linestyle=\":\")\n    ax.legend()\n    return ax\n\n\n…, but it’s still helpful to see such an experiment in action. We can, for example, recreate Figure 2.2 (Sutton and Barto 2018). It compares the performance of a greedy agent (\\(\\varepsilon = 0\\)), and two \\(\\varepsilon\\)-greedy agents with \\(\\varepsilon =0.1\\) and \\(\\varepsilon=0.01\\). We let them run for a couple of steps and then repeat this process for a couple of runs to get a smoother curve.\n\n# === comparison greediness ===\n# configuration of the experiment\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=2_000)\n\n# initialize agents with different epsilon values\nepsilons = [0.0, 0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), ε=ε, seed=config.exp_seed) for ε in epsilons]\n\n# run bandit experiment\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config    \n)\n\n# plots\nlabels = [f\"ε={e}\" for e in epsilons]\n\nplot_average_reward(avg_rwd, labels=labels)\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n(a) The average reward.\n\n\n\n\n\n\n\n\n\n\n\n(b) The percentage of optimal step selection\n\n\n\n\n\n\nFigure 2.1: This is like Figure 2.2 (Sutton and Barto 2018): the average performance of different ε-greedy sample average methods over 2000 runs. On the 10-armed testbed.\n\n\n\n\nOut of curiosity, let’s see what happens when we have only one run (so it’s not the average anymore but just the reward). It’s a mess, wow. Without averaging over a couple of runs, we can’t make out anything.\n\n\nCode\n# === experiment with only one run ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1)\n\nepsilons = [0.0, 0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), ε=ε, seed=config.exp_seed) for ε in epsilons]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nplot_average_reward(avg_rwd, labels=[f\"ε={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 (Bandit example) Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \\(\\varepsilon\\)-greedy action selection, sample-average action-value estimates, and initial estimates of \\(Q_1(a) = 0\\), for all a. Suppose the initial sequence of actions and rewards is \\(A_1 = 1\\), \\(R_1 = -1\\), \\(A_2 = 2\\), \\(R_2 = 1\\), \\(A_3 = 2\\), \\(R_3 = -2\\), \\(A_4 = 2\\), \\(R_4 = 2\\), \\(A_5 = 3\\), \\(R_5 = 0\\). On some of these time steps the \\(\\varepsilon\\) case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n\n\nSolution 2.2. Step 1 could have been exploratory, as all actions have the same estimates. After that, the value function is: \\[\nQ_2(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nAlso, step 2 could have been exploratory. Now the value function is: \\[\nQ_3(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            1,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nIn step 3, the greedy action is taken. But it could also have been an exploratory action that selected 2. At this point, the value function is: \\[\nQ_4(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            -0.5,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nIn step 4, a non-greedy action is taken, so this must have been an exploratory move. The value function is: \\[\nQ_5(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            0.33,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nIn step 5, again a non-greedy action was taken, so this must have been an exploratory move as well.\n\n\nExercise 2.3 In the comparison shown in Figure 2.1, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n\n\nSolution 2.3. Obviously, we can disregard \\(\\varepsilon = 0\\). It’s just rubbish. Before we do the quantitative analysis, let’s see what happens when we just crank up the number of steps (and reduce the runs even though now it’s a bit noisier).\n\n\nCode\n# === battle between ε=0.1 and ε=0.01 ===\nconfig = Config(\n    exp_steps=15_000,\n    exp_runs=200)\n\nepsilons = [0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), ε=ε, seed=config.exp_seed) for ε in epsilons]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nplot_average_reward(avg_rwd, labels=[f\"ε={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=[f\"ε={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that \\(\\varepsilon=0.01\\) outperforms \\(\\varepsilon=0.1\\) in average reward around step \\(2000\\). However, achieving a higher percentage of optimal actions takes more than \\(10,000\\) steps. It’s actually quite interesting that achieving a higher percentage of optimal actions takes significantly longer.\nNow, let’s consider the long-term behaviour. In the limit, we can assume both methods have near-perfect \\(Q\\)-values and the only reason they select non-optimal actions is due to their \\(\\varepsilon\\)-softness.\nThis makes calculating the optimal action probability quite easy. \\[\n\\mathrm{Pr}(\\text{optimal action}) = (1-\\varepsilon) + \\varepsilon \\frac{1}{10} = 1 - 0.9 \\varepsilon\n\\]\nSo for \\(\\varepsilon=0.1\\) this probability is \\(0.91\\), and for \\(\\varepsilon=0.01\\) this is \\(0.991\\).\nNow the average reward is trickier to compute. It can be done, but it’s quite messy and we’re here to learn reinforcement learning so we don’t need to figure out perfect analytical solutions anymore. Luckily, we get this value directly from the book\n\nIt [greedy algorithm] achieved a reward-per-step of only about \\(1\\), compared with the best possible of about \\(1.55\\) on this testbed (Sutton and Barto 2018, 29).\n\nGreat—they’ve done the work for us. Selecting the optimal action gives an average reward of \\(1.55\\). Selecting a random action has an average reward of \\(0\\) because it’s basically drawing a sample from a normal distribution with mean \\(0\\). That gives:\n\\[\n\\mathbb{E}[R_t] = (1-\\varepsilon) 1.55 + \\varepsilon 0 = 1.55 (1-\\varepsilon)\n\\]\nThis results in \\(1.40\\) for \\(\\varepsilon = 0.1\\) and \\(1.53\\) for \\(\\varepsilon = 0.01\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#sec-incremental-implementation",
    "href": "chapters/02-multi-armed-bandits.html#sec-incremental-implementation",
    "title": "2  Multi-armed Bandits",
    "section": "2.4 Incremental Implementation",
    "text": "2.4 Incremental Implementation\nThe sample average can be updated incrementally using: \\[\nQ_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n].\n\\tag{2.1}\\]\nThis is an instance of a general pattern that is central to reinforcement learning: \\[\n\\text{NewEstimate} \\gets \\text{OldEstimate} + \\text{StepSize}\n\\Big[\\overbrace{\n    \\text{Target} - \\text{OldEstimate}\n    }^\\text{error} \\Big]\n\\]\nI especially like how nice it looks in python. In value-based algorithms, this typically corresponds to the following line of code:\n\nQ[action] += α * (reward - Q[action])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#tracking-a-nonstationary-problem",
    "href": "chapters/02-multi-armed-bandits.html#tracking-a-nonstationary-problem",
    "title": "2  Multi-armed Bandits",
    "section": "2.5 Tracking a Nonstationary Problem",
    "text": "2.5 Tracking a Nonstationary Problem\nTo avoid the learning rate decreasing over time—at the cost of convergence—we can use a constant step size \\(\\alpha \\in (0,1]\\). \\[\nQ_{n+1} := Q_n + \\alpha \\Big[ R_n - Q_n \\Big],\n\\tag{2.2}\\]\nfor \\(n \\geq 1\\) and \\(Q_1\\) is our given initial estimate.\nThis can be phrased as a recurrence relation of the form \\(Q_n = \\sum_{i=0}^n \\gamma^{n-i} r_i\\), as discussed in the appendix Section 2.11.9: We add \\(Q_0\\) and \\(R_0\\). Then, Equation 2.2 is equivalent to: \\[\nQ_{n+1} = \\alpha R_n + (1 - \\alpha) Q_n \\quad \\text{and} \\quad Q_0 = 0,\n\\tag{2.3}\\]\nThis is a recurrence relation of the form Equation 2.18 and thus has the explicit form \\[\nQ_{n+1} = \\sum_{i=0}^n (1 - \\alpha)^{n-i} \\alpha R_{i}\n\\]\nSubstituting \\(R_0 = \\frac{Q_1}{\\alpha}\\)—so that \\(Q_1\\) becomes our arbitrary initial value—yields the form used by Sutton and Barto: \\[\nQ_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} R_i\n\\tag{2.4}\\]\nThis is a weighted average of the random variables involved, as the weights sum to 1. The sum of the weights for the \\(R_i\\) is (using the geometric series identity Equation 2.14): \\[\n\\begin{split}\n\\sum_{i=1}^n \\alpha (1- \\alpha)^{n-i} &= \\alpha \\sum_{i=o}^{n-1}(1-\\alpha)^i \\\\\n&= \\alpha \\frac{1 - (1-\\alpha)^n}{\\alpha}\\\\\n&= 1 - (1 - \\alpha)^n.\n\\end{split}\n\\]\nThus, the total weight sums to 1: \\[\n\\begin{split}\n(1-\\alpha)^n + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} &= 1\n\\end{split}\n\\]\nThe \\(Q_n\\) are estimators for the true action value \\(q_*\\). And depending on how we determine the \\(Q_n\\), they have different qualities. We note that the \\(R_i\\) are IID with mean \\(q_*\\) and variance \\(\\sigma^2\\). (I refer to the appendix Section 2.11 for more information about all the new terms appearing all of a sudden, as this has gotten quite a bit more technical)\nIf \\(Q_n\\)​ is the sample average, the estimator is unbiased, that is \\[\n\\mathbb{E}[Q_n] = q_* \\quad \\text{for all } n \\in \\mathbb{N}.\n\\] Which is easy to show. Its mean squared error \\(\\mathrm{MSE}(Q_n) := \\mathbb{E}[(Q_n - q_*)^2]\\) is decreasing (Lemma 2.4): \\[\n\\mathrm{MSE}(Q_n) = \\frac{\\sigma^2}{n}.\n\\]\nIf the \\(Q_n\\) are calculated using a constant step size, they are biased (there is always a small dependency on \\(Q_1\\)): \\[\n\\begin{split}\n\\mathbb{E}[Q_{n+1}] &=  (1-\\alpha)^n Q_1 + q\\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i}   \\\\\n&= (1-\\alpha)^n Q_1 + q (1 - (1 - \\alpha)^n)\n\\end{split}\n\\tag{2.5}\\]\nAnd even though they are asymptotically unbiased, i.e., \\(\\lim_{n\\to\\infty} \\mathbb{E}[Q_{n}] = q\\), their mean squared error is bounded away from zero (Lemma 2.5): \\[\n\\mathrm{MSE}(Q_n) &gt; \\sigma^2 \\frac{\\alpha}{2-\\alpha}.\n\\]\nIt’s hard for me to translate these stochastic results in statistic behaviour, but I think that means that constant step size will end up wiggling around the true value.\nLet’s define the constant step size agent to compare it with the sample average method later.\n\n# === the constant step bandit agent ===\nclass ConstantStepBanditAgent:\n    def __init__(self, Q1, α, ε, seed=None):\n        self.rng = np.random.default_rng(seed)\n        self.num_actions = len(Q1)\n        self.Q1 = Q1\n        self.α = α\n        self.ε = ε\n        self.reset()\n\n    def reset(self):\n        self.Q = self.Q1.copy()\n\n    def act(self, bandit):\n        # ε-greedy action selection\n        if self.rng.random() &lt; self.ε:\n            action = self.rng.integers(self.num_actions)\n        else:\n            action = np.argmax(self.Q)\n\n        # take action\n        reward = bandit.pull_arm(action)\n\n        # update value estimate\n        self.Q[action] += self.α * (reward - self.Q[action])\n\n        return (action, reward)\n\n\nExercise 2.4 If the step-size parameters, \\(\\alpha_n\\), are not constant, then the estimate \\(Q_n\\) is a weighted average of previously received rewards with a weighting different from that given by Equation 2.4. What is the weighting on each prior reward for the general case, analogous to Equation 2.4, in terms of the sequence of step-size parameters.\n\n\nSolution 2.4. The update rule for non-constant step size has \\(\\alpha_n\\) depending on the step. \\[\nQ_{n+1} = Q_n + \\alpha_n \\Big[ R_n - Q_n \\Big]\n\\tag{2.6}\\]\nIn this case the weighted average is given by \\[\nQ_{n+1} = \\left( \\prod_{j=1}^n 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^n \\alpha_i \\left( \\prod_{j=i+1}^n 1 - \\alpha_j \\right) R_i\n\\tag{2.7}\\]\nThis explicit form can be verified inductively. For \\(n=0\\), we get \\(Q_1\\) on both sides.\nFor the induction step we have \\[\n\\begin{split}\nQ_{n+1} &= Q_n + \\alpha_n \\Big[ R_n - Q_n \\Big] \\\\\n&= \\alpha_n R_n + (1 - \\alpha_n) Q_n \\\\\n&= \\alpha_n R_n + (1 - \\alpha_n) \\Big[ \\left( \\prod_{j=1}^{n-1} 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^{n-1} \\alpha_i \\left( \\prod_{j=i+1}^{n-1} 1 - \\alpha_j \\right) R_i \\Big] \\\\\n&= \\left( \\prod_{j=1}^n 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^n \\alpha_i \\left( \\prod_{j=i+1}^n 1 - \\alpha_j \\right) R_i\n\\end{split}\n\\]\nWe also note that in this general setting, Equation 2.7 is still a weighted average. We could prove it by induction or use a little trick. If we set \\(Q_1 = 1\\) and \\(R_n = 1\\) for all \\(n\\) in the recurrence relation Equation 2.6 we see that each \\(Q_n = 1\\). If we do the same in the explicit formula Equation 2.7 we see that each \\(Q_n\\) is equal to the sum of the weights. Therefore, the weights sum up to \\(1\\).\n\n\nExercise 2.5 Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the \\(q_\\star(a)\\) start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the \\(q_\\star(a)\\) on each step). Prepare plots like Figure Figure 2.1 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, \\(\\alpha = 0.1\\). Use \\(\\epsilon = 0.1\\) and longer runs, say of 10,000 steps\n\n\nSolution 2.5. Alright, let’s do a little experiment, just as they told us.\n\n\nCode\n# === battle between sample average and constant step ===\nconfig = Config(\n    bandit_setup_mu=0,\n    bandit_setup_sd=0,\n    bandit_value_drift=True,\n    bandit_value_drift_mu=0,\n    bandit_value_drift_sd=0.01,\n    exp_steps=10_000,\n    exp_runs=100,\n)\n\nagent0 = SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms, dtype=float), ε=0.1, seed=config.exp_seed)\nagent1 = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), α=0.1, ε=0.1, seed=config.exp_seed\n)\nagents = [agent0, agent1]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\"sample averages (ε=0.1)\", \"constant step-size (α=0.1, ε=0.1)\"]\nplot_average_reward(avg_rwd, labels=labels)\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot surprisingly, we can see how much the sample average agent struggles to keep up. For longer episode lengths, the problem only gets worse. Eventually, it will be completely out of touch with the world, like an old man unable to keep up with the times.\nHowever, it remains unclear exactly how rapidly the sample average method will deteriorate to an unacceptable level. The results from the final exercise of this chapter (Exercise 2.11) indicate that this deterioration may take a significant number of steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#optimistic-initial-values",
    "href": "chapters/02-multi-armed-bandits.html#optimistic-initial-values",
    "title": "2  Multi-armed Bandits",
    "section": "2.6 Optimistic Initial Values",
    "text": "2.6 Optimistic Initial Values\nWe later need the following figure comparing an optimistic greedy agent and a realistic \\(\\varepsilon\\)-greedy agent.\n\n\nCode\n# === realism vs optimism ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1_000\n)\nagent_optimistic_greedy = ConstantStepBanditAgent(\n    Q1=np.full(config.bandit_num_arms, 5.0, dtype=float), α=0.1, ε=0.0, seed=config.exp_seed\n)\nagent_realistic_ε_greedy = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), α=0.1, ε=0.1, seed=config.exp_seed\n)\n\nagents = [agent_optimistic_greedy, agent_realistic_ε_greedy]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"optimistic,greedy (Q1=5, ε=0, α=0.1)\",\n    \"realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.2: This is like Figure 2.3 (Sutton and Barto 2018): the effect of optimistic initial action-value estimates.\n\n\n\n\n\n\nExercise 2.6 (Mysterious Spikes) The results shown in Figure 2.2 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n\n\nSolution 2.6. We have the hard-earned luxury that we can zoom in on Figure 2.2:\n\n\nCode\n# === realism vs optimism zoomed in ===\nconfig = Config(\n    exp_steps=30,\n    exp_runs=5_000,\n)\nagent_optimistic_greedy = ConstantStepBanditAgent(\n    Q1=np.full(config.bandit_num_arms, 5.0, dtype=float), α=0.1, ε=0.0, seed=config.exp_seed\n)\nagent_realistic_ε_greedy = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), α=0.1, ε=0.1, seed=config.exp_seed\n)\n\nagents = [agent_optimistic_greedy, agent_realistic_ε_greedy]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"optimistic,greedy (Q1=5, ε=0, α=0.1)\",\n    \"realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe spike occurs at step 11. Essentially, the optimistic method samples all actions once (poor performance), and then selects the action with the best result (good performance, with a success rate of over 40%). However, regardless of the outcome (which likely pales in comparison to the current Q-values, which are still likely greater than 4), the method returns to exploring all 10 actions again. This leads to poor performance once more. Around step 22, there is another spike for similar reasons, but this time smaller and more spread out.\n\n\nExercise 2.7 (Unbiased Constant-Step-Size Trick) In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of onstant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of \\[\n\\beta_n := \\alpha / \\bar{o}_n\n\\] to process the \\(n\\)-th reward for a particular action, where \\(\\alpha &gt; 0\\) is a conventional constant step size, and \\(\\bar{o}_n\\) is a trace of one that starts at \\(0\\): \\[\n\\bar{o}_n := \\bar{o}_{n-1} + \\alpha (1 - \\bar{o}_{n-1}), \\text{ for } n \\geq 1, \\text{ with } \\bar{o}_0 := 0\n\\] Carry out an analyises like that in Equation 2.4 to show that \\(Q_n\\) is an exponential recency-weighted average without initial bias.\n\n\nSolution 2.7. My first question when I saw this was, “What’s up with that strange name \\(\\bar{o}_n\\)?”” I guess it could be something like “the weighted average of ones”, maybe? Well, whatever. Let’s crack on.\nWhen we rewrite \\(\\bar{o}_{n+1}\\) as a recurrence relation for \\(\\frac{\\bar{o}_{n+1}}{\\alpha}\\) \\[\n\\frac{\\bar{o}_{n+1}}{\\alpha} = 1 + (1 - \\alpha) \\frac{\\bar{o}_n}{\\alpha}\n\\] we see that it is just the recurrence relation for a geometric series as in Equation 2.13 for \\(\\gamma = 1-\\alpha\\). Thus we get \\[\n\\bar{o}_n = \\alpha\\sum_{i=0}^{n-1} (1 - \\alpha)^{i} = 1 - (1-\\alpha)^n\n\\] and \\(\\beta_n = \\frac{\\alpha}{1-(1-\\alpha)^n}\\)\n(btw. this is such a complicated way to define \\(\\beta_n\\) and I don’t understand why actually.)\nIn particular we have that \\(\\beta_1 = 1\\), which makes the influence of \\(Q_1\\) disappears after the first reward \\(R_1\\) is received: \\(Q_2 = Q_1 + 1 [ R_1 - Q_1] = R_1\\). Great!\nScaling the \\(\\alpha\\) by the \\(\\bar{o}_n\\) has an additional nicer effect. I don’t quite understand how, but we can calculate it.\nFrom Exercise 2.4 we know \\[\nQ_{n+1} = Q_1 \\prod_{j=1}^n (1-\\beta_j)\n+ \\sum_{i=1}^n  R_i \\beta_i \\prod_{j=i+1}^n (1- \\beta_j )\n\\]\nThere is a nice form for these products \\[\n\\prod_{j=i}^n (1 - \\beta_j) = (1-\\alpha)^{n-j+1} \\frac{\\bar{o}_{i-1}}{\\bar{o}_n}\n\\]\nsince they are telescoping using \\[\n\\begin{split}\n1- \\beta_j &= 1 - \\frac{\\alpha}{\\bar{o}_j} = \\frac{\\bar{o}_j - \\alpha}{\\bar{o}_j}\\\\\n&= \\frac{\\alpha + (1-\\alpha) \\bar{o}_{j-1}}{\\bar{o}_j} = (1-\\alpha)\\frac{\\bar{o}_{j-1}}{\\bar{o}_j}.\n\\end{split}\n\\]\nThis gives the following closed form for \\(Q_{n+1}\\) \\[\nQ_{n+1} = \\frac{\\alpha}{1 - (1-\\alpha)^n}\\sum_{i=1}^n R_i (1-\\alpha)^{n-i}\n\\]\nWe can see that the weight given to any reward \\(R_i\\)​ decreases exponentially.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#upper-confidence-bound-action-selection",
    "href": "chapters/02-multi-armed-bandits.html#upper-confidence-bound-action-selection",
    "title": "2  Multi-armed Bandits",
    "section": "2.7 Upper-Confidence-Bound Action Selection",
    "text": "2.7 Upper-Confidence-Bound Action Selection\nWe have the opportunity to introduce a new agent here. The update rule remains the same (I assume sample average), but the action selection is more informed compared to \\(\\varepsilon\\)-greedy algorithms.\n\\[\nA_t := \\mathrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{ \\frac{\\ln t}{N_t(a)} }\\right]\n\\]\nwhere \\(N_t(a)\\) is the number of times that action has been selected, c &gt; 0 controls the exploration (similar to \\(\\varepsilon\\)).\nIf an action has not been selected even once, i.e., \\(N_t(a)=0\\), then \\(a\\) is considered to be a maximizing action. (In our case, this means that in the first few steps, all actions have to be selected once, and only after that does the UCB-based action selection kick in.)\nBy the way, I have no idea where the UCB formulation comes from, but at least it looks fancy (and reasonable), and we can implement it:\n\n# === the ucb bandit agent ===\nclass UcbBanditAgent:\n    def __init__(self, num_actions, c, seed=None):\n        self.num_actions = num_actions\n        self.c = c  # exploration parameter\n        self.reset()\n        self.rng = np.random.default_rng(seed)\n\n    def reset(self):\n        self.t = 0\n        self.Q = np.zeros(self.num_actions, dtype=float)\n        self.counts = np.zeros(self.num_actions, dtype=int)\n\n    def act(self, bandit):\n        self.t += 1\n\n        # upper-Confidence-Bound Action Selection\n        if self.t &lt;= self.num_actions:\n            # if not all actions have been tried yet, select an untried action\n            action = self._choose_untaken_action()\n        else:\n            # calculate UCB values for each action\n            ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / (self.counts))\n            # select the action with the highest UCB value\n            action = np.argmax(ucb_values)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update count and value estimate\n        self.counts[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.counts[action]\n\n        return (action, reward)\n\n    def _choose_untaken_action(self):\n        return self.rng.choice(np.where(self.counts == 0)[0])\n\nLet’s recreate the figure illustrating UCB action selection performance, which we’ll need for the next exercise.\n\n\nCode\n# === ucb agent performance ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1_000,\n)\nagent_ucb = UcbBanditAgent(num_actions=config.bandit_num_arms, c=2, seed=config.exp_seed)\nagent_ε_greedy = SampleAverageBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), ε=0.1, seed=config.exp_seed\n)\n\nagents = [agent_ucb, agent_ε_greedy]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"ucb (c=2, α=0.1)\",\n    \"ε-greedy (ε=0.1, α=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: This is like Figure 2.4 (Sutton and Barto 2018): average performance of UCB action selection.\n\n\n\n\n\n\nExercise 2.8 (UCB Spikes) In Figure 2.3 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if \\(c = 1\\), then the spike is less prominent.\n\n\nSolution 2.8. I think the answer is similar to the answer to Exercise 2.6. The first \\(10\\) steps the UCB algorithm tries out all actions. Then on step \\(11\\) it will select the one that scored highest, which is quite a decent strategy. But then because of the \\(N_t​(a)\\) in the denominator it is back to exploring.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#gradient-bandit-algorithms",
    "href": "chapters/02-multi-armed-bandits.html#gradient-bandit-algorithms",
    "title": "2  Multi-armed Bandits",
    "section": "2.8 Gradient Bandit Algorithms",
    "text": "2.8 Gradient Bandit Algorithms\nThis introduces a novel method that is not value-based; instead, it directly aims to select the best actions. The agent maintains numerical preferences, denoted by \\(H_t​(a)\\), rather than estimates of the action values.\nFor action selection, gradient bandit uses the softmax distribution: \\[\n\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}.\n\\tag{2.8}\\]\nShifting all preferences by a constant \\(C\\) doesn’t affect \\(\\pi_t(a)\\): \\[\n\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} = \\frac{e^Ce^{H_t(a)}}{\\sum_{b=1}^k e^Ce^{H_t(b)}} = \\frac{e^{H_t(a)+C}}{\\sum_{b=1}^k e^{H_t(b)+C}}\n\\]\nFor learning, gradient bandit uses the following rule: \\[\n\\begin{split}\nH_{t+1}(a) &:= H_t(a) + \\alpha (R_t - \\bar{R}_t) (\\mathbb{I}_{a = A_t} - \\pi_t(a))\n\\end{split}\n\\tag{2.9}\\]\nNow, let’s implement this gradient bandit algorithm:\n\n# === the gradient agent ===\nclass GradientBanditAgent:\n    def __init__(self, H1, α, baseline=True, seed=None):\n        self.num_actions = len(H1) \n        self.α = α \n        self.H1 = np.asarray(H1, dtype=np.float64)  # initial preferences\n        self.baseline = baseline # apply average reward baseline\n        self.reset()\n        self.rng = np.random.default_rng(seed)\n\n    def reset(self):\n        self.H = self.H1.copy()  \n        self.avg_reward = 0 \n        self.t = 0  # step count\n\n    def act(self, bandit):\n        self.t += 1\n\n        # select action using softmax\n        action_probs = GradientBanditAgent.softmax(self.H)\n        action = self.rng.choice(self.num_actions, p=action_probs)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update average reward\n        if self.baseline:\n            self.avg_reward += (reward - self.avg_reward) / self.t\n\n        # update action preferences\n        advantage = reward - self.avg_reward # avg_reward = 0 if baseline = false\n        one_hot_action = np.eye(self.num_actions)[action]\n        self.H += self.α * advantage * (one_hot_action - action_probs)\n\n        return action, reward\n\n    @staticmethod\n    def softmax(x):\n        # shift vector by max(x) to avoid hughe numbers.\n        # This is basically using the fact that softmax(x) = softmax(x + C)\n        exp_x = np.exp(x - np.max(x))\n        return exp_x / np.sum(exp_x)\n\nSutton and Barto (Sutton and Barto 2018, 37) emphasize the importance of the baseline \\(\\bar{R}_t\\)​ in the update forumla and show that performance drops without it. In the derivation of the update as a form of stochastic gradient ascent, the baseline can be chosen arbitrarily (see Section 2.8.1). Whether or not a baseline is used, the resulting updates are unbiased estimators of the gradient. I assume, the baseline serves to reduce the variance of the estimator, although I have no idea about the maths behind it.\nHere we recreat Figure 2.5 (Sutton and Barto 2018), which shows how drastically the running average baseline can improve performance.\n\n\nCode\n# === gradient bandit performance ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=50,\n    bandit_setup_mu=4,\n)\n\nalphas = [0.1, 0.4]\nagents = [GradientBanditAgent(H1=np.zeros(config.bandit_num_arms), α=α, seed=config.exp_seed) for α in alphas] + [GradientBanditAgent(H1=np.zeros(config.bandit_num_arms), α=α, baseline=False, seed=config.exp_seed) for α in alphas]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [f\"α = {α}, with baseline\" for α in alphas] + [f\"α = {α}, without baseline\" for α in alphas]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: This is like Figure 2.5 (Sutton and Barto 2018): average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the \\(q_*(a)\\) are chosen to be near \\(+4\\) rather than near zero. (we averaged over 50 runs because it gives this cool jaggedy looking graph)\n\n\n\n\n\n\nExercise 2.9 Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.\n\n\nSolution 2.9. The logistic function is defined as \\[\n\\sigma(x) := \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}.\n\\] If we map the two preferences \\(H(a_1), H(a_2)\\) to a single value \\(\\Delta = H(a_1) - H(a_2)\\) then \\[\n\\pi(a_1) = \\frac{e^{H(a_1)}}{e^{H(a_1)} + e^{H(a_2)}} = \\frac{e^{H(a_1)-H(a_2)}}{e^{H(a_1)-H(a_2)} + 1} = \\sigma(\\Delta)\n\\] and similarly \\[\n\\pi(a_2) = \\sigma(-\\Delta).\n\\]\n\n\n2.8.1 the bandit gradient algorithm as stochastic gradient ascent\nThis next subsection is devoted to the (I imagine) infamous brown box (Sutton and Barto 2018, 38–40), which marks a significant leap in theoretical complexity. It shows how the update rule arises as a form of stochastic gradient ascent.\nWe’ll retrace their steps as a self-contained argument and flag two subtle points: the role of the baseline and the randomness in the preference vector.\n\n1. quick recap of gradient ascent\nLet \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\) be a differentiable function. We want to produce points \\(\\mathbf{x}_0, \\mathbf{x}_1, \\dots\\) that maximise \\(f\\). Gradient ascent updates the current point \\(\\mathbf{x}^{(t)}\\) in the direction of the gradient: \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\; \\nabla f(\\mathbf{x})\\big|_{\\mathbf{x}^{(t)}}\n\\] where \\(\\alpha &gt; 0\\) is the step size.\nIn one dimension, this becomes: \\[\nx_i^{(t+1)} = x_i^{(t)} + \\alpha \\; \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\bigg|_{x_i^{(t)}}\n\\]\nIn stochastic gradient ascent, we aim to maximise the expected value of a random vector \\(\\mathbf{R}\\) (a vector whose values are random variables), whose distribution depends on a parameter vector \\(\\mathbf{x}\\). That is, the underlying probability space \\((\\Omega, \\mathrm{Pr}_{\\mathbf{x}})\\) is parameterised by \\(\\mathbf{x}\\). Here \\(f\\) is \\(\\mathbb{E}_{\\mathbf{x}}[\\mathbf{R}]\\) where we have explicitly indicated the dependence of the expected value on \\(\\mathbf{x}\\). So this is still a deterministic gradient ascent step—although the true gradient is unknown to the algorithm and must later be estimated via sampling: \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\cdot \\nabla \\mathbb{E}_{\\mathbf{x}}[\\mathbf{R}]\\big|_{\\mathbf{x}^{(t)}}\n\\]\nOur goal is to cast the gradient bandit update in this framework.\n\n\n2. setting up the problem\nIn the gradient bandit algorithm, the parameters we adjust are the action preferences \\(H_t​(a)\\). These determine the policy via the softmax distribution: \\[\n\\pi_H(a) = \\frac{e^{H(a)}}{\\sum_{b \\in \\mathcal{A}} e^{H(b)}}\n\\]\nThis shows how our parameter \\(H\\) determines the probability space: it determines the probability distribution for \\(A_t\\) \\(\\pi_{H_t}(a) := \\mathrm{Pr}_{H_t}(A_t = a)\\) the probabilities for rewards given an action are determined by the system and independent of the parameters \\(q_*(a) := \\mathbb{E}[R_t \\mid A_t = a]\\).\nWith this set-up, it is clear how \\(\\mathbb{E}_{H_t}[R_t]\\) is a function on \\(H_t\\) \\[\n\\begin{split}\n\\mathbb{E}_{H_t}[R_t] &= \\sum_{b} \\mathrm{Pr}_{H_t}(A_t = b) \\cdot \\mathbb{E}[R_t \\mid A_t = b] \\\\\n&= \\sum_{b} \\pi_{H_t}(b) q_*(b)\n\\end{split}\n\\] (if you are unsure about this, check Theorem 2.4).\nIn this context, each action \\(a\\in\\mathcal{A}\\) corresponds to a coordinate in our parameter vector \\(H\\), so the gradient update in one dimension \\(a \\in \\mathcal{A}\\) becomes: \\[\nH_{t+1}(a) = H_t(a) + \\alpha \\frac{\\partial \\mathbf{E}[R_t]}{\\partial H(a)}\\bigg|_{H_t(a)}\n\\tag{2.10}\\]\n\n\n3. calculating the gradient\nNow look at the row of the gradient for \\(a \\in \\mathcal{A}\\): \\[\n\\begin{split}\n\\frac{\\partial \\mathbb{E}_{H}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)} &=\n\\sum_{b} q_*(b) \\cdot \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)} \\\\\n&= \\sum_{b} (q_*(b) - B_t) \\cdot \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)}\n\\end{split}\n\\] We could add here any scalar (called the baseline) as \\(\\sum_b \\pi_H(b) = 1\\) and thus \\(\\sum_{b} \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Big|_{H'(a)} = 0\\). Note that for this argument to work \\(B_t\\) cannot depend on \\(b\\).\nTo simplify that further we use the softmax derivative, (which is derived in Sutton and Barto): \\[\n\\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)}\n= \\pi_{H_t}(b) (\\mathbb{I}_{a = b} - \\pi_{H_t}(a))\n\\]\nSo we have \\[\n\\begin{split}\n\\frac{\\partial \\mathbb{E}_{H}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)} &=\n\\sum_{b} (q_*(b) - B_t) \\cdot  (\\mathbb{I}_{a = b} - \\pi_{H_t}(a)) \\pi_{H_t}(b) \\\\\n&= \\mathbb{E}_{H_t}[(q_*(A_t)- B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})] \\\\\n&= \\mathbb{E}_{H_t}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})].\n\\end{split}\n\\] We get the second equality by applying the law of the unconscious statistician in reverse (Theorem 2.1), and yes the expression inside the expectation is all just a deterministic function of \\(A_t\\). In the final equality, we substituted \\(R_t\\) for \\(q_*(A_t)\\) using that \\(q_*(A_t) = \\mathbb{E}_{H_t}[R_t]\\), and the law of iterated expectations justifies the substitution under the outer expectation.\nBefore going to the next step, you might want to check the plausibility of \\[\n\\frac{\\partial \\mathbb{E}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)}\n= \\mathbb{E}_{H_t}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})].\n\\tag{2.11}\\] On the left we have basically a number, the value of the derivative at some point, and on the right we have an expected value with a parameter \\(B_t\\). So the parameter \\(B_t\\) must cancel out somehow. Which it does indeed, you can check this for yourself.\n\n\n4. one-step update and baseline trade-off\nBy plugging Equation 2.11 into Equation 2.10 we get the deterministic gradient ascent update: \\[\nH_{t+1}(a) = H_t(a) + \\alpha \\mathbb{E}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})]\n\\]\nReplacing the expectation by the single sample \\(A_t, R_t\\) yields the stochastic gradient update: \\[\nH_{t+1}(a) = H_t(a) + \\alpha  (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})\n\\tag{2.12}\\]\nTo get Equation 2.9 we have to substitute \\(\\bar{R}_t\\) for \\(B_t\\). Which requires some discussion first.\nSutton and Barto make clear that \\(\\bar{R}_t\\) depend on \\(R_t\\):\n\n\\(\\bar{R}_t\\) is the average of all the rewards up through and including time \\(t\\) (Sutton and Barto 2018, 37).\n\nIf we use that in Equation 2.12 for \\(B_t\\) we are not using an unbiased estimator anymore. This is tightly coupled with the fact that when we introduced \\(B_t\\) we required it not to depend on \\(b\\) which does later play the role of \\(A_t\\), and \\(\\bar{R}_t\\) depends on \\(R_t\\) which depends on \\(A_t\\).\nMostl likely there is a good reason for using \\(\\bar{R}_t\\), but I don’t know the mathematical motivation. (As I said, maybe some variance reduction)\nHowever I’m saying the derivation of their update formula is wrong1 as they frame it as an unbiased estimator.\n\n\ncomment on the time parameter\nWe have keept the time parameter in the derivation to stick to the style of Sutton and Barto. We could have equally done the same derivation for \\(H'\\) (new) and \\(H\\) (old). However, conceptually keeping the time parameter is a bit shaky. Where does the \\(H_t\\) come from? If we think this through then \\(H_t\\) actually becomes part of the whole system (enviroment + agent) and thus is a random vector. And then it’s harder for me to think about how to analyse this to obtain the update formula. Once the update rule is fixed, we can then treat the entire system (agent and environment) as stochastic without any problems.\nIf correct or not Sutton-Barto derive this term for the gradient \\[\n\\mathbb{E}\\big[ (R_t - \\bar{R}_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t}(a)) \\big].\n\\] Which might look a bit fishy because maybe \\(\\mathbb{E}\\big[ R_t - \\bar{R}_t]\\) is always \\(0\\). But it is usually not, because the policy \\(\\pi\\) is not stationary, it get’s updated at every step and thus the policy’s evolution decouples the two expectations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#associative-search-contextual-bandits",
    "href": "chapters/02-multi-armed-bandits.html#associative-search-contextual-bandits",
    "title": "2  Multi-armed Bandits",
    "section": "2.9 Associative Search (Contextual Bandits)",
    "text": "2.9 Associative Search (Contextual Bandits)\n\nExercise 2.10 Suppose you face a \\(2\\)-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions \\(1\\) and \\(2\\) are respectively \\(0.1\\) and \\(0.2\\) with probability \\(0.5\\) (case A), and \\(0.9\\) and \\(0.8\\) with probability \\(0.5\\) (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?\n\n\nSolution 2.10. We are presented with two scenarios and the questions “What is the best strategy?” and “What is its expected reward?” for each scenario.\nIn the first scenario, we don’t know whether we are facing Case A or Case B at any given time step. The true values of the actions are as follows: \\[\n\\begin{split}\n\\mathbb{E}[R_t \\mid A_t = 1] &= \\mathrm{Pr}(\\text{Case A}) \\cdot 0.1 + \\mathrm{Pr}(\\text{Case B}) \\cdot 0.9\\\\\n&= 0.5 (0.1 + 0.9) = 0.5\\\\[3ex]\n\\mathbb{E}[R_t \\mid A_t = 2] &= \\mathrm{Pr}(\\text{Case A}) \\cdot 0.2 + \\mathrm{Pr}(\\text{Case B}) \\cdot 0.8\\\\\n&= 0.5 (0.2 + 0.8) = 0.5\n\\end{split}\n\\]\nSince both actions have the same expected reward of 0.5, it does not matter which action is chosen. Thus, any algorithm is optimal and has an expected of 0.5.\nIn the second scenario, we know whether we are facing Case A or Case B. The expected reward under the optimal strategy, which always chooses the action with the highest expected value, is: \\[\n\\mathbb{E}_{\\pi_*}[ R_t ] = \\overbrace{0.5 \\cdot 0.2}^{\\text{case A}} + \\overbrace{0.5 \\cdot 0.9}^{\\text{case B}} = 0.55\n\\]\nTo achieve this expected reward, we need to keep track of the two bandit problems separately and maximise their rewards. How to do this approximately is the topic of the whole chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#summary",
    "href": "chapters/02-multi-armed-bandits.html#summary",
    "title": "2  Multi-armed Bandits",
    "section": "2.10 Summary",
    "text": "2.10 Summary\n\nExercise 2.11 Make a figure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size \\(\\varepsilon\\)-greedy algorithm with \\(\\alpha\\)= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.\n\n\nSolution 2.11. That last exercise is a banger to finish with. There’s a lot going on here. I’ll explain what was difficult for me at the end, but let’s start with what I actually did and what we can see in the graph.\nI ran a parameter sweep for four different bandit agents over 200 episodes of 300,000 steps each. For each episode, I only looked at the average reward over the last 50,000 steps to measure steady-state performance.\n\n\nCode\n# === Parameter sweep for nonstationary bandit ===\n\"\"\"\nWe compare 4 bandit agents:\n0 - ε-greedy sample-average, parameter = ε\n1 - ε-greedy constant α = 0.1, parameter = ε\n2 - Gradient ascent with baseline, parameter = α\n3 - Gradient ascent no baseline, parameter = α\n\"\"\"\nfrom functools import partial\nfrom matplotlib.ticker import FuncFormatter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# custom import\nfrom scripts.parameter_study.episode_mean import episode_mean\n\n# --- Global config\nCONFIG = dict(\n    seed=1000000,\n    num_arms=10,\n    steps=300_000,\n    keep=50_000,\n    runs=200,\n    q_0=10,\n    drift_sd=0.1,\n)\n\nAGENTS = {\n    \"ε in ε-greedy (sample-avg)\": dict(\n        id=0,\n        x_name=\"ε\",\n        x_vals=[2**-k for k in (12, 9, 6, 3, 1, 0)],\n        fixed=dict(),\n    ),\n    \"ε in ε-greedy (α = 0.1)\": dict(\n        id=1,\n        x_name=\"ε\",\n        x_vals=[2**-k for k in (12, 9, 6, 3, 1, 0)],\n        fixed=dict(α=0.1),\n    ),\n    \"α in gradient ascent\": dict(\n        id=2,\n        x_name=\"α\",\n        x_vals=[2**-k for k in (20, 18, 14, 9, 6, 3, 1)],\n        fixed=dict(),\n    ),\n    \"α in gradient ascent (no base)\": dict(\n        id=3,\n        x_name=\"α\",\n        x_vals=[2**-k for k in (20, 18, 14, 9, 6, 3, 1)],\n        fixed=dict(),\n    ),\n}\n\n\n# --- Experiment helper\ndef evaluate(agent_type: int, **kwargs) -&gt; float:\n    \"\"\"Mean reward over the last *keep* steps averaged across *runs* runs.\"\"\"\n    args = {**CONFIG, **kwargs}\n    rng = np.random.default_rng(args[\"seed\"])\n    seeds = rng.integers(0, 2_000_000, size=args[\"runs\"])\n\n    rewards = [\n        episode_mean(\n            agent_type,\n            args[\"num_arms\"],\n            args[\"steps\"],\n            args[\"keep\"],\n            args[\"q_0\"],\n            1,  # bandit_action_sd\n            0,  # drift_mu\n            args[\"drift_sd\"],\n            seed,\n            kwargs.get(\"ε\", 0.1),\n            kwargs.get(\"α\", 0.1),\n        )\n        for seed in seeds\n    ]\n    return float(np.mean(rewards))\n\n\n# --- run the sweeps\nresults = {}\nfor label, spec in AGENTS.items():\n    run = partial(evaluate, spec[\"id\"], **spec[\"fixed\"])\n    results[label] = [run(**{spec[\"x_name\"]: x}) for x in spec[\"x_vals\"]]\n\n# --- plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\nmarkers = [\"^\", \"o\", \"s\", \"*\"]  # one per agent\nfor (label, spec), marker in zip(AGENTS.items(), markers):\n    ax.plot(\n        spec[\"x_vals\"],\n        results[label],\n        marker=marker,\n        label=label,\n    )\n\nax.set_xscale(\"log\", base=2)\nax.set_xlabel(\"Exploration / step-size (log scale)\")\nax.set_ylabel(\n    f\"Average reward for the last {CONFIG['keep']:,} steps ({CONFIG['steps']:,} steps total)\"\n)\nax.set_title(f\"Average reward (mean of {CONFIG['runs']} runs)\")\nax.grid(True, which=\"both\", linewidth=0.5)\n\nax.xaxis.set_major_formatter(\n    FuncFormatter(lambda x, _: f\"1/{int(1/x)}\" if x &lt; 1 else str(int(x)))\n)\n\nax.legend()\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\nParameter sweep for a non-stationary bandit, similar to Figure 2.6 (Sutton and Barto 2018). The plot displays the average reward of the last 50,000 steps of a 300,000-step episode, averaged over 200 runs. The bandit was initialized with action-value means set to 10, which drifted according to a normal distribution with a standard deviation of 0.1 at each step. The standard deviation of the reward noise was 1.\n\n\n\n\nThe agents I tested were:\n\n\\(\\varepsilon\\)-greedy with sample-average action values, sweeping over \\(\\varepsilon\\)\n\\(\\varepsilon\\)-greedy with constant step-size (α = 0.1), also sweeping \\(\\varepsilon\\)\ngradient ascent with a sample-average baseline, sweeping \\(\\alpha\\)\ngradient ascent with no baseline, again sweeping \\(\\alpha\\)\n\nTo encourage adaptability, I changed the drift to 0.1.\nLet’s go through some observations:\n\nconstant step-size ε-greedy outperforms the others. Not that surprising.\ngradient ascent underperforms. That surprised me. I’d assumed the gradient method would be a good fit for a drifting environment. However, it does worse than the sample-average ε-greedy.\nsmall α performs best for gradient ascent. This one I don’t have a great explanation for, as very small step sizes typically result in slow learning.\nalso, the baseline doesn’t matter for verly low α. Again no clue.\nsample-average ε-greedy agent is better than I expected. I thought that, with that many steps, it wouldn’t be able to keep up with the changing environment.\nrandom Behaviour for ε = 1. The ε-greedy agents with ε = 1 behaves randomly, as expected. Both agents achieve a reward of 10, which is consistent with random action selection.\n\nSo, this figure raises quite a few questions I can’t yet answer. I think that makes this execrise really good. And maybe in the future I will be able to provide more insights about these findings.\nThis was a lot of work. Running 200 episodes at 300,000 steps each is way too slow in pure Python. I had to offload the inner loop into a separate file and used Numba to JIT-compile it, basically rewriting all the algorithms used in this experiment. That’s also why I don’t want to spend more time trying out gradient ascent with a constant-step baseline. Additionally, I can’t guarantee that there isn’t some subtle bug in the code for the main loop, as I didn’t really test all the algorithms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#sec-appendix-multi-arm-bandits",
    "href": "chapters/02-multi-armed-bandits.html#sec-appendix-multi-arm-bandits",
    "title": "2  Multi-armed Bandits",
    "section": "2.11 appendix",
    "text": "2.11 appendix\nHere are some more details on concepts that came up during this chapter.\n\n2.11.1 distribution\nEvery random variable \\(X\\) has a distribution, denoted \\(p_X\\), which maps each possible value to its probability: \\[\np_X(x) := \\mathrm{Pr}(X = x).\n\\]\nOften, we say \\(X\\) is distributed according to \\(f\\) for a function \\(f \\colon \\mathcal{X} \\to [0,1]\\), which means that \\(f(x) = \\mathrm{Pr}(X = x)\\). We write this as: \\[\nX \\sim f.\n\\]\nTwo random variables \\(X\\) and \\(Y\\) have the same distribution if \\(p_X = p_Y\\).\nThe distribution of \\(X\\) turns \\((\\mathcal{X}, p_X)\\) into a probability space, where \\(p_X\\) is called the pushforward measure.\n\n\n2.11.2 independent and identically distributed random variables\nA very important concept: IID. A collection of random variables \\(X_1, \\dots, X_n\\) is independent and identically distributed (IID) if all random variables have the same probability distribution, and all are mutually independent.\nFormally, this means:\n\n\\(\\mathrm{Pr}(X_i = x) = \\mathrm{Pr}(X_j = x)\\)\n\\(\\mathrm{Pr}(X_i = x,\\, X_j = x') = \\mathrm{Pr}(X_i = x) \\cdot \\mathrm{Pr}(X_j = x')\\)\n\nfor all distinct indices \\(i \\neq j\\).\n\n\n2.11.3 lotus\nThe following theorem is widely known as the “law of the unconscious statistician”. It is fundamental in many calculations as it allows us to compute the expected value of functions of random variables by only knowing the distributions of the random variables.\n\nTheorem 2.1 Let \\(X \\colon \\Omega \\to \\mathcal{X}\\) be a random variable, and let \\(g \\colon \\mathcal{X} \\to \\mathbb{R}\\) be a real-valued function on the result space.\nThen the expected value of \\(g\\) with respect to the pushforward distribution \\(p_X\\) is the same as the expected value of the random variable \\(g(X) := g \\circ X\\) on \\(\\Omega\\): \\[\n\\mathbb{E}[g(X)] = \\sum_{x \\in \\mathcal{X}} g(x)\\, p_X(x)\n\\]\n\n\nProof. Pretty sure this proof could be beautifully visualised: summing over columns is the same as summing over rows. But indicator functions \\(\\mathbb{I}\\) do the trick too.\n\\[\n\\begin{split}\n\\sum_{x \\in \\mathcal{X}} g(x) p_X(x)\n&= \\sum_{x \\in \\mathcal{X}} g(x) \\mathrm{Pr}(X = x) \\\\\n&= \\sum_{x \\in \\mathcal{X}} g(x) \\left( \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) \\mathbb{I}_{X = x} \\right) \\\\\n&= \\sum_{x \\in \\mathcal{X}, \\omega \\in \\Omega} g(x) \\mathrm{Pr}(\\omega) \\mathbb{I}_{X = x} \\\\\n&= \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) \\left( \\sum_{x \\in \\mathcal{X}} g(x) \\mathbb{I}_{X = x} \\right) \\\\\n&= \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) g(X) \\\\\n&= \\mathbb{E}[g(X)]\n\\end{split}\n\\]\n\n\n\n2.11.4 multiplication rule of conditional probabilities\nThe multiplication rule of conditional probabilities is great for manipulating unknown distributions into known distributions.\n\nTheorem 2.2 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R}\\) be random variables. Then \\[\n\\mathrm{Pr}[R = r, A = a] =  \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\] for \\(a \\in \\mathcal{A}\\) and \\(r \\in \\mathcal{R}\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}[R = r, A = a]\n&= \\mathrm{Pr}(A = a) \\frac{\\mathrm{Pr}[R = r, A = a]}{\\mathrm{Pr}(A = a)} \\\\\n&= \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\end{split}\n\\]\n\n\nTheorem 2.3 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R}\\) be random variables. Then \\[\n\\mathrm{Pr}[R =r] = \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\] for \\(r \\in \\mathcal{R}\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}[R =r]\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}[R = r, A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\end{split}\n\\]\n\n\nTheorem 2.4 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R} \\subseteq \\mathbb{R}\\) be random variables. Then \\[\n\\mathbb{E}[R] = \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathbb{E}[R \\mid A = a]\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathbb{E}[R]\n&= \\sum_{r \\in \\mathcal{R}} r \\mathrm{Pr}(R = r) \\\\\n&= \\sum_{r \\in \\mathcal{R}} r \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\sum_{r \\in \\mathcal{R}} r \\mathrm{Pr}[R = r \\mid A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathbb{E}[R \\mid A = a]\n\\end{split}\n\\]\n\n\n\n2.11.5 variance\nThe variance \\(\\mathrm{Var}(X)\\) of a random variable is defined as: \\[\n\\mathrm{Var}(X) := \\mathbb{E}[(X-\\mu)^2]\n\\]\nwhere \\(\\mu = \\mathbb{E}[X]\\) is the mean of \\(X\\).\nIt can be easily shown that \\[\n\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2.\n\\]\n\n\n2.11.6 independent variables\nTwo random variables \\(X,Y\\) are independent if \\[\n\\mathrm{Pr}(X = x, Y = y) = \\mathrm{Pr}(X = x) \\cdot \\mathrm{Pr}(Y = y).\n\\]\nIn this case, the conditioned probabilities are equal to the ordinary probabilities\n\nLemma 2.1 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathrm{Pr}(X = x \\mid Y = y) = \\mathrm{Pr}(X = x).\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}(X = x \\mid Y = y) &= \\frac{\\mathrm{Pr}(X = x, Y = y)}{\\mathrm{Pr}(Y = y)} \\\\\n&= \\frac{\\mathrm{Pr}(X = x) \\mathrm{Pr}(Y = y)}{\\mathrm{Pr}(Y = y)} \\\\\n&= \\mathrm{Pr}(X = x)\n\\end{split}\n\\]\n\n\nLemma 2.2 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathbb{E}(XY) = \\mathbb{E}(X) \\cdot \\mathbb{E}(Y)\n\\]\n\n\nProof. We are cheating a bit here (but not doing anything wrong) and apply LOTUS on two random variables at once. \\[\n\\begin{split}\n\\mathbb{E}[XY] &= \\sum_{x,y} x\\cdot y \\; \\mathrm{Pr}(X = x, Y = y) \\\\\n&= \\left(\\sum_{x} x \\mathrm{Pr}(X = x)\\right) \\cdot \\left(\\sum_{y} y \\mathrm{Pr}(Y = y)\\right) \\\\\n&= \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]\n\\end{split}\n\\]\n\nWe can use this lemma to prove “linearity” for independent variables.\n\nLemma 2.3 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Var}(X + Y) &= \\mathbb{E}[(X+Y)^2] - \\mathbb{E}[X+Y]^2 \\\\\n&= (\\mathbb{E}[X^2] + 2 \\mathbb{E}[XY] + \\mathbb{E}[Y^2]) - (\\mathbb{E}[X]^2 + 2 \\mathbb{E}[X]\\mathbb{E}[Y] + \\mathbb{E}[Y]^2) \\\\\n&= (\\mathbb{E}[X^2] - \\mathbb{E}[X]^2) + (\\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2) \\\\\n&= \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n\\end{split}\n\\]\n\n\nTheorem 2.5 The population mean \\(\\bar{X}_n\\) of IID real-valued random variables \\(X_1, \\dots, X_n\\) has variance \\[\n\\mathrm{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of the \\(X_i\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Var}(\\bar{X}_n) &= \\mathrm{Var}(\\frac{1}{n}\\sum_{i=1}^n X_i) \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n \\mathrm{Var}(X_i)\\\\\n&= \\frac{1}{n^2} n \\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\n\n\n\n2.11.7 estimators\nEstimators are functions used to infer the value of a hidden parameter from observed data.\nI don’t want to create too much theory for estimators. Let’s look at the \\(Q_n\\) and \\(R_i\\) from Section 2.4.\nThe \\(Q_n\\) are somehow based on the \\(R_1, \\dots, R_{n-1}\\) and called estimators for \\(q_*\\).\nThere are some common metrics for determining the quality of an estimator.\n\nbias\nThe bias of \\(Q_n\\) is \\[\n\\mathrm{Bias}(Q_n) = \\mathbb{E}[Q_n] - q_*.\n\\]\nIf this is \\(0\\) then \\(Q_n\\) is unbiased. If the bias disappears asymptotically, then \\(Q_n\\)​ is asymptotically unbiased.\n\n\nmean squared error\nIt is used to indicate how far, on average, the collection of estimates are from the single parameter being estimated. \\[\n\\mathrm{MSE}(Q_n) = \\mathbb{E}[ (Q_n - q_*)^2]\n\\]\nThe mean squared error can be expressed in terms of bias and variance. \\[\n\\mathrm{MSE}(Q_n) = \\mathrm{Var}(Q_n) + \\mathrm{Bias}(Q_n)^2\n\\]\nIn particular, for unbiased estimators, the mean squared error is just the variance.\n\nLemma 2.4 Let \\(Q_n\\) be the sample average of \\(R_1, \\dots, R_n\\). Then \\[\n\\mathrm{MSE}(Q_n) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of the \\(X_i\\).\n\n\nProof. The sample average is unbiased. Thus, its mean squared error is its variance given in Theorem 2.5.\n\n\nLemma 2.5 Let \\(Q_n\\) be the constant step size weighted average of the \\(R_1, \\dots, R_n\\). Then, the Mean Squared Error of \\(Q_{n+1}\\) is given by: \\[\n\\mathrm{MSE}(Q_{n+1}) = \\sigma^2 \\frac{\\alpha}{2-\\alpha} + (1 - \\alpha)^{2n} [(Q_n - q_*)^2 - \\sigma^2\\frac{\\alpha}{2-\\alpha}],\n\\] where \\(\\sigma^2\\) is the variance of the \\(R_i\\).\nIn particular, the MSE is bounded from below by \\[\n\\lim_{n\\to\\infty}\\mathrm{MSE}(Q_n) = \\sigma^2 \\frac{\\alpha}{2-\\alpha}.\n\\]\n\n\nProof. The weighted average \\(Q_{n+1}\\)​ is defined as: \\[\nQ_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i\n\\]\n\nstep 1: compute the expected value\nThis has been done in Equation 2.5 \\[\n\\mathbb{E}(Q_{n+1}) = (1-\\alpha)Q_1 + (1 - (1-\\alpha)^n)q_*\n\\]\n\n\nstep 2: compute the bias of\nUsing \\(\\mathrm{Bias}(Q_{n+1}) = \\mathbb{E}[Q_{n+1}] - q_*\\) we get \\[\n\\begin{split}\n\\mathrm{Bias}(Q_{n+1}) &= (1-\\alpha)Q_1 + (1 - (1-\\alpha)^n)q_* - q_* \\\\\n&= (1-\\alpha)^n [Q_n - q_*].\n\\end{split}\n\\]\n\n\nstep 3: compute the variance\n\\[\n\\begin{split}\n\\mathrm{Var}(Q_{n+1}) &= \\mathrm{Var}\\big((1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i \\big) \\\\\n&= \\sum_{i=1}^n \\alpha^2 \\big((1-\\alpha)^{2}\\big)^{n-i} \\sigma^2 \\\\\n&= \\sigma^2\\alpha^2 \\sum_{i=0}^{n-1} \\big((1-\\alpha)^{2}\\big)^{i} \\\\\n&= \\sigma^2\\alpha^2 \\frac{1 - (1-\\alpha)^{2n}}{1 - (1-\\alpha)^2} \\\\\n&= \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\big(1 - (1-\\alpha)^{2n}\\big).\n\\end{split}\n\\] Here it’s crucial that the \\(R_i\\) are independent.\n\n\n2.11.7.0.1 step 4: compute the mean squared error\nNow we can use \\(\\mathrm{MSE}(Q_{n+1}) = \\mathrm{Var}(Q_{n+1}) + \\mathrm{Bias}(Q_{n+1})^2\\) \\[\n\\begin{split}\n\\mathrm{MSE}(Q_{n+1}) &= \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\big(1 - (1-\\alpha)^{2n}\\big) + \\Big( (1-\\alpha)^n [Q_n - q_*] \\Big)^2 \\\\\n&= \\sigma^2 \\frac{\\alpha}{2-\\alpha} + (1 - \\alpha)^{2n} [(Q_n - q_*)^2 - \\sigma^2\\frac{\\alpha}{2-\\alpha}]\n\\end{split}\n\\]\n\n\n\n\n\n2.11.8 geometric series\nIn the context of reinforcement learning, the concept of discounting naturally requires the notion of geometric series. This series is defined as, \\[\nS(n+1) := \\sum_{i=0}^n \\gamma^i,\n\\]\nwhere \\(\\gamma \\in \\mathbb{R}\\). By convention, an empty sum is considered to be 0, thus \\(S(0)=0\\).\nIf \\(\\gamma = 1\\), then the geometric series simplifies to \\(S(n+1) = n+1\\). So let’s assume \\(\\gamma \\neq 1\\) from now on.\nBy pulling out the term for \\(i=0\\) and factoring out a \\(\\gamma\\), we can derive a recurrence relation for the geometric series \\[\nS(n+1) = 1 + \\gamma S(n) \\quad \\text{and} \\quad S(0) = 0\n\\tag{2.13}\\]\nWhen we even add a clever \\(0 = \\gamma^{n+1} - \\gamma^{n+1}\\), we get this equation for \\(S(n)\\) \\[\nS(n) = (1 - \\gamma^{n+1}) + \\gamma S(n).\n\\]\nFrom this, we can deduce the closed-form expression for the geometric series: \\[\n\\sum_{i=0}^n \\gamma^i  = \\frac{1 - \\gamma^{n+1}}{1 - \\gamma}\n\\tag{2.14}\\]\nBy omitting the first term (starting from \\(i = 1\\)), we obtain: \\[\n\\sum_{i=1}^n \\gamma^i =  \\frac{\\gamma - \\gamma^{n+1}}{1 - \\gamma}\n\\tag{2.15}\\]\nThe infinite geometric series converges, if and only if, \\(|\\gamma| &lt; 1\\). Using the previous formulas, we can derive their limits: \\[\n\\sum_{i=0}^\\infty \\gamma^i = \\frac{1}{1-\\gamma}\n\\tag{2.16}\\] \\[\n\\sum_{i=1}^\\infty \\gamma^i = \\frac{\\gamma}{1-\\gamma}\n\\tag{2.17}\\]\n\n\n2.11.9 recurrence relations\nAs it turns out, the basic geometric series we’ve explored isn’t quite enough to handle discounting and cumulative discounted returns in reinforcement learning. While the geometric series solves the homogeneous linear recurrence relation given by Equation 2.13, dealing with cumulative discounted returns introduces a non-homogeneous variation, where the constants 1s are replaced by a some \\(r_i\\)​, leading to the recurrence relation: \\[\nQ(n+1) := r_n + \\gamma Q(n) \\quad \\text{and} \\quad Q(0) := 0\n\\tag{2.18}\\]\nWe can also give an explicit formula for \\(Q(n+1)\\): \\[\nQ(n+1) = \\sum_{i=0}^n \\gamma^{n-i} r_i.\n\\tag{2.19}\\]\nIt’s easy to verify that this fulfils the recursive definition: \\[\n\\begin{split}\nQ(n+1) &= r_n + \\sum_{i=0}^{n-1} \\gamma^{n-i} r_i \\\\\n&= r_n + \\gamma\\sum_{i=0}^{n-1} \\gamma^{n-1-i} r_{i} \\\\\n&= r_n + \\gamma Q(n).\n\\end{split}\n\\]\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#footnotes",
    "href": "chapters/02-multi-armed-bandits.html#footnotes",
    "title": "2  Multi-armed Bandits",
    "section": "",
    "text": "I’m saying this not very loudly. Maybe I’m somewhere wrong.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html",
    "href": "chapters/03-finite-markov-decision-processes.html",
    "title": "3  Finite Markov Decision Processes",
    "section": "",
    "text": "3.1 The Agent–Environment Interface\nWe are already anticipating Exercise 3.5 and will give the formulations for a Markov Decision Process (MDP) for continuing and episodic tasks.\nA continuing trajectory looks like this: \\[\nS_0, A_0, R_1, S_1, A_1, R_2, S_2 A_2, R_3, \\dots,\n\\] and an episodic trajectory looks like this: \\[\nS_0, A_0, R_1, S_1, A_1, \\dots R_{T-1}, S_{T-1}, A_{T-1}, R_T, S_T.\n\\] Note that the indexes of actions and rewards has changed from the previous chapter. Now, the reward for an action \\(A_t\\) is \\(R_{t+1}\\), not \\(R_t\\) as it was defined for the armed bandits.\nAn MDP is completely described by its dynamics: \\[\np(s', r |s,a) := \\mathrm{Pr}(S_t = s', R_t = r \\mid S_{t-1} = s, A_{t-1} = a)\n\\tag{3.1}\\]\ngiving the probability that, from state \\(s \\in \\mathcal{S}\\) under action \\(a \\in \\mathcal{A}(s)\\), the environment transitions to state \\(s' \\in \\mathcal{S}^+\\) (\\(\\mathcal{S}^+\\) denotes the state space with any possible terminal states) and gives reward \\(r \\in \\mathcal{R}\\).\nIn particular when \\(s\\) and \\(a\\) are fixed \\(p(s', r | s,a)\\) is a discrete probability density, i.e., \\[\np(\\cdot, \\cdot | s,a)\\colon \\mathcal{S}^+ \\times \\mathcal{R} \\to [0,1]\n\\] and \\[\n\\sum_{s' \\in \\mathcal{S}^+, r \\in \\mathcal{R}} p(s',r | s,a) = 1.\n\\tag{3.2}\\]\nI want to add some more words about MDP and other Markov Chains that will be important for us.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#sec-the-agent-environment-interface",
    "href": "chapters/03-finite-markov-decision-processes.html#sec-the-agent-environment-interface",
    "title": "3  Finite Markov Decision Processes",
    "section": "",
    "text": "3.1.1 markov chains\nVery generally, Markov chains are processes modelled with sequences of random variables \\(X_1, X_2, \\dots\\), where the conditional probabilities have a finite horizon. We will describe Markov Chains with increasing complexity until we end up at MDPs.\n\nmarkov process (MP)\nMPs model systems that evolve randomly over discrete time steps. They are a sequence of random variables \\(S_0, S_1, \\dots\\), where \\(S_t\\) is the state of the system at time \\(t\\). In the past the system was in the states \\(S_0, \\dots, S_{t-1}\\) and its future is \\(S_{t+1}\\).\nThe defining property of a Markov chain is that the future is independent of the past given the present state of the process. This is expressed as: \\[\n\\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s, (S_{t'} = s_{t'})_{t' &lt; t}) = \\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s)\n\\]\nUsually we require the environment to be stationary, i.e., the transition probabilities are independent of \\(t\\): \\[\n\\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s) = \\mathrm{Pr}(S_{t'+1} = s' \\mid S_t' = s)\n\\]\nSo, in our case a Markov Process1 is completely described by\n\nstate space \\(\\mathcal{S}\\) and\ntransition probabilities: \\(p(s' | s) := P(S_{t+1}=s′∣ S_t=s)\\).\n\n\n\nmarkov reward process (MRP)\nA Markov Reward Process adds a reward structure to a Markov Process. What are we rewarded for? Simply for observing the process diligently and keeping our feet still as there is no interaction with the environment yet.\nHere, we have a sequence of random variables \\(R_0, S_0, R_1, S_1, R_2, \\dots\\). Basically it’s a sequence of random vectors \\((R_i, S_i)\\), where \\(S_i\\) tracks the state and the \\(R_i\\) give us some numerical information about the system. (Sutton and Barto usually omit the 0-th reward, which occurs before anything really happens—essentially a reward for starting the environment. It doesn’t change much, of course, but I like the symmetry it brings.)\nA Markov reward process (MRP) is therefore specified by:\n\nfinite state space \\(\\mathcal{S}\\)\nfinite reward space \\(\\mathcal{R} \\subseteq \\mathbb{R}\\)\n\\(p(s', r | s) := \\mathrm{Pr}(S_{t+1}=s', R_{t+1} = r∣ S_t = s)\\).\n\nHere \\(p(\\cdot, \\cdot | s)\\) is a probability measure on the product space \\(\\mathcal{S} \\times \\mathcal{R}\\), in particular \\(\\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s',r|s) = 1\\)\n\n\nmarkov decision process (MDP)\nNow we add interaction to the environment.\nThe trajectory looks like this: \\[\nR_0, S_0, A_0, R_1, S_1, A_1, \\dots ,\n\\] where \\(R_i\\) take values in the reward space \\(\\mathcal{R}\\), \\(S_i\\) values in the state space \\(\\mathcal{S}\\), and \\(A_i\\) in the action space \\(\\mathcal{A}\\).\nThe full dynamic of this process is an interwoven interaction between environment and agent. It looks a bit like this: \\[\n(R_0, S_0) \\overset{\\text{agent}}{\\to} A_0  \\overset{\\text{env}}{\\to}(R_1, S_1) \\overset{\\text{agent}}{\\to}  A_1 \\overset{\\text{env}}{\\to} \\dots\n\\] So, going from \\(S_t, A_t\\) to the next state-reward pair is given by the environment \\[\np(s', r | s, a) := \\mathrm{Pr}(S_{t+1}=s', R_{t+1} = r∣ S_t = s, A_t = a).\n\\] and going from a state reward pair to an action is is given by the agent \\[\n\\pi_t(a|s) = \\mathrm{Pr}(A_t = a | S_t = s).\n\\] If the agent is stationary, we can drop the \\(t\\). \\[\n\\pi(a|s) = \\mathrm{Pr}(A_t = a | S_t = s)\n\\]\n\nExercise 3.1 Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n\n\nSolution 3.1. TBD\n\n\nExercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n\n\nSolution 3.2. No, I can’t think of any clear exceptions. There’s only the challenge of how to model MDP for goals that we don’t know how to specify properly in the reward signal, e.g., human happiness. I can’t come up with a reward signal that wouldn’t be vulnerable to reward hacking, like “number pressed by user on screen”, “time smiling”, “endorphins level in brain”.\n\n\nExercise 3.3 Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n\n\nSolution 3.3. TBD\n\n\nExercise 3.4 Give a table analogous to that in Example 3.3, but for \\(p(s' , r |s, a)\\). It should have columns for \\(s, a, s' , r\\) and \\(p(s' , r |s, a)\\), and a row for every 4-tuple for which \\(p(s', r |s, a) &gt; 0\\).\n\n\nSolution 3.4. \n\n\n\ns\na\ns’\nr\np(s’,r | s,a)\n\n\n\n\nhigh\nwait\nhigh\nr_wait\n1\n\n\nhigh\nsearch\nhigh\nr_search\nα\n\n\nhigh\nsearch\nlow\nr_search\n1 - α\n\n\nlow\nwait\nlow\nr_wait\n1\n\n\nlow\nsearch\nlow\nr_search\nβ\n\n\nlow\nsearch\nhigh\n-3\n1 - β\n\n\nlow\nrecharge\nhigh\n0\n1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#goals-and-rewards",
    "href": "chapters/03-finite-markov-decision-processes.html#goals-and-rewards",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.2 Goals and Rewards",
    "text": "3.2 Goals and Rewards",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#returns-and-episodes",
    "href": "chapters/03-finite-markov-decision-processes.html#returns-and-episodes",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.3 Returns and Episodes",
    "text": "3.3 Returns and Episodes\nThe expected (discounted) return is defined as: \\[\nG_t := \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}.\n\\tag{3.3}\\] For episodic tasks we have the convention that \\(R_{t} = 0\\) when \\(t &gt; T\\), and thus, in particular, \\(G_T = 0\\).\nIn an undiscounted, episodic task, this becomes \\[\nG_t := \\sum_{k=0}^{T-t-1}  R_{t+k+1}\n\\tag{3.4}\\]\nAnd as for the armed bandit this recurrence relation tying immediate reward with future return will be used often: \\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\tag{3.5}\\]\n\nExercise 3.5 The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of Equation 3.2\n\n\nSolution 3.5. We already described Section 3.1 for continuing and episodic tasks. So, Equation 3.2 is already in the right form.\n\n\nExercise 3.6 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n\n\nSolution 3.6. The reward at time \\(t\\) would be \\[\nG_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t + k+1} =  -\\gamma^{T - t - 1},\n\\] where \\(T\\) is the length of that episode.\nIn the continuing formulation there can be multiple failures in the future so the return is of the form \\(-\\gamma^{K_1} - \\gamma^{K_2} - \\dots\\). Here there can always just be one failure.\n\n\nExercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward Equation 3.4. After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n\n\nSolution 3.7. In this setup the reward basically says: “Finish the maze eventually.”. So when the robot has learned to finish a maze somehow, it can’t perform better regarding this reward.\n\n\nExercise 3.8 Suppose \\(\\gamma = 0.5\\) and the following sequence of rewards is received \\(R_1 = 1, R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2\\), with \\(T = 5\\). What are \\(G_0, G_1 \\dots, G_5\\)? Hint: Work backwards.\n\n\nSolution 3.8. We can use the recurrence relation Equation 3.5 for the reward: $\n\n\n\nt\n\\(R_{t+1}\\)\n\\(\\gamma G_{t+1}\\)\n\\(G_t\\)\n\n\n\n\n5\n0\n0\n0\n\n\n4\n2\n0\n2\n\n\n3\n3\n1\n4\n\n\n2\n6\n2\n8\n\n\n1\n2\n8\n10\n\n\n0\n1\n5\n6\n\n\n\nWe recall that, by convention, \\(R_t := 0\\) for \\(t &gt; T\\)\n\n\nExercise 3.9 Suppose \\(\\gamma = 0.9\\) and the reward sequence is \\(R_1 = 2\\) followed by an infinite sequence of 7s. What are \\(G_1\\) and \\(G_0\\)?\n\n\nSolution 3.9. \\[\nG_1 = \\sum_{k=0}^\\infty 0.9^k R_{2+k} = 7 \\sum_{k=0}^\\infty 0.9^k = 7 / 0.1 = 70\n\\] and \\[\nG_0 = R_1 + \\gamma G_1 =  2 + 0.9 \\cdot G_1 = 2 + 0.9 \\cdot 70 = 65\n\\]\n\n\nExercise 3.10 Prove the second equality in (3.10).\n\n\nSolution 3.10. See Section 2.11.8 for a proof.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#unified-notation-for-episodic-and-continuing-tasks",
    "href": "chapters/03-finite-markov-decision-processes.html#unified-notation-for-episodic-and-continuing-tasks",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.4 Unified Notation for Episodic and Continuing Tasks",
    "text": "3.4 Unified Notation for Episodic and Continuing Tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#policies-and-value-functions",
    "href": "chapters/03-finite-markov-decision-processes.html#policies-and-value-functions",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.5 Policies and Value Functions",
    "text": "3.5 Policies and Value Functions\nThe policy distribution, together with the MDP dynamics, completely specify the distribution over trajectories. We write \\(\\mathrm{Pr}_\\pi\\) and \\(\\mathbb{E}_\\pi\\) to indicate which policy is used.\nWe want to evaluate stationary policies \\(\\pi\\). For example, something like \\(\\mathbb{E}_\\pi[R_{t+1} \\mid S_t = s]\\).\n\nExercise 3.11 If the current state is \\(S_t\\), and actions are selected according to the stochastic policy \\(\\pi\\), then what is the expectation of \\(R_{t+1}\\) in terms of \\(\\pi\\) and the four-argument function \\(p\\) (Equation 3.1)?\n\n\nSolution 3.11. It’s clearer to rephrase the exercise as “given that the current state \\(S_t\\) is \\(s\\)”. So, let’s proceed with that.\nWe’ll solve this two ways: first, intuitively; then, using our theory machine.\nIntuitively, when \\(S_t = s\\) then we know that \\(A_t\\) is distributed according to \\(\\pi(\\cdot | s)\\) and then from \\(S_t\\) and \\(A_t\\) we can get the next \\(S_{t+1}, R_{t+1}\\) via the MDP dynamics measure. So let’s put this together. The agent selects \\(a\\) with probability \\(\\pi(a \\mid s)\\), and then the environment transitions to \\((s', r)\\) with probability \\(p(s', r | s,a)\\). We don’t care about the \\(s'\\) right now. So, we get reward \\(r\\) with probability \\(\\sum_{s'} p(s',r | s,a)\\). Thus we have \\[\n\\mathbb{E}_{\\pi}[R_{t+1} \\mid S_t = s]\n= \\sum_{a} \\pi(a|s) \\sum_{r} r \\left(\\sum_{s'} p(s',r | s,a)\\right)\n\\] Or in a nicer format \\[\n\\mathbb{E}_{\\pi}[R_{t+1} \\mid S_t = s]\n= \\sum_{a} \\pi(a|s) \\sum_{r,s'} r \\; p(s',r | s,a)\n\\]\nNow let us derive this using LOTUS (Theorem 2.1) the law of total expectation (Theorem 2.3) \\[\n\\begin{split}\n\\mathbb{E}_{\\pi}&[R_{t+1} \\mid S_t = s] =\n\\sum_{r} r \\; \\mathrm{Pr}_{\\pi}[R_{t+1} = r \\mid S_t = s] \\\\\n&= \\sum_r r \\sum_{a, s'} \\mathrm{Pr}_{\\pi}[R_{t+1} = r, S_{t+1} = s' \\mid A_t = a, S_t = s] \\mathrm{Pr}_{\\pi}[A_t = a \\mid S_t = s] \\\\\n&=  \\sum_{r,a,s'} p(s',r | a,s) \\pi(a|s)\n\\end{split}\n\\]\n\nThe value functions quantify how desirable it is to be in a given state (or to take a given action in a state) under a policy:\n\nstate value function: \\[\nv_\\pi(s) := \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]\n\\tag{3.6}\\]\naction-value function: \\[\nq_{\\pi}(s,a) := \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]\n\\tag{3.7}\\]\n\n\nExercise 3.12 Give an equation for \\(v_\\pi\\) in terms of \\(q_\\pi\\) and \\(\\pi\\).\n\n\nSolution 3.12. The quick and easy answer is: \\[\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n\\tag{3.8}\\]\n\n\nExercise 3.13 Give an equation for \\(q_\\pi\\) in terms of \\(v_\\pi\\) and the four-argument \\(p\\)\n\n\nSolution 3.13. Again quick and easy: \\[\nq_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n\\tag{3.9}\\]\n\nWe can get the Bellman equations for \\(v_\\pi\\) and \\(q_\\pi\\) by writing Equation 3.6 and Equation 3.7 in their “one-step look-ahead” form \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s]\n\\tag{3.10}\\] \\[\nq_\\pi(s,a) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a]\n\\tag{3.11}\\]\nWe then write down the expectations with the MDP’s transition dynamics and the policy’s action probabilities: \\[\nv_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_\\pi(s')]\n\\tag{3.12}\\] \\[\nq_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma \\sum_{a'}\\pi(a'|s')q_{\\pi}(s'|a')]\n\\tag{3.13}\\]\nEquivalently, one can derive these by combining Equation 3.8 with Equation 3.9.\nThis is typically stated as a recursive relationship between the state’s value and the next state’s value. (I wouldn’t call this ‘recursive’ myself, especially in continuing tasks where there’s no base case)\nWhat I found helpful for understanding was writing the Bellman equations as a system of linear equations. We see the value function as a vector \\(v_\\pi\\) (indexed by \\(\\mathcal{S}\\)) and then we can write the Bellman equations like: \\[\n\\mathbf{v}_\\pi = \\mathbf{r}_{\\pi} + \\gamma \\mathbf{P}_\\pi \\mathbf{v}_\\pi,\n\\]\nwhere \\(\\mathbf{r}_\\pi\\) is the expected reward, and \\(\\mathbf{P}_\\pi\\) is the transition matrix under policy \\(\\pi\\). Specifically, \\[\n(\\mathbf{R}_\\pi)_s = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)r\n\\]\nand \\[\n(\\mathbf{P}_\\pi)_{s,s'} = \\sum_{a} \\pi(a|s) \\sum_{r} p(s',r|s,a).\n\\]\nOne can check that the Bellman equation for \\(v_\\pi\\) (Equation 3.12) is indeed one row of this vector equation. For episodic tasks, we use the convention that that \\((\\mathbf{R}_\\pi)_s = 0\\), \\((\\mathbf{P}_\\pi)_{s,s} =1\\), and \\((\\mathbf{v}_\\pi)_{s} = 0\\) for terminal states \\(s\\).\n\nExample 3.1 This is (Sutton and Barto 2018, example 3.5: Gridworld).\nFirst, Apologies for this ugly gridworld diagram below - this was the best I could manage with Graphviz.\nHere’s a quick recap of the gridworld setup. The states are the cells in an \\(5 \\times 5\\)-grid. The actions are up, down, left, right each normally with a reward of +0. When bumping into a wall the position does not change and the reward is -1. When moving any direction from A or B the reward is +10 respectively +5 and the agent gets beamed to A’ respectively B’.\n\n\n\n\n\n\n\nG\n\n\n\ngrid\n\n\nA\n\n  \n\nB\n\n  \n\n  \n\n\n\n\n\n  \n\n\n\nB'\n\n\n  \n\n\n\n\n\n\nA'\n\n\n\n\n\n\ngrid:01-&gt;grid:41\n\n\n+10\n\n\n\ngrid:03-&gt;grid:23\n\n\n+5\n\n\n\n\n\n\n\n\nLet’s solve the Bellman equation for the Gridworld in python. Here is some code that sets up the Gridworld class and a nice plotter for the value function. It’s good code but no need to read the fine print.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom enum import Enum\nfrom typing import Callable, Dict, Tuple\n\n\nclass Action(Enum):\n    UP = (-1, 0)\n    DOWN = (1, 0)\n    LEFT = (0, -1)\n    RIGHT = (0, 1)\n\n\nclass SpecialCell:\n    def __init__(self, reward: float, teleport: Tuple[int, int]):\n        self.reward = reward\n        self.teleport = teleport\n\n\nclass Gridworld:\n    \"\"\"\n    A simple Gridworld MDP with teleporting special cells.\n    \"\"\"\n\n    def __init__(\n        self,\n        size: int,\n        special_cells: Dict[Tuple[int, int], SpecialCell],\n        default_reward: float = 0.0,\n        boundary_penalty: float = -1.0,\n    ):\n        self.size = size\n        self.n_states = size * size\n        self.special_cells = special_cells\n        self.default_reward = default_reward\n        self.boundary_penalty = boundary_penalty\n        self.actions = list(Action)\n        self.n_actions = len(self.actions)\n\n    def state_index(self, position: Tuple[int, int]) -&gt; int:\n        i, j = position\n        return i * self.size + j\n\n    def index_state(self, idx: int) -&gt; Tuple[int, int]:\n        return divmod(idx, self.size)\n\n    def is_boundary(self, position: Tuple[int, int]) -&gt; bool:\n        i, j = position\n        return not (0 &lt;= i &lt; self.size and 0 &lt;= j &lt; self.size)\n\n    def expected_rewards(self, policy: Callable[[int, Action], float]) -&gt; np.ndarray:\n        \"\"\"\n        Compute expected immediate rewards R[s] under a given policy.\n        \"\"\"\n        R = np.zeros(self.n_states)\n        for s in range(self.n_states):\n            pos = self.index_state(s)\n            for action in self.actions:\n                prob = policy(s, action)\n                if pos in self.special_cells:\n                    reward = self.special_cells[pos].reward\n                else:\n                    new_pos = (pos[0] + action.value[0], pos[1] + action.value[1])\n                    reward = (\n                        self.boundary_penalty\n                        if self.is_boundary(new_pos)\n                        else self.default_reward\n                    )\n                R[s] += prob * reward\n        return R\n\n    def transition_matrix(self, policy: Callable[[int, Action], float]) -&gt; np.ndarray:\n        \"\"\"\n        Compute state-to-state transition probabilities P[s, s'] under a policy.\n        \"\"\"\n        P = np.zeros((self.n_states, self.n_states))\n        for s in range(self.n_states):\n            pos = self.index_state(s)\n            for action in self.actions:\n                prob = policy(s, action)\n                if pos in self.special_cells:\n                    new_pos = self.special_cells[pos].teleport\n                else:\n                    raw = (pos[0] + action.value[0], pos[1] + action.value[1])\n                    # clip to remain in grid\n                    new_pos = (\n                        min(max(raw[0], 0), self.size - 1),\n                        min(max(raw[1], 0), self.size - 1),\n                    )\n                s_prime = self.state_index(new_pos)\n                P[s, s_prime] += prob\n        return P\n\n\ndef plot_value_grid(\n    V: np.ndarray,\n    size: int,\n    cmap: str = \"viridis\",\n    fmt: str = \".2f\",\n    figsize: Tuple[int, int] = (6, 6),\n):\n    \"\"\"\n    Plot the state-value grid with annotations and a colorbar.\n    \"\"\"\n    V = V.reshape(size, size)\n\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Display heatmap\n    cax = ax.matshow(V, cmap=cmap, origin=\"upper\")\n\n    # Annotate each cell\n    for (i, j), val in np.ndenumerate(V):\n        ax.text(j, i, format(val, fmt), ha=\"center\", va=\"center\", fontsize=12)\n\n    # Configure ticks\n    ax.set_xticks(range(size))\n    ax.set_yticks(range(size))\n    ax.set_xlabel(\"Column\")\n    ax.set_ylabel(\"Row\")\n    ax.set_title(\"State-Value Function under Random Policy\")\n\n    # Add colorbar\n    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.show()\n\n\nTo solve for \\(\\mathbf{v}\\) we let python compute \\[\n\\mathbf{v} = (\\mathbf{I} - \\gamma \\mathbf{P})^{-1} \\mathbf{r}.\n\\]\n(I left the \\(\\pi\\) indices implicit here)\nSo here is the meat of this computation. We let numpy solve the bellmann equations for grid world.\n\n# === solving gridworld ===\n# setup parameters\ngrid_size = 5\nγ = 0.9\n\n# setup gridworld\nspecials = {\n    (0, 1): SpecialCell(reward=10, teleport=(4, 1)),\n    (0, 3): SpecialCell(reward=5, teleport=(2, 3)),\n}\nenv = Gridworld(size=grid_size, special_cells=specials)\n\n\n# setup random policy\ndef random_policy(_: int, __: Action) -&gt; float:\n    \"\"\"Uniform random policy over all actions.\"\"\"\n    return 1 / len(Action)\n\n\n# obtain variables of Bellman equation\nR = env.expected_rewards(random_policy)\nP = env.transition_matrix(random_policy)\n\n# --- solve the Bellman equation ---\nI = np.eye(grid_size * grid_size)\nv = np.linalg.solve(I - γ * P, R)\n\nplot_value_grid(v, grid_size)\n\n\n\n\n\n\n\nFigure 3.1: This is like the right part of Figure 3.2 (Sutton and Barto 2018): the state-value function for the equiprobable random policy\n\n\n\n\n\n\n\nExercise 3.14 The Bellman equation (Equation 3.12) must hold for each state for the value function \\(v_\\pi\\) shown in Figure 3.1 of Example 3.1. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n\n\nSolution 3.14. We’ll use the numbers accurate to two decimal places. Basically we have to show that for the middle state \\(s\\) we have \\[\n\\begin{split}\nv_\\pi(s) &\\approx 0.9 \\cdot 0.25 \\cdot \\big( v_\\pi(s + (1,0)) + v_\\pi(s + (-1,0)) \\\\\n&+ v_\\pi(s + (0,1)) + v_\\pi(s + (0,-1)) \\big),\n\\end{split}\n\\] (Here, we’re using vector notation to denote the directions to neighbouring states.)\nIndeed this is true \\[\n0.9 \\cdot 0.25 \\cdot (2.25 + 0.36 + (-0.35) + 0.74) = 0.669\n\\] which is approximately \\(0.67\\)\n\n\nExercise 3.15 In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using Equation 3.3, that adding a constant \\(c\\) to all the rewards adds a constant, \\(v_c\\), to the values of all states, and thus does not affect the relative values of any states under any policies. What is \\(v_c\\) in terms of \\(c\\) and \\(\\gamma\\)?\n\n\nSolution 3.15. Adding a constant \\(c\\) to all rewards in a continuing task adds a constant \\[\nv_c = \\frac{c}{ 1 − \\gamma}\n\\] to the value of every state. This can be shown as follows. \\[\n\\begin{split}\nG_t &= \\sum_{k=0}^{\\infty} \\gamma^k (R_{t+k+1} + c) \\\\\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} + \\sum_{k=0}^\\infty \\gamma^kc\\\\\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} + \\frac{c}{1-\\gamma}\n\\end{split}\n\\]\nThus, the relative ordering of state values is preserved (it doesn’t change which states are better than others). However, value ratios do change.\nFor completeness, we can also verify this result using the vector form of the Bellman equation. Suppose \\(\\mathbf{v}\\) is the original value function, and \\(\\mathbf{r}\\) the original reward vector. After adding a constant \\(c\\) to every reward, the new reward vector is \\(r+c\\mathbf{1}\\), where \\(\\mathbf{1}\\) is the vector of all ones. The new value function is \\[\n\\begin{split}\n\\mathbf{v}' &= (\\mathbf{I} - \\gamma \\mathbf{P})^{-1} (\\mathbf{r} + c\\mathbf{1}) \\\\\n&= (\\mathbf{I} - \\gamma \\mathbf{P})^{-1} \\mathbf{r} + c(\\mathbf{I} - \\gamma \\mathbf{P})^{-1} \\mathbf{1} \\\\\n&= \\mathbf{v} + \\frac{c}{1 - \\gamma} \\mathbf{1},\n\\end{split}\n\\] since \\((\\mathbf{I} - \\gamma \\mathbf{P}) \\frac{1}{1-\\gamma}\\mathbf{1}\n= \\frac{1}{1-\\gamma}(\\mathbf{1} - \\gamma \\mathbf{1}) = \\mathbf{1}\\).\n\n\nExercise 3.16 Now consider adding a constant \\(c\\) to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n\n\nSolution 3.16. In an episodic task, adding a constant \\(c\\) to all rewards the task in a meaningful way. The additive term depends on both the state and the policy through the expected remaining time in the episode, so it can change the relative values of states and also reverse the ranking of policies.\nFormally, for the undiscounted case, the value function under a modified reward \\(R'_t = R_t + c\\) becomes:\n\\[\n\\begin{split}\nv'_{\\pi}(s) &= \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{T-t-1} (R_{t+k+1} + c) \\mid S_t = s \\right] \\\\\n&= v_{\\pi}(s) + c \\cdot \\mathbb{E}_{\\pi}[T - t \\mid S_t = s],\n\\end{split}\n\\] where \\(T\\) is the random variable for the time step at which the episode ends.\nWe consider the easiest maze in the world with two states: \\(S\\) and \\(T\\) (terminal). In state \\(S\\), there are two actions:\n\n“continue”: returns to \\(S\\) with reward \\(r\\)\n\n“stop”: transitions to \\(T\\) with reward \\(0\\)\n\nIf \\(r=−1\\), then policies that choose “stop” with highen probability perform better. However, if we add \\(c=2\\) to all rewards, then this reverses to selecting “stop” with lower probability.\n\n\nExercise 3.17 What is the Bellman equation for action values, that is, for \\(q_\\pi\\)? It must give the action values \\(q_\\pi(s,a)\\) in terms of the action values \\(q_\\pi(s'a')\\), of possible successors to the state-action pair \\((s,a)\\).\n\n\nSolution 3.17. This is just Equation 3.13.\n\n\nExercise 3.18 The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n\n\n\n\n\n\n\nPolicyDiagram\n\n\n\ns\n\ns\n\n\n\na1\n\n\n\n\ns-&gt;a1\n\n\na₁\n\n\n\na2\n\n\n\n\ns-&gt;a2\n\n\na₂\n\n\n\na3\n\n\n\n\ns-&gt;a3\n\n\na₃\n\n\n\nvpi\nv_π(s)\n\n\n\ns-&gt;vpi\n\n\n\n\nqpi\nq_π(s, a), has prob. π(a|s)\n\n\n\na3-&gt;qpi\n\n\n\n\n\n\n\n\n\nGive the equation corresponding to this intuition and diagram for the value at the root node, \\(v_\\pi(s)\\), in terms of the value at the expected leaf node, \\(q_{\\pi}(s, a)\\), given \\(S_t = s\\). This equation should include an expectation conditioned on following the policy, \\(\\pi\\). Then give a second equation in which the expected value is written out xplicitly in terms of \\(\\pi(a|s)\\) such that no expected value notation appears in the equation.\n\n\nSolution 3.18. The hardest part of this exercise is to understand the problem description. And in the end isn’t this just Exercise 3.12 again? \\[\n\\begin{split}\nv_{\\pi}(s) &= \\mathbb{E}_{\\pi}[q_\\pi(s, A_t) \\mid S_t = s] \\\\\n&= \\sum_{a} \\pi(a|s)q_{\\pi}(s,a)\n\\end{split}\n\\]\n\n\nExercise 3.19 The value of an action, \\(q_{\\pi}(s, a)\\), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state–action pair) and branching to the possible next states:\n\n\n\n\n\n\n\nmdp\n\n\n\nsa\n\ns, a\n\n\n\ns1\n\ns₁'\n\n\n\nsa-&gt;s1\n\n\nr₁\n\n\n\ns2\n\ns₂'\n\n\n\nsa-&gt;s2\n\n\nr₂\n\n\n\ns3\n\ns₃'\n\n\n\nsa-&gt;s3\n\n\nr₃\n\n\n\nq\nq_π(s, a)\n\n\n\nq-&gt;sa\n\n\n\n\nv3\nv_π(s')\n\n\n\nv3-&gt;s3\n\n\n\n\n\n\n\n\n\nGive the equation corresponding to this intuition and diagram for the action value, \\(q_{\\pi}(s, a)\\), in terms of the expected next reward, \\(R_{t+1}\\), and the expected next state value, \\(v_{\\pi}(S_{t+1})\\), given that \\(S_{t} = s\\)\n\n\nSolution 3.19. Ok, let’s do it again. \\[\n\\begin{split}\nq_{\\pi}(s,a) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s, A_t = a] \\\\\n&= \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n\\end{split}\n\\]\nNote, I have left out the subscript \\(\\pi\\) for the expectation as the transition \\((S_t,A_t)\\to (S_{t+1}, R_{t+1})\\) is independent of the agent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#optimal-policies-and-optimal-value-functions",
    "href": "chapters/03-finite-markov-decision-processes.html#optimal-policies-and-optimal-value-functions",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.6 Optimal Policies and Optimal Value Functions",
    "text": "3.6 Optimal Policies and Optimal Value Functions\nIn the next chapter (Section 4.2), we will see that any MDP has an optimal deterministic policy. We’ll take this fact for granted for now and focus on their value functions.\nA policy is deterministic if \\(\\pi(a|s) = 1\\) for exactly one action for each \\(s \\in \\mathcal{S}\\). We then also just write \\(\\pi(s)\\) for this action.\nA policy \\(\\pi_*\\) is optimal if \\(\\pi_* \\geq \\pi\\) for any other \\(\\pi\\). Here we use \\(\\geq\\), a partial order on all policies for the MDP, defiend by \\[\n\\pi \\leq \\pi' \\quad:\\Longleftrightarrow\\quad v_\\pi(s) \\leq \\pi'(s) \\; \\text{, for all }s \\in \\mathcal{S}.\n\\]\nThere might be more than one optimal policy, but they must have the same value function (otherwise, they wouldn’t be optimal). We can define the optimal state-value function as \\[\nv_*(s) := \\max_{\\pi} v_{\\pi}(s)\n\\]\nand the optimal action-value function as \\[\nq_*(s,a) := \\max_{\\pi} q_\\pi(s,a).\n\\]\nThese definitions are well-defined by the existence of optimal policies.\nThe conversion formula from \\(q_*\\) to \\(v_*\\) takes a special simple form (we also prove this in the next chapter in Theorem 4.2): \\[\nv_*(s) = \\max_{a} q_\\pi(a,s)\n\\]\nCombining this with Equation 3.9, we get the Bellman optimality equations for the state value: \\[\nv_*(s) = \\max_{a} \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_*(s')]\n\\]\nand the state-action value: \\[\nq_*(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma \\max_{a'}q_*(s',a')]\n\\]\nIn principal, these Bellman optimality equations can be solved and having access to either \\(v_*\\) or \\(q_*\\) gives us direct access to an optimal \\(\\pi_*\\) (for \\(v_* \\to \\pi_*\\) we need to know the dynamics of the MDP though; see Exercise 3.28)\n\nExercise 3.20 Draw or describe the optimal state-value function for the golf example.\n\n\nSolution 3.20. I’m not 100% sure about this solution as I don’t like the golf example too much and as a result haven’t thought it completely through:\nUse driver when not on the green. Use putter on the green.\n\n\nExercise 3.21 Draw or describe the contours of the optimal action-value function for putting, \\(q_*(s, \\mathrm{putter})\\), for the golf example.\n\n\nSolution 3.21. I’m even less sure here.\nInside the -2 contour of \\(v_\\mathrm{putt}\\) it’s like \\(v_{\\mathrm{putt}}\\). In the sand it’s -3. Everywhere else it’s like the \\(q_*(s,\\mathrm{driver})\\) but shifted by a putter distance outwards and the value is one less.\n\n\nExercise 3.22 Consider the continuing MDP shown on to the right. The only decision to be made is that in the top state, where two actions are available, \\(\\mathrm{left}\\) and \\(\\mathrm{right}\\). The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, \\(\\pi_{\\mathrm{left}}\\) and \\(\\pi_{\\mathrm{right}}\\). What policy is optimal if \\(\\gamma = 0\\)? If \\(\\gamma = 0.9\\)? If \\(\\gamma = 0.5\\)?\n\n\nSolution 3.22. Here is a table indicating which policy is best in which case.\n\n\n\n\\(\\gamma\\)\n\\(v_{\\pi_{\\mathrm{left}}}\\)\n\\(v_{\\pi_{\\mathrm{right}}}\\)\n\n\n\n\n0\n1\n0\n\n\n0.9\n1\n1.8\n\n\n0.5\n0.5\n0.5\n\n\n\n\n\nExercise 3.23 Give the Bellman equation for \\(q_*\\) for the recycling robot.\n\n\nSolution 3.23. \n\n\nExercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.\n\n\nSolution 3.24. The solution is \\[\n\\begin{split}\nv_*(s) &= 10 + 0\\gamma + 0\\gamma^2 + 0\\gamma^3 + 0\\gamma^4 + 10\\gamma^5 + \\dots \\\\\n&= 10 \\sum_{i=0}^\\infty \\gamma^{5i} = \\frac{10}{1-\\gamma^5}\n\\end{split}\n\\] Which is for \\(\\gamma = 0.9\\) approximately \\(24.41943\\).\n\n\nExercise 3.25 Give an equation for \\(v_*\\) in terms of \\(q_*\\).\n\n\nSolution 3.25. \\[\nv_*(s) = \\max_a q_*(s,a)\n\\]\n\n\nExercise 3.26 Give an equation for \\(q_*\\) in terms of \\(v_*\\) and the four-argument \\(p\\).\n\n\nSolution 3.26. \\[\nq_*(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_*(s)]\n\\]\n\n\nExercise 3.27 Give an equation for \\(\\pi_*\\) in terms of \\(q_\\pi\\).\n\n\nSolution 3.27. \\[\n\\pi_*(s) = \\underset{a}{\\mathrm{argmax}} \\; q_*(s,a)\n\\]\n\n\nExercise 3.28 Give an equation for \\(\\pi_*\\) in terms of \\(v_*\\) and the four-argument \\(p\\).\n\n\nSolution 3.28. \\[\n\\pi_*(s) = \\underset{a}{\\mathrm{argmax}} \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_*(s)]\n\\]\n\n\nExercise 3.29 Rewrite the four Bellman equations for the four value functions (\\(v_\\pi\\), \\(v_*\\), \\(q_\\pi\\), and \\(q_*\\)) in terms of the three argument function \\(p\\) (3.4) and the two-argument function \\(r\\) (3.5).\n\n\nSolution 3.29. \\[\n\\begin{split}\nv_\\pi(s) &= \\sum_{a} \\pi(a|s) \\Big[r(a,s) + \\gamma \\sum_{s'} p(s'|a,s) v_\\pi(s')\\Big] \\\\\nv_*(s) &= \\max_a r(a,s) + \\gamma \\sum_{s'} p(s'|a,s) v_*(s') \\\\\nq_\\pi(a,s) &= r(a,s) + \\gamma \\sum_{s'} p(s'|s,a) \\sum_{a'} \\pi(a'|s') q_\\pi(a',s') \\\\\nq_*(a,s) &= r(a,s) + \\gamma \\sum_{s'} p(s'|s,a) \\max_{a'} q_*(a',s')\n\\end{split}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#optimality-and-approximation",
    "href": "chapters/03-finite-markov-decision-processes.html#optimality-and-approximation",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.7 Optimality and Approximation",
    "text": "3.7 Optimality and Approximation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#summary",
    "href": "chapters/03-finite-markov-decision-processes.html#summary",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.8 Summary",
    "text": "3.8 Summary\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#footnotes",
    "href": "chapters/03-finite-markov-decision-processes.html#footnotes",
    "title": "3  Finite Markov Decision Processes",
    "section": "",
    "text": "“Our” Markov processes could be called more precisely stationary discrete-time Markov process.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html",
    "href": "chapters/04-dynamic-programming.html",
    "title": "4  Dynamic Programming",
    "section": "",
    "text": "4.1 Policy Evaluation\nThe Bellman equations for state-value Equation 3.12 and for action-value Equation 3.13 can be used as update rules to approximate \\(v_\\pi\\) and \\(q_\\pi\\): \\[\nv_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_{k}(s')]\n\\tag{4.1}\\] \\[\nq_{k+1}(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma \\sum_{a'}\\pi(a'|s')q_k(s',a')]\n\\tag{4.2}\\]\nThese equations form the basis for iterative policy evaluation. The algorithm below demonstrates how to approximate \\(v_\\pi\\), where updates are performed in “sweeps” rather than “chunk updates”. This constitutes the policy evaluations step,\\(\\pi \\overset{\\mathrm{Eval}}{\\to} v_{\\pi}\\), in the policy iteration algorithm (Section 4.3).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#policy-evaluation",
    "href": "chapters/04-dynamic-programming.html#policy-evaluation",
    "title": "4  Dynamic Programming",
    "section": "",
    "text": "Listing 4.1: Iterative Policy Evaluation, for estimating \\(V \\approx v_\\pi\\)\n\n\nInput: \\(\\pi\\), the policy to be evaluated\nParameters: \\(\\theta &gt; 0\\), determining accuracy of estimation\nInitialisation: \\(V(s)\\), for all \\(s \\in \\mathcal{S}\\) arbitrarily, and \\(V(\\mathrm{terminal}) = 0\\)\nLoop:\n   \\(\\Delta \\gets 0\\)\n   Loop for each \\(s \\in \\mathcal{S}\\):\n       \\(v \\gets V(s)\\)\n       \\(V(s) \\gets \\sum_a \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]\\)\n       \\(\\Delta \\gets \\max(\\Delta, |v - V(s))\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\n\n\n\nExample 4.1 Here we explore Example 4.1 from Sutton and Barto (2018)\nHere is the quick summary:\n\nstates: non-terminal states are numbered 1 through 14. The two gray cells are treated as a single terminal state.\nactions: Four deterministic actions available in each state: up, down, left, right. Moving “off the grid” results in no state change.\nrewards: A reward of -1 is given for every transition until the terminal state is reached\nreturn: undiscounted\n\n\n\n\n\n\n\n\nG\n\n\n\ngrid\n\n\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n\n\n\n\n\n\n\n\n\nAnd here are the state-values for the random policy:\n\n\n\n\n\n\n\nG\n\n\n\ngrid\n\n\n0\n\n-14\n\n-20\n\n-22\n\n-14\n\n-18\n\n-20\n\n-20\n\n-20\n\n-20\n\n-18\n\n-14\n\n-22\n\n-20\n\n-14\n\n\n0\n\n\n\n\n\n\n\n\nNote that these values are (luckily) exact, which will be useful for the next exercises\n\n\nExercise 4.1 In Example 4.1, if \\(\\pi\\) is the equiprobable random policy, what is \\(q_\\pi(11, \\mathrm{down})\\)? What is \\(q_\\pi(7, \\mathrm{down})\\)?\n\n\nSolution 4.1. We can use the state-value function given in the example: \\[\n\\begin{split}\nq_\\pi(11, \\mathrm{down}) &= -1 + 0 = -1\\\\\nq_\\pi(7, \\mathrm{down}) &= -1 + v_\\pi(11) = -1 + (-14) = -15\n\\end{split}\n\\]\n\n\nExercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is \\(v_\\pi(15)\\) for the equiprobable random policy?\nNow suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is \\(v_\\pi(15)\\) for the equiprobable random policy in this case?\n\n\nSolution 4.2. Since the MDP is deterministic and all transitions give the same reward, the undiscounted Bellman equation Equation 3.12 simplifies to: \\[\nv_{\\pi}(s) = r + \\sum_{a}\\pi(a|s') [v_{\\pi}(s')],\n\\]\nwhere \\(r = -1\\).\nThe first case is quite easy to compute. The transitions for all original states remain unchanged, so their values also remain unchanged. For the new state 15, we can write: \\[\nv_\\pi(15) = -1 + \\frac{1}{4}(v_\\pi(12) + v_\\pi(13) + v_\\pi(14) + v_\\pi(15))\n\\] which gives \\(v_\\pi(15) = -20\\).\nNow in the second case we might be up for a lot of work, as state 13 has a new transition: taking action “down” leads to state 15. This changes the dynamics of the MDP, so in principle the values might change. However, luckily the existing state-value function still satisfies the Bellman equation for state 13: \\[\nv_\\pi(13) = -1 + \\frac{1}{4}(v_\\pi(12) + v_\\pi(9) + v_\\pi(13) + v_\\pi(15))\n\\]\nSubstitute the known values we see that the equation holds \\[\nv_\\pi(13) = -20 = -1 + \\frac{1}{4}(-22 - 20 - 14 - 20)\n\\]\nSo \\(v_\\pi(13)\\) remains consistent with the new dynamics. Since all Bellman equations continue to hold with the same values, the state-value function does not change. So, \\(v_\\pi(15)=-20\\) also in this case.\n\n\nExercise 4.3 What are the equations analogous to Equation 3.10, Equation 3.12, and Equation 4.1 for the action-value function \\(q_\\pi\\) and its successive approximation by a sequence of functions \\(q_0, q_1, \\dots\\)?\n\n\nSolution 4.3. We have already stated these equations in tandem as Equation 3.11, Equation 3.13, and Equation 4.2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#sec-policy-improvement",
    "href": "chapters/04-dynamic-programming.html#sec-policy-improvement",
    "title": "4  Dynamic Programming",
    "section": "4.2 Policy Improvement",
    "text": "4.2 Policy Improvement\nAn optimal policy can always be chosen to be deterministic. This is quite intuitive: why would introducing randomness in action selection be beneficial if all you care about is maximising expected return? More rigorously, if you are choosing between two actions, \\(a_1\\) and \\(a_2\\), and you know their values \\(q_\\pi(s,a_1)\\) and \\(q_\\pi(s,a_2)\\), then it is clearly best to take the one with the higher value. A key tool for this kind of reasoning is the policy improvement theorem.\n\nTheorem 4.1 Let \\(\\pi\\) be any policy and \\(\\pi'\\) a deterministic policy. Then \\(\\pi \\leq \\pi'\\) if \\[\nv_\\pi(s) \\leq q_\\pi(s,\\pi'(s)),\n\\] for all \\(s \\in \\mathcal{S}\\).\n\n\nProof. From the assumption, we have: \\[\n\\begin{split}\nv_\\pi(s) &\\leq q_\\pi(s, \\pi'(s)) \\\\\n&= \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s, A_t = \\pi'(s)] \\\\\n&= \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] \\\\\n\\end{split}\n\\] (if you wonder about the indices in the expectation: the first expectation is completely determined by the MDP, in the second one we stipulate action selection according to \\(\\pi'\\).)\nNow, we can unroll this expression recursively: \\[\n\\begin{align}\nv_\\pi(s) &\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] \\\\\n&\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma \\mathbb{E}_{\\pi'}[R'_{t+2} + \\gamma v_\\pi(S'_{t+2}) \\mid S'_{t+1} = S_{t+1}] \\mid S_t = s] \\\\\n&= \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2}) \\mid S_t = s]\n\\end{align}\n\\]\nand so on. The last equality should be justified formally by the law of total expectation and the law of the unconscious statistician (Theorem 2.4 and Theorem 2.1).\nIterating this process a couple of times we get \\[\nv_\\pi(s) \\leq \\mathbb{E}_{\\pi'}\\bigg[\\sum_{i=0}^{N} \\gamma^i R_{t+1+i} + \\gamma^{N+1} v_\\pi(S_{t+1+N}) \\;\\bigg|\\; S_t = s \\bigg]\n\\]\nand in the limit (everything is bounded so this should be kosher) \\[\nv_\\pi(s) \\leq \\mathbb{E}_{\\pi'}\\bigg[\\sum_{i=0}^{\\infty} \\gamma^i R_{t+1+i} \\;\\bigg| \\; S_t = s \\bigg] = v_{\\pi'}(s).\n\\]\n\nThis result allows us to show that every finite MDP has an optimal deterministic policy.\nLet \\(\\pi\\) be any policy. Define a new deterministic policy \\(\\pi'\\) by \\[\n\\pi'(s)= \\underset{a \\in \\mathcal{A}}{\\mathrm{argmax}} q_{\\pi}(s,a)\n\\]\nBy the policy improvement theorem, we have \\(\\pi \\leq \\pi'\\). Now consider two deterministic policies, \\(\\pi_1\\) and \\(\\pi_2\\), and define their meet (pointwise maximum policy) as \\[\n(\\pi_1 \\vee \\pi_2)(s) =\n\\begin{cases}\\pi_1(s) &\\text{if } v_{\\pi_1}(s) \\geq v_{\\pi_2}(s) \\\\\n\\pi_2(s) &\\text{else}\n\\end{cases}\n\\]\nThen, again by the policy improvement theorem, we have \\(\\pi_1, \\pi_2 \\leq \\pi_1 \\vee \\pi_2\\).\nNow, since the number of deterministic policies is finite (as both \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) are finite), we can take the meet over all deterministic policies and obtain an optimal deterministic policy.\nThis leads directly to a characterisation of optimality in terms of greedy action selection.\n\nTheorem 4.2 A policy \\(\\pi\\) is optimal, if and only if, \\[\nv_\\pi(s) = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi}(s,a),\n\\tag{4.3}\\]\nfor all \\(s \\in \\mathcal{S}\\).\n\n\nProof. If \\(\\pi\\) is optimal then \\(v_\\pi(s) &lt; \\max_{a} q_\\pi(s,a)\\) would lead to a contradiction using the policy improvement theorem.\nFor the converse we do an argument very similar to the proof of Theorem 4.1. So similar in fact that I’m afraid that were doing the same work twice. Let \\(\\pi\\) satisfy Equation 4.3. We show that \\(\\pi\\) is optimal by showing that \\[\n\\Delta(s) = v_{\\pi_*}(s) - v_{\\pi}(s)\n\\] is \\(0\\) for all \\(s \\in \\mathcal{S}\\), where \\(\\pi_*\\) is any deterministic, optimal policy.\nWe can bound \\(\\Delta(s)\\) like so \\[\n\\begin{split}\n\\Delta(s) &= q_{\\pi_*}(s,\\pi_*(s)) - \\max_a q_{\\pi}(s,a) \\\\\n&\\leq q_{\\pi_*}(s,\\pi_*(s)) - q_\\pi(s,\\pi_*(s)) \\\\\n&= \\mathbb{E}_{\\pi_*}[ R_{t+1} + \\gamma v_{\\pi_*}(S_{t+1}) - (R_{t+1} + \\gamma v_{\\pi}(S_{t+1})) | S_{t} = s] \\\\\n&= \\mathbb{E}_{\\pi_*}[\\gamma \\Delta(S_{t+1}) | S_t = s]\n\\end{split}\n\\]\nIterating this and taking the limit gives \\[\n\\Delta(s) \\leq \\lim_{k \\to \\infty} \\mathbb{E}_{\\pi_*}[\\gamma^k \\Delta(S_{t+k}) \\mid S_t = s] = 0.\n\\]\n\nFor a policy \\(\\pi\\), if we define \\(\\pi'(s) := \\underset{a}{\\mathrm{argmax}}\\;q_\\pi(s,a)\\), we get an improved policy, unless \\(\\pi\\) was already optimal. This constitutes the policy improvement step,\\(v_\\pi \\overset{\\mathrm{Imp}}{\\to} \\pi'\\), in the policy iteration algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#sec-policy-iteration",
    "href": "chapters/04-dynamic-programming.html#sec-policy-iteration",
    "title": "4  Dynamic Programming",
    "section": "4.3 Policy Iteration",
    "text": "4.3 Policy Iteration\nThe policy iteration algorithm chains evaluation and improvement and converges to an optimal policy, for any initial policy \\(\\pi_0\\): \\[\n\\pi_0 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_0} \\overset{\\mathrm{Imp}}{\\to}\n\\pi_1 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_1} \\overset{\\mathrm{Imp}}{\\to}\n\\pi_2 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_2} \\overset{\\mathrm{Imp}}{\\to}\n\\dots\n\\overset{\\mathrm{Imp}}{\\to} \\pi_* \\overset{\\mathrm{Eval}}{\\to} v_{*}\n\\]\nAnd here is the pseudo code.\n\n\n\nListing 4.2: Policy Iteration (using iterative policy evaluation) for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nParameter:\n\\(\\theta\\): a small positive number determining the accuracy of estimation\n\n1. Initialisation:\n\\(V(s) \\in \\mathbb{R}\\), \\(\\pi(s) \\in \\mathcal{A}(s)\\) arbitrarily, \\(V(\\mathrm{terminal}) = 0\\)\n\n2. Policy Evaluation\nLoop:\n    \\(\\Delta \\gets 0\\)\n    Loop for each \\(s \\in \\mathcal{S}\\):\n        \\(v \\gets V(s)\\)\n        \\(V(s) \\gets  \\sum_{s',r}p(s',r|s,\\pi(s))[r + \\gamma V(s')]\\)\n        \\(\\Delta \\gets \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\n3. Policy Improvement\n\\(\\text{policy-stable} \\gets \\mathrm{true}\\)\nFor each \\(s \\in \\mathcal{S}\\):\n    \\(\\text{old-action} \\gets \\pi(s)\\)\n    \\(\\pi(s) \\gets \\underset{a}{\\mathrm{argmax}} \\sum_{s',r} p(s', r |s,a)[r + \\gamma V(s')]\\)\n    If \\(\\text{old-action} \\neq \\pi(s)\\), then \\(\\text{policy-stable} \\gets \\text{false}\\)\nIf \\(\\text{policy-stable}\\):\n    return \\(V \\approx v_*\\) and \\(\\pi \\approx \\pi_*\\)\nelse:\n    go to Policy Evaluation\n\n\n\nNote that the final policy improvement step does not change the policy and is basically just checking that the current policy is optimal. So this is basically an extra step to see that this chain is done: \\(\\pi_0 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_0} \\overset{\\mathrm{Imp}}{\\to}\n\\dots\n\\overset{\\mathrm{Imp}}{\\to} \\pi_* \\overset{\\mathrm{Eval}}{\\to} v_{*} \\overset{\\mathrm{Imp}}{\\to} \\text{Finished}\\)\n\nExample 4.2 Here we explore Example 4.2 from Sutton and Barto (2018) - Jack’s car rental.\nHere’s a quick summary:\n\ntwo locations, each with a maximum of 20 cars (more cars added to a location magically vanish into thin air)\nduring the day, a random amount of customers rent cars and then another random amount of customers return cars\nat the end of a day, up to 5 cars can be moved between the locations\neach car rented rewards 10\neach move car costs 2\nrentals and returns are Poisson distributed:\n\\(\\lambda_{1,\\text{rent}} =3\\), \\(\\lambda_{1,\\text{return}} =3\\), \\(\\lambda_{2,\\text{rent}} =4\\), \\(\\lambda_{2,\\text{return}} =2\\)\n\nWe solve Jack’s car rental using policy iteration. The core computation in policy iteration is getting a state-action value from the state values, the one-step lookahead, which is used both in policy evaluation and policy improvement. This is the main performance bottleneck: \\[\nQ(s,a) = \\sum_{s',r} p(s',r|s,a) [ r + \\gamma V(s')]\n\\]\nThis expression is good for theorizing about the algorithm but its direct implementation feels awkward and inefficient. In practice, it’s better to split the four-argument \\(p(s',r|s,a)\\) into expected immediate reward \\(r(s,a)\\) and the transition probability \\(p(s'|s,a)\\).\n\\[\n\\begin{split}\nQ(s,a) &= \\sum_{s',r}p(s',r|s,a)r + \\gamma\\sum_{s',r}p(s',r|s,a)V(s') \\\\\n&= r(s,a) + \\gamma \\sum_{s'}p(s'|s, a) V(s')\n\\end{split}\n\\]\nFurthermore, we can make the algorithm more natural for this problem by introducing afterstates (Sutton and Barto 2018, sec. 6.8). Although we don’t use them to their full potential (we don’t learn an afterstate value function).\nAn afterstate is the environment state immediately after the agent’s action, but before the environment’s stochastic dynamics. This formulation is particularly effective when actions have deterministic effects. In Jack’s car rental, moving cars deterministically leads to an afterstate \\(s \\oplus a\\), while the stochastic dynamics - rentals and returns - then determine the next state \\(s'\\).\nThe one-step lookahead using afterstates becomes: \\[\nQ(s,a) = c(a) + r(s \\oplus a) + \\gamma \\sum_{s'} p(s'|s\\oplus a)V(s'),\n\\tag{4.4}\\]\nwhere \\(c(a)\\) is the cost of the, \\(r(s \\oplus a)\\) is the expected immediate reward for the afterstate.\nThe following code is a nearly verbatim implementation of the policy iteration pseudocode, using Equation 4.4 for evaluation. (Select annotations to see inline explanations.)\n\n# === policy iteration for Jack's car rental ===\nfrom collections import namedtuple\nfrom typing import Dict, List\n1from scripts.jacks_car_rental.jacks_car_rental import (\n    JacksCarRental,\n    State,\n    Action,\n)\n\n2Policy = Dict[State, Action]\nValueFn = Dict[State, float]\n\n\n3def compute_state_action_value(\n    env: JacksCarRental, state: State, action: Action, value: ValueFn, γ: float\n) -&gt; float:\n    \"\"\"\n    Compute the expected one‐step return\n    \"\"\"\n    after_state, cost = env.move(state, action)\n\n    future_return = 0.0\n    for state_new in env.state_space:\n        p = env.get_transition_probability(after_state, state_new)\n        future_return += p * value[state_new]\n\n    return cost + env.get_expected_revenue(after_state) + γ * future_return\n\n\ndef policy_evaluation(\n    env: JacksCarRental, π: Policy, value: ValueFn, θ: float, γ: float\n):\n    \"\"\"\n    Approximates the ValueFn from a deterministic policy\n    \"\"\"\n    while True:\n        Δ = 0.0\n\n        for s in env.state_space:\n            v_old = value[s]\n            v_new = compute_state_action_value(env, s, π[s], value, γ)\n            value[s] = v_new\n            Δ = max(Δ, abs(v_old - v_new))\n\n        if Δ &lt; θ:\n            break\n\n\ndef policy_improvement(\n    env: JacksCarRental, π: Policy, value: ValueFn, γ: float\n) -&gt; bool:\n    \"\"\"\n    Improve a policy according to the provided value‐function\n\n    If no state's action changes, return True (policy is stable). Otherwise return False.\n    \"\"\"\n    stable = True\n\n    for s in env.state_space:\n        old_action = π[s]\n4        best_action = max(\n            env.action_space,\n            key=lambda a: compute_state_action_value(env, s, a, value, γ),\n        )\n\n        π[s] = best_action\n        if best_action != old_action:\n            stable = False\n\n    return stable\n\n\nPolicyIterationStep = namedtuple(\"PolicyIterationStep\", [\"policy\", \"values\"])\n\n\n5def policy_iteration(env, θ, γ) -&gt; List[PolicyIterationStep]:\n    optimal = False\n    history = []\n\n    # init policy and value-function\n    π: Policy = {s: 0 for s in env.state_space}\n    value: ValueFn = {s: 0.0 for s in env.state_space}\n\n    while not optimal:\n\n        # evaluation and save\n        policy_evaluation(env, π, value, θ, γ)\n        history.append(PolicyIterationStep(π.copy(), value.copy()))\n\n        # find better policy\n        optimal = policy_improvement(env, π, value, γ)\n\n    return history\n\n\n1\n\nImport the environment class and type aliases (State, Action) from the Jack’s car‐rental module.\n\n2\n\nSince we are dealing with deterministic policies we can define Policy as a mapping.\n\n3\n\nImplements the one‐step lookahead with afterstates (see Equation 4.4).\n\n4\n\nChoose the action \\(a\\in \\mathcal{A}(s)\\) that maximises the one‐step lookahead; this is a concise way to do “\\(\\mathrm{argmax}_a \\dots\\)”” in Python\n\n5\n\nThe only slight change to the pseudocode: instead of just returning the optimal policy and its state-value function we return the history of policies.\n\n\n\n\nHere is some more code just to be able to visualize our solution for Jack’s car rental problem…\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm\nfrom matplotlib import cm\n\nfrom scripts.jacks_car_rental.jacks_car_rental import (\n    JacksCarRentalConfig,\n)\n\n\ndef plot_policy(title: str, config: JacksCarRentalConfig, π: dict):\n    max_cars = config.max_cars\n    max_move = config.max_move\n\n    # Build a (max_cars+1)×(max_cars+1) integer grid of “action” values\n    policy_grid = np.zeros((max_cars + 1, max_cars + 1), dtype=int)\n    for (cars1, cars2), action in π.items():\n        policy_grid[cars1, cars2] = action\n\n    # X/Y coordinates for pcolormesh:\n    x = np.arange(max_cars + 1)\n    y = np.arange(max_cars + 1)\n    X, Y = np.meshgrid(x, y)\n\n    fig, ax = plt.subplots(figsize=(9, 9))\n\n    # Discrete actions range\n    actions = np.arange(-max_move, max_move + 1)\n    n_colors = len(actions)\n\n    # Create a “coolwarm” colormap with exactly n_colors bins\n    cmap = plt.get_cmap(\"coolwarm\", n_colors)\n\n    # For a discrete colormap, we want boundaries at x.5, so that integer values\n    # get mapped to their own color. Example: if max_move=2, actions = [-2, -1, 0, 1, 2],\n    # then boundaries = [-2.5, -1.5, -0.5, 0.5, 1.5, 2.5].\n    bounds = np.arange(-max_move - 0.5, max_move + 0.5 + 1e-6, 1)\n    norm = BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n\n    cax = ax.pcolormesh(\n        X,\n        Y,\n        policy_grid,\n        cmap=cmap,\n        norm=norm,\n        edgecolors=\"black\",\n        linewidth=0.4,\n    )\n\n    # Axis labels and title\n    ax.set_xlabel(\"Cars at Location 2\", fontsize=12)\n    ax.set_ylabel(\"Cars at Location 1\", fontsize=12)\n    ax.set_title(title, fontsize=14, pad=12)\n\n    # Square aspect ratio so each cell is a square:\n    ax.set_aspect(\"equal\")\n\n    # Ticks every 5 cars\n    step = 5\n    ax.set_xticks(np.arange(0, max_cars + 1, step))\n    ax.set_yticks(np.arange(0, max_cars + 1, step))\n\n    # Colorbar (horizontal, at the bottom)\n    cbar = fig.colorbar(\n        cax,\n        ax=ax,\n        orientation=\"horizontal\",\n        pad=0.08,\n        shrink=0.85,\n        boundaries=bounds,\n        ticks=actions,\n        label=\"Action (Car Movement)\",\n    )\n    cbar.ax.xaxis.set_label_position(\"bottom\")\n    cbar.ax.xaxis.tick_bottom()\n\n    fig.tight_layout(rect=[0, 0.03, 1, 1])\n\n    plt.show()\n\n\ndef plot_valueFn(title, config, val):\n    \"\"\"\n    3D surface plot of the value function\n    \"\"\"\n    max_cars = config.max_cars\n\n    # Build a (max_cars+1)×(max_cars+1) grid of value estimates\n    value_grid = np.zeros((max_cars + 1, max_cars + 1), dtype=float)\n    for (l1, l2), v in val.items():\n        value_grid[l1, l2] = v\n\n    # Meshgrid for locations on each axis\n    x = np.arange(max_cars + 1)\n    y = np.arange(max_cars + 1)\n    X, Y = np.meshgrid(x, y)\n\n    fig = plt.figure(figsize=(11, 7))\n    ax = fig.add_subplot(111, projection=\"3d\")\n\n    # Shaded surface plot\n    surf = ax.plot_surface(\n        X,\n        Y,\n        value_grid,\n        rstride=1,\n        cstride=1,\n        cmap=cm.viridis,\n        edgecolor=\"none\",\n        antialiased=True,\n    )\n\n    ax.set_xlabel(\"Cars at Location 2\", fontsize=12, labelpad=10)\n    ax.set_ylabel(\"Cars at Location 1\", fontsize=12, labelpad=10)\n    ax.set_title(title, fontsize=14, pad=12)\n    ax.view_init(elev=35, azim=-60)\n\n    fig.tight_layout()\n    plt.show()\n\n\n…and now we can solve it:\n\n# === Solving Jack's car rental ===\n\n# Hyperparameter for \"training\"\nγ = 0.9\nθ = 1e-5\n\n# config and environment\nconfig = JacksCarRentalConfig(max_cars=20)\nenv = JacksCarRental(config)\n\n# do policy iteration\nhistory = policy_iteration(env, θ, γ)\n\n# print last (optimal) policy and its value function\nplot_policy(\n    f\"Optimal Policy after {len(history)-1} iterations\", config, history[-1].policy\n)\nplot_valueFn(f\"Value function for optimal policy\", config, history[-1].values)\n\n\n\n\n\n\n\n\n\n\n(a) Heatmap of the optimal policy \\(\\pi_∗(s)\\). Each cell at coordinates \\((i,j)\\) shows the number of cars moved from location 1 to location 2.\n\n\n\n\n\n\n\n\n\n\n\n(b) 3D surface of the optimal state‐value function \\(v_∗​(s)\\) corresponding to the policy in (a). The maximum value here is approximately \\(\\max⁡_s v_∗(s)\\approx 625\\); Sutton & Barto’s reported maximum is \\(\\approx 612\\).\n\n\n\n\n\n\nFigure 4.1: This is like the last two diagrmas in Figure 4.2 (Sutton and Barto 2018): the first diagrams shows the optimal policy - which looks to me identical to Sutton-Barto’s optimal policy. The second diagram shows its value function (the optimal value function), which seems to have a higher maximum as Sutton-Barto’s - I don’t know why.\n\n\n\n\nThe runtime of this policy iteration is quite reasonable - just a few seconds. But considering we’re only dealing with \\(441\\) states, it highlights how dynamic programming is limited to small MDPs.\nAnother thought: although we now have the optimal solution, the meaning of the value function is still somewhat abstract. Mathematically it’s clear, but I couldn’t, for example, say whether Jack can pay his monthly rent with this business.\n\n\nExercise 4.4 The policy iteration algorithm Listing 4.2 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the seudocode so that convergence is guaranteed.\n\n\nSolution 4.4. If ties in the argmax are determined randomly, this could maybe result in a soft-lock when there are enough states with ties in their evaluation. Here we should add a condition that the policy is only changed if the change results in an actual improvement. Often in application there is an order on the actions, and we could also choose the smallest. However, this might also have consequences for the exploration of the algorithms.\nSo maybe the best solution is to only change the policy action if a better action improves the value by more than some \\(\\epsilon&gt;0\\):\n\n\n\nListing 4.3: Modified step Policy Improvement in Policy Iteration Listing 4.2\n\n\nExtra Paramater:\n\\(\\epsilon\\): small parameter determining of policy is stable\n\nPolicy Improvement\n\\(\\text{policy-stable} \\gets \\mathrm{true}\\)\n\nfor each \\(s \\in \\mathcal{S}\\):\n    \\(\\text{old-action} \\gets \\pi(s)\\)\n\n    for each \\(a \\in \\mathcal{A}(s)\\):\n        \\(Q(a) \\gets \\sum_{s',r} p(s',r | s,a) [ r + \\gamma V(s') ]\\)\n\n    \\(\\text{best-value} \\gets \\max_{a} Q(a)\\)\n\n    if \\(\\text{best-value} - Q(\\text{old-action}) &gt; \\epsilon\\):\n        \\(\\pi(s) \\gets a^*\\) for which \\(Q(a_*) = \\text{best-value}\\)\n        \\(\\text{policy‐stable} \\gets \\mathrm{false}\\)\n\nif policy‐stable:\n    return \\(V \\approx v_*\\) and \\(π ≈ π_*\\)\nelse:\n    go to Policy Evaluation\n\n\n\n\n\nExercise 4.5 How would policy iteration be defined for action values? Give a complete algorithm for computing \\(q_*\\), analogous to Listing 4.2 for computing \\(v_*\\). Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.\n\n\nSolution 4.5. The code depends on the convention that episodic tasks have an absorbing state that transitions only to itself and that generates only rewards of zero (Sutton and Barto 2018, sec. 3.4).\n\n\n\nListing 4.4: Policy Iteration (using iterative policy evaluation) for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nParameter:\n\\(\\theta\\): a small positive number determining the accuracy of estimation\n\nInitialisation:\n\\(Q(s,a) \\in \\mathbb{R}\\), \\(\\pi(s) \\in \\mathcal{A}(s)\\) arbitrarily, but \\(Q(\\mathrm{terminal},a) = 0\\)\n\nPolicy Evaluation\nLoop:\n    \\(\\Delta \\gets 0\\)\n    Loop for each \\(s \\in \\mathcal{S}\\) and \\(a \\in \\mathcal{A}(s)\\):\n        \\(q \\gets Q(s,a)\\)\n        \\(Q(s,a) \\gets  \\sum_{s',r}p(s',r|s,a)[r + \\gamma Q(s',\\pi(s'))]\\)\n        \\(\\Delta \\gets \\max(\\Delta, |q - Q(s,a)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\nPolicy Improvement\n\\(\\text{policy-stable} \\gets \\mathrm{true}\\)\nFor each \\(s \\in \\mathcal{S}\\):\n    \\(\\text{old-action} \\gets \\pi(s)\\)\n    \\(\\pi(s) \\gets \\underset{a}{\\mathrm{argmax}}  \\; Q(s,a)\\)\n    If \\(\\text{old-action} \\neq \\pi(s)\\), then \\(\\text{policy-stable} \\gets \\text{false}\\)\nIf \\(\\text{policy-stable}\\):\n    return \\(Q \\approx q_*\\) and \\(\\pi \\approx \\pi_*\\)\nelse:\n    go to Policy Evaluation\n\n\n\n\n\nExercise 4.6 Suppose you are restricted to considering only policies that are \\(\\varepsilon\\)-soft, meaning that the probability of selecting each action in each state, \\(s\\), is at least \\(\\varepsilon/|\\mathcal{A}(s)|\\). Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm Listing 4.2 for \\(v_*\\).\n\n\nSolution 4.6. First of all, we are now dealing with a policy \\(\\pi(a|s)\\) that assigns probabilities to actions instead of \\(\\pi(s)\\) that choses an action.\nFor step 3, policy improvement, we assign to every possible action \\(a\\) a propability \\(\\varepsilon\\), then the remainining propability \\(1 - \\varepsilon|\\mathbfcal{A}(s)|\\) to the actions that maximize the expression under the argmax.\nFor step 2, policy evaluation, we have to change the assignment to \\(V(s) \\gets \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma V(s')]\\)\nFor step 1, initialisation, we have to make sure that each action gets a probability of at least \\(\\varepsilon\\).\n\n\nExercise 4.7 Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem.\n\n\nSolution 4.7. We have already set up everyting in Example 4.2. I made sure in the implementation for the environment to include parameters for these changes.\n\n# === solving Jack's car rental again ===\n# Hyperparameter for \"training\"\nγ = 0.9\nθ = 1e-5\n\n# config and environment\nconfig = JacksCarRentalConfig(\n    max_cars=20, free_moves_from_1_to_2=1, max_free_parking=10, extra_parking_cost=4\n)\nenv = JacksCarRental(config)\n\n# do policy iteration\nhistory = policy_iteration(env, θ, γ)\n\n# print last optimal policy\nplot_policy(\n    f\"Optimal Policy after {len(history)-1} iterations\", config, history[-1].policy\n)\n\n\n\n\n\n\n\n\nInterestingly this somewhat more complex problem, does need 1 less iteration than the original.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#value-iteration",
    "href": "chapters/04-dynamic-programming.html#value-iteration",
    "title": "4  Dynamic Programming",
    "section": "4.4 Value Iteration",
    "text": "4.4 Value Iteration\nValue iteration is policy iteration with a single sweep of policy evaluation per iteration.\nIn policy iteration, policy improvement is given by: \\[\n\\pi_k(s) = \\mathrm{argmax}_{a} \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_k(s')]\n\\]\nThis picks an action \\(\\hat{a}\\) that gives the best one-step lookahead from the current value function.\nThen, policy evaluation, uses the one-step lookahead for the updates: \\[\nv_{k+1}(s) = \\sum_{s',r}p(s',r|s,\\pi_k(s)) [r + \\gamma v_k(s')]\n\\]\nBut if we’re only doing one sweep, we may as well just plug in \\(\\hat{a}\\) directly from the lookahead, which gives the maximum of this expression: \\[\nv_{k+1}(s) = \\max_a \\sum_{s',} p(s',r|s,a)[r + \\gamma v_k(s')]\n\\tag{4.5}\\]\nSo, value iteration performs both greedy action selection and value backup in one go.\nAnd here is the respective pseudocode:\n\n\n\nListing 4.5: Value Iteration, for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nParameter:\n\\(\\theta\\): a small positive number determining the accuracy of estimation\n\nInitialisation:\n\\(V(s)\\) arbitrarily for \\(s \\in \\mathcal{S}\\), when episodic \\(V(\\mathrm{terminal}) = 0\\)\n\nValue Iteration\nLoop:\n    \\(\\Delta \\gets 0\\)\n    Loop for each \\(s \\in \\mathcal{S}\\):\n        \\(v \\gets V(s)\\)\n        \\(V(s) \\gets  \\max_a \\sum_{s',r} p(s',p|s,a) [r + \\gamma V(s')]\\)\n        \\(\\Delta \\gets \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\nGreedy Policy Extraction\nOutput a deterministic policy \\(\\pi \\approx \\pi_*\\), such that\n\\(\\pi(s) = \\mathrm{argmax}_a \\sum_{s',r} p(s',p|s,a) [r + \\gamma V(s')]\\)\n\n\n\n\n4.4.1 gambler’s problem\nLet’s talk about Example 4.3 from Sutton and Barto (2018) - the Gambler’s Problem. It gets its own little subsection because there’s actually quite a bit to say about it.\nFirst, a quick summary of the MPD:\n\nthe idea is to win a target amount \\(N\\) of chips by betting on coin flips\nnon-terminal states are the current amount of chips (captial): \\(\\mathcal{S} = \\{0,1,\\dots,N\\}\\), and the terminal states are \\(0\\) and \\(N\\).\nactions are how mamny chips you wager (the stake): \\(\\mathcal{A}(s) = \\{1,\\dots, \\min(s, N-s)\\}\\) for \\(s \\in \\mathcal{S}\\)\nthe environment dynamics are \\[\np(s' \\mid a,s) = \\begin{cases}p_{\\mathrm{win}} &\\text{if }s' = s+a\\\\1-p_{\\mathrm{win}} &\\text{if }s' = s-a \\end{cases}\n\\]\nrewards are 0 except for reaching the goal state \\(N\\), which gives a rewards of +1\nthe task is episodic and undiscounted (\\(\\gamma = 1\\))\n\nThe reward structure is set up so that the state-value function gives the probability of eventually reaching the goal from a given state: \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] = 1 \\cdot \\mathrm{Pr}_{\\pi}(S_T = N \\mid S_t = s)\n\\]\nSo under any policy \\(\\pi\\), the value of a state is just the probability of hitting \\(N\\) before hitting \\(0\\).\n\n4.4.1.1 general remarks\nThere’s a lot of content about this problem floating around online. Maybe that’s because it appears quite early in the book, or because it’s so simple to implement. Or maybe it’s because the problem hides a few trip wires. For one thing, if you implement it yourself without really understanding what’s going on, your correct solution might look completely wrong (see Figure 4.2 (b)).\nA lot of the articles I’ve come across either lack substance or seem a bit confused - which is totally fair, but I personally prefer reading something more insightful when digging into a problem.\nSo here, I’m trying to give this a bit of extra depth.\n\n\n4.4.1.2 no 0 stakes\nAnother issue you might stumble across is that the original example allows wagers of size 0. Which seems innocuous, but it actually complicates things when thinking about deterministic policies.\nSince the problem is undiscounted, we have: \\[\nq_*(s,0) = v_*(s) = \\max_a q_*(s,a)\n\\]\nSo technically, wagering nothing could be considered an optimal action. However, any deterministic policy following such an action will not complete an episode.\nFurthermore, zero stakes can also mess with value iteration. If we initialise the non-terminal states with a value greater than their optimal value, the algorithm will not update them, because it will just set \\(v(s) = q(s,0)\\).\nIt mostly leads to problems, so let’s leave them just out here.\n\n\n4.4.1.3 one-step lookahead and environment\nWe’ll follow the design proposed by Sutton and Barto (2018): no rewards, and two terminal states:\n\nruin: 0 capital, with value \\(0\\)\nwin: 100 captial, with value \\(1\\)\n\nThe terminal states are not updated during value iteration, of course.\nGiven that, the one-step lookahead for stake \\(s\\) and wager \\(a\\) is especially simple to compute: \\[\nQ(s,a) = p_\\mathrm{win} \\cdot V(s+a) + (1-p_\\mathrm{win}) \\cdot V(s-a).\n\\]\nIn code, it’s the environment’s responsibility to calculate the one-step lookaheads for a given state-value function (one_step_lookaheads), as well as to initialise the value function (make_initial_values). This gives us some nice encapsulation.\n\nfrom typing import List\n\nValueFn = List[float]\nState = int\n\n\nclass EnvGamblersProblem:\n    def __init__(self, p_win: float, goal: int):\n        self.p = p_win\n        self.goal = goal\n        self.non_terminal_states = list(range(1, goal))\n\n    def make_initial_values(self, non_terminal_value: float) -&gt; ValueFn:\n        v = [0.0]  # terminal: ruin\n        v.extend([non_terminal_value] * (self.goal - 1))  # non-terminals\n        v.append(1.0)  # terminal: win\n        return v\n\n    def one_step_lookaheads(self, s: State, v: ValueFn):\n        \"\"\"returns a list of the q-values for state s.\n        q[i] contains the value of betting an amount of i+1\"\"\"\n        p = self.p\n        goal = self.goal\n        return [\n            p * v[s + a] + (1 - p) * v[s - a] for a in range(1, min(s, goal - s) + 1)\n        ]\n\n\n\n4.4.1.4 solving the problem\nNow we can implement value iteration, as described in Listing 4.5. We split the process into the two parts: value iteration and policy extraction.\nI’ve modified the value iteration function so that it returns all intermediate value functions - the final one is the optimal state-value function.\n\ndef value_iteration_gamblers_problem(\n    env: EnvGamblersProblem, θ: float, init_value=0.0\n) -&gt; List[ValueFn]:\n    # **Init**\n    v = env.make_initial_values(init_value)\n\n    # **Value iteration**\n    value_functions = []\n    while True:\n        value_functions.append(v.copy())\n        Δ = 0.0\n        for s in env.non_terminal_states:\n            v_old = v[s]\n            v_new = max(env.one_step_lookaheads(s, v))\n            v[s] = v_new\n            Δ = max(Δ, abs(v_old - v_new))\n        if Δ &lt; θ:\n            break\n\n    return value_functions\n\n\ndef get_greedy_policy(v: ValueFn, env: EnvGamblersProblem):\n    # ** Greedy Policy Extraction **\n    policy = {}\n    for s in env.non_terminal_states:\n        action_values = env.one_step_lookaheads(s, v)\n1        greedy_action_idx = action_values.index(max(action_values))\n        policy[s] = greedy_action_idx + 1  # convert idx -&gt; stake\n    return policy\n\n\n1\n\nthis isn’t the most efficient way to compute the argmax of a list, but it’s fine for a simple problem like this.\n\n\n\n\nNow, using the code below, we can solve the gambler’s problem for \\(N = 100\\) and \\(p_\\mathrm{win} = 0.4\\).\n\n# set up environment\nenv = EnvGamblersProblem(p_win=0.4, goal=100)\n\n# run value_iteration\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\n\n# optain optimal value function (result of last sweep)\nv_star = value_functions[-1]\n\n# optain optimal policy (greedy w.r.t to v_star)\npolicy_star = get_greedy_policy(v_star, env)\n\nWe plot the value function sweeps and a (somewhat strangely shaped) optimal policy below in Figure 4.2.\nWe’re not going to go into the mathematical analysis of the exact solution here, but if you’re curious about exact formulas for the value function, check out the section about ‘Bold Strategy’ for the game ‘Red and Black’ - which is apparently the mathematicans way of calling the gambler’s problem - in Siegrist (2023). 1\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple\n\n\ndef plot_value_function(\n    value_functions: List[Tuple[str, ValueFn]],\n    title: str = \"Value Function Sweeps\",\n):\n    plt.figure(figsize=(10, 6))\n\n    for label, value_function in value_functions:\n        plt.plot(range(0, len(value_function)), value_function, label=label)\n\n    plt.xlabel(\"State (Capital)\")\n    plt.ylabel(\"Value\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n# sweeps to plot\nsweeps = (1, 2, 3, len(value_functions) - 1)\nvalue_functions_with_labels = [\n    (\n        f\"{\"final \" if i == len(value_functions) -1 else \"\" }sweep {i}\",\n        value_functions[i],\n    )\n    for i in sweeps\n]\nplot_value_function(\n    value_functions_with_labels,\n    title=r\"Value Function Sweeps for $N=100$ and $p_\\mathrm{win}=0.4$\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) This shows the value function over several sweeps of value iteration. The final sweep is the optimal value function. The first few sweeps (1-3) look very similar to what Sutton and Barto (2018) show. Interestingly, though, in their plot it seems to take more sweeps, even sweep 32 still noticeably differs from the final result.\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple, Dict\n\n\ndef plot_policy(policy: Dict[int, int], title: str = \"Policy\"):\n    plt.figure(figsize=(10, 6))\n\n    states = sorted(policy.keys())\n    actions = [policy[s] for s in states]\n    plt.scatter(states, actions)\n\n    plt.xlabel(\"State (Capital)\")\n    plt.ylabel(\"Action\")\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nplot_policy(policy_star, title=r\"An optimal policy for $N=100$ and p_\\mathrm{win$ = 0.4$\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) This shows one optimal policy. It looks quite different to the one shown by Sutton and Barto (2018). That’s because many states have multiple optimal actions. The greedy action selection just picks one, and the result is ultimatively decided by floating-point imprecisions.\n\n\n\n\n\n\n\nFigure 4.2: This is the analogue of Figure 4.3 from Sutton and Barto (2018). It shows the solution to the gambler’s problem for \\(N=100\\) and \\(p_\\mathrm{win} = 0.4\\).\n\n\n\n\n\n4.4.1.5 optimal actions\nSo, Figure 4.2 (b) looks jagged because, in some states, there are multiple optimal actions. The choice of which one is used isn’t governed by any particular logic in the implementation - it’s effectively decided by floating-point imprecision.\nHere we want to answer this somewhat understated question from Sutton and Barto (2018, 84):\n\nThis policy [shown in Figure 4.3] is optimal, but not unique. In fact, there is a whole family of optimal policies, all corresponding to ties for the argmax action selection with respect to the optimal value function. Can you guess what the entire family looks like?\n\nThe short answer is: no, I can’t. It’s not something that’s easy to guess, in my view.\nHowever, Siegrist (2023) gives an account of some well-developed theory addressing this question in. The section about bold play in the game Red and Black proves the existence of optimal strategies.\nOne such optimal strategy pattern applicable for all \\(p_\\mathrm{win} &lt; 0.5\\) and \\(N\\) is bold play, where you always bet the maximum amount possible: \\[\nB(s) := \\max \\mathcal{A}(s) =  \\min(\\{s, N-s\\})\n\\]\nActually, for any \\(p_\\mathrm{win} &lt; 0.5\\), all optimal actions are the same - only the underlying value functions change. The shape of \\(B(s)\\) is triangular, and if \\(N\\) is odd, it is the unique optimal policy, as seen in Figure 4.3 (c).\nIf \\(N\\) is even, then there exists a second-order bold strategy \\(B_2\\), which effectively applies divide and conquer to the problem (although I don’t have a really good intuition why it works). Create two subproblems, one from \\(0\\) to \\(N/2\\), and another from \\(N/2\\) to \\(N\\), each treated with their own bold strategy. It looks a bit like this (where \\(\\left\\lfloor \\frac{N}{4} \\right\\rfloor\\) means \\(N/4\\) rounded down):\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the x values\nx = np.linspace(0, 1, 1000)\n\n\n# Define the B_2(x)\ndef B2(x):\n    return np.where(\n        x &lt; 0.25, x, np.where(x &lt; 0.5, 0.5 - x, np.where(x &lt; 0.75, -0.5 + x, 1 - x))\n    )\n\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(x, B2(x), color=\"blue\")\n\n# Highlight key points\nax.plot([0.5], [0.5], \"o\", color=\"blue\")\nax.plot(\n    [0.5], [0], \"o\", mfc=\"white\", mec=\"blue\", zorder=5, clip_on=False\n)  # Draw over axis\n\n# Add dashed grid lines for key points\nax.axhline(0.25, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nax.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nax.axvline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nax.plot([0.25, 0.5], [0.25, 0.5], \"gray\", linestyle=\"--\", linewidth=0.5)\nax.plot([0.75, 0.5], [0.25, 0.5], \"gray\", linestyle=\"--\", linewidth=0.5)\n\n# Add axis labels and ticks\nax.set_xticks([0, 0.5, 1])\nax.set_xticklabels([\"0\", r\"$\\frac{N}{2}$\", r\"$N$\"])\nax.set_yticks([0.25, 0.5])\nax.set_yticklabels([r\"$\\left\\lfloor \\frac{N}{4} \\right\\rfloor$\", r\"$\\frac{N}{2}$\"])\n\nax.set_xlabel(r\"$s$\")\nax.set_ylabel(r\"$B_2(s)$\", rotation=0, labelpad=15)\n\n# Style the plot\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"left\"].set_position(\"zero\")\nax.spines[\"bottom\"].set_position(\"zero\")\nax.set_xlim(0, 1)\nax.set_ylim(0, 0.55)\n\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf \\(N\\) is divisible by \\(4\\), we can dividive and conquer again to get \\(B_3\\). This is exactly what we see in the original problem for \\(N = 100\\) (see Figure 4.3 (a)).\nBasically, if \\(N\\) is divisible by \\(2^\\ell\\) then \\(B_\\ell\\) is an optimal stategy. In the limit, this family gives rise to a kind of fractal pattern of stacked diamonds (see Figure 4.3 (b)) - although these diamonds are missing their bottom tips, which would correspond to wagering 0.\nSo my finally answer to the inital question is. The family of optimal policies consists of any selection of actions from the bold-strategy hierarchy: \\(B\\) (one big triangle), \\(B_2\\) (two triangles), and \\(B_3\\) (four triangles).\n\n\n\n\n\n\nCode\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n\ndef get_greedy_actions(v, env, ε):\n    optimal_actions = {}\n    for s in env.non_terminal_states:\n        q_values = env.one_step_lookaheads(s, v)\n        max_value = max(q_values)\n        bests = []\n        for a, q in enumerate(q_values):\n            if max_value - q &lt; ε:\n                bests.append(a + 1)\n        optimal_actions[s] = bests\n\n    return optimal_actions\n\n\n@dataclass\nclass MultiPolicy:\n    actions: Dict[int, List[int]]  # state -&gt; list of stakes\n    name: str = None\n    marker: str = \"o\"\n    size: int = None\n\n\ndef plot_multi_policy(multi_policies: List[MultiPolicy], title: str):\n    plt.figure(figsize=(10, 6))\n\n    draw_legend = False\n\n    for pol in multi_policies:\n        pts = [(s, a) for s, al in pol.actions.items() for a in al]\n        states, acts = zip(*pts)\n        if pol.name:\n            draw_legend = True\n        plt.scatter(\n            states,\n            acts,\n            marker=pol.marker,\n            s=pol.size,\n            label=pol.name,\n        )\n\n    plt.xlabel(\"State (Capital)\")\n    plt.ylabel(\"Action (Stake)\")\n    if draw_legend:\n        plt.legend()\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\n\nbest_minimal_actions = {}\nfor s in best_actions:\n    best_min = min(best_actions[s])\n    best_minimal_actions[s] = [best_min]\n\noptimals = [\n    MultiPolicy(best_actions, name=\"optimal actions\"),\n    MultiPolicy(best_minimal_actions, name=\"third-order bold strategy\", marker=\"x\", size=30),\n]\nplot_multi_policy(optimals, title=\"Optimal Actions for N=100\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The optimal actions for the gambler’s problem of example 4.3 of Sutton and Barto (2018) for \\(p_\\mathrm{win} = 0.4\\) and \\(N = 100\\). The actions chosen by ‘third-order bold strategy’ is the optimal policy shown by Sutton and Barto (2018).\n\n\n\n\n\n\nCode\ngoal = 5 * 32\nenv = EnvGamblersProblem(p_win=0.4, goal=goal)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-14)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\nplot_multi_policy([MultiPolicy(best_actions, size=12)], title=f\"Optimal actions for N={goal}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The optimal actions for the gambler’s problem for \\(N = 5\\cdot 32\\) and \\(p_\\mathrm{win} &lt; 0.5\\). It consists of the bold strategies up to order \\(6\\).\n\n\n\n\n\n\nCode\ngoal = 101\nenv = EnvGamblersProblem(p_win=0.4, goal=goal)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\nplot_multi_policy([MultiPolicy(best_actions)], title=f\"Optimal actions for N={goal}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) For \\(N = 101\\) only bold play is optimal. The same is true for all odd \\(N\\) and \\(p_\\mathrm{win} &lt; 0.5\\).\n\n\n\n\n\n\n\nFigure 4.3: Showcase of all optimal actions for \\(p &lt; 0.5\\) for various \\(N\\).\n\n\n\n\n\n4.4.1.6 gambler’s ruin\nCheck out the value function in Figure 4.2 (a). It looks like, even though a single coin flip is stacked against the gambler with \\(p_\\mathrm{win} = 0.4\\), if they start with a large capital, say 80 chips, they still reach the goal of \\(N = 100\\) about 70% of the time. Doesn’t sound too bad. But of course, if they succeed, they only win 20 chips. If they lose, they lose 80 chips.\nOne general result that captures this asymmetry is gambler’s ruin, which states essentially that when the odds are stacked against the gambler, there is no strategy to turn the odds in their favour.\nWe can make this concrete by calculating the expected monetary return when following the optimal strategy, starting with \\(s\\) chips: \\[\n\\mathbb{E}_{\\pi_*}[S_T - S_0 \\mid S_0 = s] = p_{\\mathrm{goal}}(N-s) + (1-p_{\\mathrm{goal}})(-s),\n\\]\nwhere \\[\np_{\\mathrm{goal}}(s) = \\mathrm{Pr}_{\\pi_*}(S_T = N \\mid S_0 = s) = v_*(s).\n\\]\nLet’s compute and plot it for \\(p = 0.4\\) and \\(N = 100\\):\n\n\nCode\ngoal = 100\np_win = 0.4\nenv = EnvGamblersProblem(p_win=p_win, goal=goal)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nexpected_profit = [\n    value * (env.goal - s) - (1 - value) * s for s, value in enumerate(v_star)\n]\n\n\nplt.figure(figsize=(10, 6))\n\nplt.plot(range(0, len(expected_profit)), expected_profit)\nplt.xlabel(\"Startign Capital\")\nplt.ylabel(\"Expected profit\")\nplt.title(\n    r\"Profit when following an optimal strategy for $N = 100$ and $p_\\mathrm{win} = 0.4$\"\n)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nEven though the plot shows a nice (fractal-looking) w shape, you’re expected to lose chips, no matter what your initial capital is.\nIf you have to play, then the best is to start with either very low or very high capital - this is not financial advice though 🥸.\nAs a risk-averse person, my actual advice is: start with \\(0\\) or \\(100\\) chips - i.e., don’t play at all.\n\n\n4.4.1.7 gambler’s fortune\nWhen we stack the game in favour of the gambler \\(p_{\\mathrm{win}} &gt; 0.5\\) everything becomes somewhat easier.\nThere is just one optimal strategy, timid play, that is, always bet exactly one coin \\[\n\\pi_*(s) = 1.\n\\]\nThe value function in this case has a clean analytical form: \\[\nv(s) = \\frac{1 - \\left(\\frac{1-p}{p}\\right)^s}{1 - \\left(\\frac{1-p}{p}\\right)^N}\n\\]\nAnd… well, that’s basically it. Winnig all the time doesn’t require any creative policies.\n\n\n4.4.1.8 floating point imprecissions\nThere’s one interesting thing about the gambler’s fortune case. The high win probabilities can lead to floating point imprecisions.\nWhen we run value iteration with \\(N = 100\\) and \\(p_\\mathrm{win} = 0.6\\), these imprecisions can bleed into the computed optimal policy.\n\nCode\nenv = EnvGamblersProblem(p_win=0.6, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nplot_value_function([(\"optimal value function\", v_star)])\nbest_actions = get_greedy_actions(v_star, env, 1e-10)\nplot_multi_policy([MultiPolicy(best_actions)], title=\"bla\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe value function approaches \\(1\\) very quickly. That means from a certain point onward, the \\(q(s, a)\\) values are all indistinguishably close to \\(1\\)…\n\n\n\n\n\n\n\n\n\n\n\n…which leads to floating point imprecisions during optimal action selection. The code ends up including some sub-optimal actions.\n\n\n\nTwo thoughts on this:\n\nPractically speaking, this isn’t really a problem. When all candidate actions yield a value practically indistinguishable from the max, it doesn’t matter which one we take. They’re all effectively optimal.\nTheoretically, it’s a useful reminder: a computation is not a proof. It can return wrong answers. Our algorithms produce approximations of the optimal policy, and here it’s harmless, but we should keep it in mind.\n\nThis kind of issue always arises when the \\(q\\)-values are close together. You can also see it with very low win probabilities:\n\n\nCode\nenv = EnvGamblersProblem(p_win=0.01, goal=128)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\nplot_multi_policy([MultiPolicy(best_actions)], title=r\"$p_\\mathrm{win} = 0.01$ and $N=100$\")\n\n\n\n\n\n\n\n\n\nNote: For illustration, I’ve deliberately used ‘large’ \\(\\varepsilon\\) values (\\(10^{-10}\\) for \\(p_\\mathrm{win} = 0.6\\) and \\(10^{-8}\\) for \\(p_\\mathrm{win} = 0.01\\)) when deciding which actions to treat as equally good, that is, if \\(|q(s,a) - q(s,a')| &lt; \\varepsilon\\), we consider both actions equally good.\n\nExercise 4.8 Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?\n\n\nSolution 4.8. As discussed in Section 4.4.1.5, there is no single optimal policy.\nFor capital 50, there’s only one optimal action - bet it all. This reflects the general strategy of bold play, which intuitively limits the number of steps (and thus the compounding risk) needed to reach the goal.\nFor capital 51, though, there are two optimal actions: continue being bold (bet 49), or - as shown in the policy from Sutton and Barto (2018) - just bet 1.\nThat both are equally good and optimal follows from the maths. But in my opinion, any simple intuitive explanation is just a posteriori justification of the mathematical facts.\n\n\nExercise 4.9 Implement value iteration for the gambler’s problem and solve it for \\(p_\\mathrm{win} = 0.25\\) and \\(p_{\\mathrm{win}} = 0.55\\). In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital of \\(0\\) and \\(100\\), giving them values of \\(0\\) and \\(1\\) respectively. Show your results graphically, as in Figure 4.2. Are your results stable as \\(\\theta \\to 0\\)?\n\n\nSolution 4.9. Here are the computed solutions - both the state-value functions and the optimal policies. For the policies, we show all optimal actions.\nThere are no surprises here; everything behaves as previously discussed.\n\nCode\nenv = EnvGamblersProblem(p_win=0.25, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-10)\nplot_value_function([(\"optimal value function\", v_star)], title=r\"optimal value function for $p_\\mathrm{win} = 0.25$ and $N=100$\")\nplot_multi_policy(\n    [MultiPolicy(best_actions)], title=r\"Optimal actions for $p_\\mathrm{win} = 0.25$\"\n)\n\nenv = EnvGamblersProblem(p_win=0.55, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-10)\nplot_value_function([(\"optimal value function\", v_star)], title=r\"optimal value function for $p_\\mathrm{win} = 0.55$ and $N=100$\")\nplot_multi_policy(\n    [MultiPolicy(best_actions)], title=r\"$Optimal actions for p_\\mathrm{win} = 0.55$\"\n)\n\n\n\n\n\n\n\nState value function for \\(p_\\mathrm{win} = 0.25\\) and \\(N = 100\\)\n\n\n\n\n\n\n\nOptimal actions for \\(p_\\mathrm{win} = 0.25\\) and \\(N = 100\\)\n\n\n\n\n\n\n\n\n\nState value function for \\(p_\\mathrm{win} = 0.55\\) and \\(N = 100\\)\n\n\n\n\n\n\n\nOptimal actions for \\(p_\\mathrm{win} = 0.55\\) and \\(N = 100\\)\n\n\n\n\n\n\nAll used \\(\\theta = 10^{-10}\\)\n\n\n\nThese solutions are not stable as \\(\\theta \\to 0\\) (pushing \\(\\theta &lt; 10^{-18}\\) seems to be numerically fragile).\nIf we go that low, we get residual differences in the values, which can make some optimal actions appear non-optimal:\n\n\nCode\nenv = EnvGamblersProblem(p_win=0.25, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-18)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-16)\nplot_multi_policy(\n    [MultiPolicy(best_actions)], title=r\"$p_\\mathrm{win} = 0.25$ and $N=100$ with $\\theta = 10^{-18}\"\n)\n\n\n\n\n\n\n\n\n\nThat said, unlike the issues in Section 4.4.1.8, this doesn’t lead to wrong policies - they still contain only optimal actions.\n\n\nExercise 4.10 What is the analog of the value iteration update Equation 4.5 for action values, \\(q_{k+1}(s, a)\\)?\n\n\nFrom the greedy policy update: \\[\n\\pi_k(s) = \\mathrm{argmax}_a q_k(s,a)\n\\] and the action-value Bellman update: \\[\nq_{k+1}(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma Q(s', \\pi_k(s'))]\n\\] we get the action-value version of the value iteration update: \\[\nq_{k+1}(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\max_{a'} Q(s',a')]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#asynchronous-dynamic-programming",
    "href": "chapters/04-dynamic-programming.html#asynchronous-dynamic-programming",
    "title": "4  Dynamic Programming",
    "section": "4.5 Asynchronous Dynamic Programming",
    "text": "4.5 Asynchronous Dynamic Programming\nNothing to add here,",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#generalized-policy-iteration",
    "href": "chapters/04-dynamic-programming.html#generalized-policy-iteration",
    "title": "4  Dynamic Programming",
    "section": "4.6 Generalized Policy Iteration",
    "text": "4.6 Generalized Policy Iteration\nnor here,",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#efficiency-of-dynamic-programming",
    "href": "chapters/04-dynamic-programming.html#efficiency-of-dynamic-programming",
    "title": "4  Dynamic Programming",
    "section": "4.7 Efficiency of Dynamic Programming",
    "text": "4.7 Efficiency of Dynamic Programming\nor here,",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#summary",
    "href": "chapters/04-dynamic-programming.html#summary",
    "title": "4  Dynamic Programming",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nand not even here.\n\n\n\n\nSiegrist, Kyle. 2023. “Probability, Mathematical Statistics, Stochastic Processes.” https://www.randomservices.org/random/index.html.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#footnotes",
    "href": "chapters/04-dynamic-programming.html#footnotes",
    "title": "4  Dynamic Programming",
    "section": "",
    "text": "They use a continuous version of the gambler’s problem, but the continous value function \\(F\\) can be translated by \\(v_\\ast(s) = F(\\frac{x}{N})\\). That’s because the optimal strategy, ‘bold play’, can also be used in the discrete caes.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  }
]