[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Sutton & Barto",
    "section": "",
    "text": "Preface\nWelcome to my study notes on ‚ÄúReinforcement Learning: An Introduction‚Äù by Richard S. Sutton and Andrew G. Barto (Sutton and Barto 2018). These notes primarily focus on my solutions and insights while working through the exercises in the book.\nI largely copy to the chapter and section structure of the book. I recreate material from the book for the exercises as references and solve the exercises within this framework.\nThe depth of these notes can vary quite a bit. For some chapters I just copy the structure and present the exercises and their solutions. For others I go deeper and add my own insights and also some additional sections. I mark those extra sections with a üîç.\nNote that these notes are still work in progress and even older chapters can change to a later time.\nI hope the notes help you understand this great book better!\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-intro.html",
    "href": "chapters/01-intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "Exercise 1.1 (Self-Play) Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n\n\nSolution 1.1. The algorithm would learn the true game-theoretic values of each board state because, in the long run, both sides would learn the optimal strategy against their opponent.\n\n\nExercise 1.2 (Symmetries) Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n\n\nSolution 1.2. We could alter the learning process by using a canonical representative for each board state instead of the board state itself. This would speed up learning (the algorithm would generalise for symmetric states) and require less memory.\nIf the opponent does not respect the board symmetries, then the environment (board state plus opponent) should be treated as having no symmetries.\n\n\nExercise 1.3 (Greedy Play) Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a non-greedy player? What problems might occur?\n\n\nSolution 1.3. It could potentially learn to play better or worse. The greedy player has the advantage of always exploiting its knowledge. However, it has the significant disadvantage of never exploring. It could end up valuing a position as a draw that is actually a win as it never explores subsequent positions that would lead to win.\n\n\nExercise 1.4 (Learning from Exploration) Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n\n\nSolution 1.4. Without exploratory moves we learn the values of the states according to the optimal policy. With exploratory moves we learn the values of the states according to the \\(\\varepsilon\\)-optimal policy.\nIf we continue playing with \\(\\varepsilon\\)-soft, the latter values are preferable.\n\n\nExercise 1.5 (Other Improvements) Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?\n\n\nSolution 1.5. Surely there are many ways. And I think this book will discuss many of them. Just to give one example we could use \\(n\\)-step temporal difference (update a state‚Äôs value only after \\(n\\) moves).\n\n\n\nCode\nimport scripts.environment.race_track as rt\nimport random\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\ndef plot():\n    fig, ax = plt.subplots(figsize=(4, 4))\n\n    grid = np.array(\n        [[0, 1, 2, 3], [1, 2, 3, 0], [0, 0, 0, 0], [3, 2, 1, 0], [3, 2, 1, 0]]\n    )\n    width, height = grid.shape\n    img = grid.T\n\n    cmap = ListedColormap([\"#444444\", \"#e68a8a\", \"#f2f2f2\", \"#8fd18f\"])\n\n    ax.imshow(img, cmap=cmap, origin=\"lower\", interpolation=\"nearest\")\n\n    ax.set_xticks(np.arange(width))\n    ax.set_yticks(np.arange(height))\n    ax.set_xlim(-0.5, width - 0.5)\n    ax.set_ylim(-0.5, height - 0.5)\n    ax.set_aspect(\"equal\")\n\n    # Rotate x-tick labels\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n    # grid lines\n    for x in range(width):\n        for y in range(height):\n            rect = plt.Rectangle(\n                (x - 0.5, y - 0.5),\n                1,\n                1,\n                fill=False,\n                edgecolor=\"#444444\",\n                linewidth=0.1,\n            )\n            ax.add_patch(rect)\n\n    return fig, ax\n\n\nfig, ax = plot()\nfig.set_size_inches(1, 20)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html",
    "href": "chapters/02-multi-armed-bandits.html",
    "title": "2¬† Multi-armed Bandits",
    "section": "",
    "text": "2.1 A \\(k\\)-armed Bandit Problem\nThe k-armed bandit problem is a very simple example that already introduces a key structure of reinforcement learning: the reward depends on the action taken. It‚Äôs technically not a full Markov chain (more on that in Section 3.1)‚Äîsince there are no states‚Äîbut very similar in that we model it as a sequence of dependent random variables \\(A_1, R_1, A_2, \\dots\\), where each reward \\(R_t\\) depends on the previous action \\(A_t\\).\nThe true value of an action is defined as: \\[\nq_*(a) := \\mathbb{E}[ R_t \\mid A_t = a].\n\\]\nThe time index here doesn‚Äôt play a special role as the action-reward probabilities in the armed bandit are stationary. You can think of the condition meaning ‚Äúwhen/if \\(a\\) is picked‚Äù.\nIf you‚Äôre feeling queasy about the conditional expected value, here‚Äôs a quick refresher on the relevant notation and concepts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#a-k-armed-bandit-problem",
    "href": "chapters/02-multi-armed-bandits.html#a-k-armed-bandit-problem",
    "title": "2¬† Multi-armed Bandits",
    "section": "",
    "text": "2.1.1 Random Variables and Probability üîç\nThe foundations of all processes we discuss here‚Äîarmed bandits or more involved environments that appear later‚Äîare discrete probability spaces \\((\\Omega, \\mathrm{Pr})\\). \\(\\Omega\\) is the set of all possible behaviours of the algorithm on the environment, i.e., complete description of a run, and \\(\\mathrm{Pr}\\) assigns a probability to each outcome: \\[\n\\mathrm{Pr}\\colon \\Omega \\to [0,1] \\quad \\text{with} \\quad\n\\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) = 1.\n\\]\nSince basically everything in this world is finite‚Äîat least computers and the simulated environments we use to learn on them‚Äîthey will end up in a loop, even if we let them run for an infinite number of time steps. I‚Äôm just using this to justify that for this introduction \\(\\Omega\\) is countable, which saves us a bunch of technical details about measurability and other questions that don‚Äôt really matter for us.\nA random variable is a function \\(X\\colon \\Omega \\to \\mathcal{X}\\) from \\(\\Omega\\) to a result space \\(\\mathcal{X}\\).\nWe largely follow the convention of Sutton and Barto (2018): random variables are written in capital letters, and their possible values in lowercase.\nThe actions and rewards on a \\(k\\)-armed bandits are random variables. If we want to refer to the concrete outcome of a random variable (which we actually don‚Äôt often do in theory crafting), we evaluate random variables on a specific \\(\\omega \\in \\Omega\\), which fixes their values. So an arbitrary but concrete action-reward sequence looks like this \\(A_1(\\omega), R_1(\\omega), A_2(\\omega), R_2(\\omega) \\dots\\).\nLet‚Äôs bring up a common conventions that is used for a notation like this ‚Äú\\(\\mathbb{E}[ R_t \\mid A_t = a]\\)‚Äù. When defining a subset of \\(\\Omega\\) like \\(\\{\\omega \\in \\Omega \\colon X(\\omega) = x\\}\\), we just write \\(\\{X = x\\}\\). So the above expression is shorthand for ‚Äú\\(\\mathbb{E}[R_t \\mid \\omega \\in \\Omega : A_t(\\omega) = a]\\)‚Äù. Also, \\(\\mathrm{Pr}(X = x)\\) is shorthand for \\(\\mathrm{Pr}(\\omega \\in \\Omega : X(\\omega) = x)\\)\n\n\n2.1.2 Conditional Probability üîç\nThe reward \\(R_t\\)‚Äã depends on the action \\(A_t\\)‚Äã taken. If we know the value of \\(A_t\\), then the conditional probability that \\(R_t=r\\) given \\(A_t = a\\) is: \\[\n\\mathrm{Pr}(R_t = r \\mid A_t = a) = \\frac{\\mathrm{Pr}(R_t = r, A_t = a)}{\\mathrm{Pr}(A_t = a)},\n\\]\nwhere the comma denotes conjunction of the statements.\nThis is only well-defined if \\(\\mathrm{Pr}(A_t = a) &gt; 0\\), so we have to keep that in mind when we use this formula.\n\n\n2.1.3 Expected Value üîç\nReal-valued random variables like \\(R_t\\colon \\Omega \\to \\mathbb{R}\\) have an expected value \\(\\mathbb{E}[R_t]\\), which is also called their mean. It is defined as: \\[\n\\mathbb{E}[ R_t ] := \\sum_{\\omega \\in \\Omega} R_t(\\omega) \\mathrm{Pr}(\\omega)\n\\]\nA more commonly used form due the ‚Äúlaw of the unconscious statistician‚Äù (LOTUS) is: \\[\n\\mathbb{E}[R_t] = \\sum_{r \\in \\mathbb{R}} r \\; \\mathrm{Pr}(R_t = r)\n\\] (see appendix Theorem¬†2.1).\nTo compute a conditional expectation, we just make everything conditional: \\[\n\\mathbb{E}[R_t \\mid A_t = a] = \\sum_{\\omega \\in \\Omega} R_t(\\omega) \\mathrm{Pr}(\\omega \\mid A_t = a).\n\\]\nOr, using the more practical LOTUS form: \\[\n\\mathbb{E}[R_t \\mid A_t = a] = \\sum_{r \\in \\mathbb{R}} r \\; \\mathrm{Pr}(R_t = r \\mid A_t = a).\n\\]\nWith all that said a more explicit formula of the true value of an action can be given as: \\[\nq_*(a) = \\sum_{r \\in \\mathbb{R}} r \\; \\frac{\\mathrm{Pr}(R_t = r, A_t = a)}{\\mathrm{Pr}(A_t = a)}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#action-value-methods",
    "href": "chapters/02-multi-armed-bandits.html#action-value-methods",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.2 Action-value Methods",
    "text": "2.2 Action-value Methods\nThis part always trips me up, so let me clarify it for myself: \\(Q_t(a)\\) is the estimated value of action \\(a\\) prior to time \\(t\\), not including \\(A_t\\) and it‚Äôs corresponding reward \\(R_t\\).\nInstead, \\(A_t\\) is selected based on the current estimates \\(\\{Q_{t}(a):a \\in \\mathcal{A}\\}\\). For example, our algorithm could pick \\(A_t\\)‚Äã greedily as \\(A_t:=\\mathrm{argmax}_{a \\in \\mathcal{A}} Q_t(a)\\), or \\(\\varepsilon\\)-greedily.\n\nExercise 2.1 In \\(\\varepsilon\\)-greedy action selection, for the case of two actions and \\(\\varepsilon = 0.5\\), what is the probability that the greedy action is selected?\n\n\nSolution 2.1. The total probability of selecting the greedy action is: \\[\n\\mathrm{Pr}(\\text{greedy}) + \\mathrm{Pr}(\\text{exploratory}) \\cdot \\frac{1}{|\\mathcal{A}|} = 0.75\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#the-10-armed-testbed",
    "href": "chapters/02-multi-armed-bandits.html#the-10-armed-testbed",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.3 The 10-armed Testbed",
    "text": "2.3 The 10-armed Testbed\nThe 10-armed testbed will accompany us through the rest of this chapter. So here‚Äôs an implementation of it.\n\n# === the armed bandit ===\nclass ArmedBandit:\n    \"\"\"k-armed Gaussian bandit.\"\"\"\n\n    def __init__(self, action_values, reward_sd, seed):\n1        self.action_values = np.asarray(action_values, dtype=np.float64)\n        self.reward_sd = np.float64(reward_sd)\n        self.seed = seed\n        self.rng = np.random.default_rng(self.seed)\n\n    def pull_arm(self, arm):\n        return self.rng.normal(\n            loc=self.action_values[arm],\n            scale=self.reward_sd,\n        )\n\n\n1\n\nAs you can see internally the armed bandit uses numpy. Actually, I forgot why I made this choice, as this looks needlessly complicated and numpy is only necessary for parallel processing which doesn‚Äôt happen in these notes.\n\n\n\n\nAnd here how the standard setup looks like that we typically use for the experiments in this chapter.\n\n# === typical bandit setup ===\n\nn_arms = 10\n1rng = np.random.default_rng(0)\n\naction_values = rng.normal(loc=0, scale=1, size=n_arms)\nreward_sd = 1\nenv = ArmedBandit(action_values, reward_sd, seed=0)\n\n\n1\n\nI usually set the random generator‚Äôs seed in my examples. It‚Äôs just better to have reproducible results. Usually, I use a seed of \\(0\\), but sometimes I cherry-pick a seed to highlight an issue.\n\n\n\n\nAs an example how the rewards look like that we get from the bandit, let‚Äôs pull each arm of the bandit we have just set up a couple of times. The violin plots of these rewards creates a picture similar to (Sutton and Barto 2018, fig. 2.1) only that here we see the empirical data.\n\nCode\nn_pulls = 10\nrewards = []\nfor arm in range(n_arms):\n    rewards.append([env.pull_arm(arm) for _ in range(n_pulls)])\n\n\n# __plot stuff__\ndf_rewards = pd.DataFrame(\n    {\n        \"Arm\": np.repeat(np.arange(n_arms), n_pulls),\n        \"Reward\": np.concatenate(rewards),\n    }\n)\n\n\nplt.figure(figsize=(12, 6))\nplt.scatter(\n    x=np.arange(n_arms),\n    y=action_values,\n    label=\"True Action Value\",\n    s=100,\n    marker=\"o\",\n    zorder=3,\n    alpha=0.5,\n)\nsns.violinplot(x=\"Arm\", y=\"Reward\", data=df_rewards)\nplt.title(f\"Violin plot of rewards for each arm after {n_pulls} pulls\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.1: Plot of the random rewards for each arm. The bandit setup is such that the action values are \\(q_*(a) \\sim N(0,1)\\) and each reward \\(R\\) for pulling \\(a\\) is \\(R \\sim N(q_*(a), 1)\\). This basically visualises the information available to the bandit algorithms, only that we want algorithms that minimize the pulls for ‚Äòbad arms‚Äô. (Sutton and Barto 2018, fig. 2.1)\n\n\n\nAnd here‚Äôs the code for the sample-average bandit algorithm. For clarity, I‚Äôll refer to this and upcoming algorithms as ‚Äòagents‚Äô. Note that this implementation is using the incremental implementation from Section 2.4.\n\n# === the sample average bandit agent ===\nclass SampleAverageBanditAgent:\n    def __init__(self, Q1, Œµ, seed=None):\n        self.rng = np.random.default_rng(seed)\n        self.num_actions = len(Q1)\n        self.Q1 = np.asarray(Q1, dtype=np.float64)\n        self.Œµ = Œµ\n        self.reset()\n\n    def reset(self):\n        self.Q = self.Q1.copy()\n        self.counts = np.zeros(self.num_actions, dtype=int)\n\n    def act(self, bandit):\n        # Œµ-greedy action selection\n        if self.rng.random() &lt; self.Œµ:\n            action = self.rng.integers(self.num_actions)\n        else:\n            action = np.argmax(self.Q)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update count and value estimate\n        self.counts[action] += 1\n        Œ± = 1 / self.counts[action]\n        self.Q[action] += Œ± * (reward - self.Q[action])\n\n        return (action, reward)\n\nThe next code bit, which is collapsed, defines the bandit_experiment function that we‚Äôll use throughout this chapter. In short, it repeatedly runs a bunch of bandit agents for fixed number of steps and returns two arrays: the average reward per step and the percentage of optimal actions taken per step‚Äîboth averaged over all runs. These results can then be visualised using the plotting functions also provided in this code snipped. You don‚Äôt have to read the code unless you‚Äôre curious‚Ä¶\n\n\nCode\n# === the core bandit experiment ===\n\n\n# --- config\n@dataclass\nclass Config:\n    bandit_num_arms: int = 10\n    bandit_q_mu: float = 0.0\n    bandit_q_sd: float = 1.0\n    bandit_reward_sd: float = 1.0\n    bandit_q_drift: bool = False\n    bandit_q_drift_mu: float = 0.0\n    bandit_q_drift_sd: float = 0.0\n    exp_steps: int = 1_000\n    exp_runs: int = 200\n    exp_seed: int = 0\n\n\n# -- core experiment\ndef bandit_experiment(agents, config: Config) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run `exp_runs` √ó `exp_steps` episodes and return:\n\n    average_rewards         shape = (len(agents), exp_steps)\n    optimal_action_percent  shape = (len(agents), exp_steps)\n    \"\"\"\n    rng = np.random.default_rng(config.exp_seed)\n    num_agents = len(agents)\n    average_rwds = np.zeros((num_agents, config.exp_steps))\n    optimal_acts = np.zeros((num_agents, config.exp_steps))\n\n    # single bandit object shell\n    bandit = ArmedBandit(\n        action_values=np.empty(config.bandit_num_arms),\n        reward_sd=config.bandit_q_sd,\n        seed=config.exp_seed,\n    )\n\n    for run in range(config.exp_runs):\n        # fresh true values for this run\n        bandit.action_values[:] = rng.normal(\n            config.bandit_q_mu, config.bandit_q_sd, size=config.bandit_num_arms\n        )  # todo: is [:] necessary?\n        best_action = np.argmax(bandit.action_values)\n\n        for agent in agents:\n            agent.reset()\n\n        # drift noise for the whole run\n        if config.bandit_q_drift:\n            drift_noise = rng.normal(\n                config.bandit_q_drift_mu,\n                config.bandit_q_drift_sd,\n                size=(config.exp_steps, config.bandit_num_arms),\n            )\n\n        # main loop\n        for t in range(config.exp_steps):\n            for i, agent in enumerate(agents):\n                act, rwd = agent.act(bandit)\n                average_rwds[i, t] += rwd\n                optimal_acts[i, t] += act == best_action\n\n            if config.bandit_q_drift:\n                bandit.action_values += drift_noise[t]\n                best_action = np.argmax(bandit.action_values)\n\n    # mean over runs\n    average_rwds /= config.exp_runs\n    optimal_acts = 100 * optimal_acts / config.exp_runs\n    return average_rwds, optimal_acts\n\n\n# --- thin plotting helpers\ndef plot_average_reward(\n    average_rewards: np.ndarray,\n    *,\n    labels: Sequence[str] | None = None,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"One line per agent: average reward versus step.\"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(8, 4))\n\n    steps = np.arange(1, average_rewards.shape[1] + 1)\n    if labels is None:\n        labels = [f\"agent {i}\" for i in range(average_rewards.shape[0])]\n\n    for i, lbl in enumerate(labels):\n        ax.plot(steps, average_rewards[i], label=lbl)\n\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Average reward\")\n    ax.set_title(\"Average reward per step\")\n    ax.grid(alpha=0.3, linestyle=\":\")\n    ax.legend()\n    return ax\n\n\ndef plot_optimal_action_percent(\n    optimal_action_percents: np.ndarray,\n    *,\n    labels: Sequence[str] | None = None,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"One line per agent: % optimal action versus step.\"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(8, 4))\n\n    steps = np.arange(1, optimal_action_percents.shape[1] + 1)\n    if labels is None:\n        labels = [f\"agent {i}\" for i in range(optimal_action_percents.shape[0])]\n\n    for i, lbl in enumerate(labels):\n        ax.plot(steps, optimal_action_percents[i], label=lbl)\n\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"% optimal action\")\n    ax.set_title(\"Optimal-action frequency\")\n    ax.grid(alpha=0.3, linestyle=\":\")\n    ax.legend()\n    return ax\n\n\n‚Ä¶ but it‚Äôs still helpful to see such an experiment in action. We can, for example, recreate Figure 2.2 (Sutton and Barto 2018). It compares the performance of a greedy agent (\\(\\varepsilon = 0\\)), and two \\(\\varepsilon\\)-greedy agents with \\(\\varepsilon =0.1\\) and \\(\\varepsilon=0.01\\). We let them run for a couple of steps (exp_steps) and also repeat such a run a couple of times (exp_runs) so we can take the average over all the runs.\n\nconfig = Config(exp_steps=1_000, exp_runs=2_000)\n\n# initialize agents with different epsilon values\nepsilons = [0.0, 0.1, 0.01]\nagents = [\n    SampleAverageBanditAgent(\n        Q1=np.zeros(config.bandit_num_arms),\n        Œµ=Œµ,\n        seed=config.exp_seed,\n    )\n    for Œµ in epsilons\n]\n\n# run bandit experiment\navg_rwd, opt_pct = bandit_experiment(agents, config)\n\nThe result of this experiment is shown in these figures.\n\n\n\n\n\n\nCode\nlabels = [f\"Œµ={e}\" for e in epsilons]\n\nplot_average_reward(avg_rwd, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The average reward\n\n\n\n\n\n\nCode\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The percentage of optimal step selection\n\n\n\n\n\n\n\nFigure¬†2.2: The performance of Œµ-greedy sample average agents on the 10-armed testbed for different values \\(\\varepsilon\\) averaged over 2000 runs. (Sutton and Barto 2018, fig. 2.2)\n\n\n\nOut of curiosity, let‚Äôs see what happens when we have only one run, so there is no averaging to be done. It‚Äôs a mess, now. Without averaging over a couple of runs, we can‚Äôt make out anything.\n\n\nCode\n# === experiment with only one run ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1)\n\nepsilons = [0.0, 0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), Œµ=Œµ, seed=config.exp_seed) for Œµ in epsilons]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nplot_average_reward(avg_rwd, labels=[f\"Œµ={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 (Bandit example) Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \\(\\varepsilon\\)-greedy action selection, sample-average action-value estimates, and initial estimates of \\(Q_1(a) = 0\\), for all a. Suppose the initial sequence of actions and rewards is \\(A_1 = 1\\), \\(R_1 = -1\\), \\(A_2 = 2\\), \\(R_2 = 1\\), \\(A_3 = 2\\), \\(R_3 = -2\\), \\(A_4 = 2\\), \\(R_4 = 2\\), \\(A_5 = 3\\), \\(R_5 = 0\\). On some of these time steps the \\(\\varepsilon\\) case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n\n\nSolution 2.2. Generally, we can only positively conclude that an exploratory was taken, when the taken action is not greedy. Exculding the posibility for an exploratory action selection is imposible.\nSo each time step could have been an explorotory action.\nIf we keep track of the estimated value function \\[\n\\begin{split}\nQ_1(a) &= 0 \\\\\nQ_2(a) &= \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases} \\\\\nQ_3(a) &= \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            1,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases} \\\\\nQ_4(a) &= \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            -0.5,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\\\\\nQ_5(a) &= \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            0.33,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}            \n\\end{split}                      \n\\]\nwe can see that in step 4 and 5 a non-greedy action were taken, so these must have been exploratory moves.\n\n\nExercise 2.3 In the comparison shown in Figure¬†2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n\n\nSolution 2.3. Obviously, we can disregard \\(\\varepsilon = 0\\). It‚Äôs just rubbish. Before we do the quantitative analysis, let‚Äôs see what happens when we just crank up the number of steps (and in return reduce the number of runs, which makes it a bit noisier).\n\n\nCode\n# === battle between Œµ=0.1 and Œµ=0.01 ===\nconfig = Config(\n    exp_steps=15_000,\n    exp_runs=200)\n\nepsilons = [0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), Œµ=Œµ, seed=config.exp_seed) for Œµ in epsilons]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nplot_average_reward(avg_rwd, labels=[f\"Œµ={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=[f\"Œµ={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that \\(\\varepsilon=0.01\\) outperforms \\(\\varepsilon=0.1\\) in average reward around step \\(2000\\). However, achieving a higher percentage of optimal actions takes more than \\(10,000\\) steps. It‚Äôs actually quite interesting that this takes significantly longer.\nNow, let‚Äôs consider the asymptotic long-term behaviour. We can assume both methods have near-perfect \\(Q\\)-values and the only reason they select non-optimal actions is due to their \\(\\varepsilon\\)-softness.\nThis makes calculating the optimal action probability quite easy. \\[\n\\mathrm{Pr}(\\text{optimal action}) = (1-\\varepsilon) + \\varepsilon \\frac{1}{10} = 1 - 0.9 \\varepsilon\n\\]\nSo for \\(\\varepsilon=0.1\\) this probability is \\(0.91\\), and for \\(\\varepsilon=0.01\\) this is \\(0.991\\).\nNow the average reward is trickier to compute. It can be done, but it‚Äôs quite messy and we‚Äôre here to learn reinforcement learning so we don‚Äôt need to figure out perfect analytical solutions anymore. Luckily, we get this value directly from the book\n\nIt [greedy algorithm] achieved a reward-per-step of only about \\(1\\), compared with the best possible of about \\(1.55\\) on this testbed (Sutton and Barto 2018, 29).\n\nGreat‚Äîthey‚Äôve done the work for us. Selecting the optimal action gives an average reward of \\(1.55\\). Selecting a random action has an average reward of \\(0\\) because it‚Äôs basically drawing a sample from a normal distribution with mean \\(0\\). That gives:\n\\[\n\\mathbb{E}[R_t] = (1-\\varepsilon) 1.55 + \\varepsilon 0 = 1.55 (1-\\varepsilon)\n\\]\nThis results in \\(1.40\\) for \\(\\varepsilon = 0.1\\) and \\(1.53\\) for \\(\\varepsilon = 0.01\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#sec-incremental-implementation",
    "href": "chapters/02-multi-armed-bandits.html#sec-incremental-implementation",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.4 Incremental Implementation",
    "text": "2.4 Incremental Implementation\nThe sample average can be updated incrementally using: \\[\nQ_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n].\n\\tag{2.1}\\]\nThis is an instance of a general pattern that is central to reinforcement learning: \\[\n\\text{NewEstimate} \\gets \\text{OldEstimate} + \\text{StepSize}\n\\Big[\\overbrace{\n    \\text{Target} - \\text{OldEstimate}\n    }^\\text{error} \\Big]\n\\]\nI especially like how nice it looks in python. In value-based algorithms, this typically corresponds to the following line of code:\n\nQ[action] += Œ± * (reward - Q[action])",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#tracking-a-nonstationary-problem",
    "href": "chapters/02-multi-armed-bandits.html#tracking-a-nonstationary-problem",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.5 Tracking a Nonstationary Problem",
    "text": "2.5 Tracking a Nonstationary Problem\nTo avoid the learning rate decreasing over time we can use a constant step size \\(\\alpha \\in (0,1]\\). Keep in mind that with a constant step size we don‚Äôt have convergence in the long run.\nThe update formula with constant step is: \\[\nQ_{n+1} := Q_n + \\alpha \\Big[ R_n - Q_n \\Big],\n\\tag{2.2}\\]\nfor \\(n \\geq 1\\) and \\(Q_1\\) is our given initial estimate.\nThis recursive definition can also be be phrased as a recurrence relation. We this recursive relation for \\(n=0\\) by \\(Q_0 = 0\\) and \\(R_0 = \\frac{Q_1}{\\alpha}\\). Then, Equation¬†2.2 is equivalent to: \\[\nQ_{n+1} = \\alpha R_n + (1 - \\alpha) Q_n \\quad \\text{and} \\quad Q_0 = 0,\n\\tag{2.3}\\]\nBy Theorem¬†2.6 we get \\[\nQ_{n+1} = \\sum_{i=0}^n (1 - \\alpha)^{n-i} \\alpha R_{i}\n\\]\nSubstituting back \\(R_0 = \\frac{Q_1}{\\alpha}\\) yields the form used by Sutton and Barto (2018): \\[\nQ_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} R_i\n\\tag{2.4}\\]\nThis is a weighted sum. The sum of the weights for the \\(R_i\\) is (using the geometric series identity Equation¬†2.15): \\[\n\\begin{split}\n\\sum_{i=1}^n \\alpha (1- \\alpha)^{n-i} &= \\alpha \\sum_{i=o}^{n-1}(1-\\alpha)^i \\\\\n&= \\alpha \\frac{1 - (1-\\alpha)^n}{\\alpha}\\\\\n&= 1 - (1 - \\alpha)^n.\n\\end{split}\n\\]\nThus, the total weight sums to 1: \\[\n\\begin{split}\n(1-\\alpha)^n + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} &= 1\n\\end{split}\n\\]\nThe \\(Q_n\\) are estimators for the true action value \\(q_*\\). And depending on how we determine the \\(Q_n\\), they have different qualities. We note that the \\(R_i\\) are IID with mean \\(q_*\\) and variance \\(\\sigma^2\\). (I refer to the appendix Section 2.11 for more information about all the new terms appearing all of a sudden, as this has gotten quite a bit more technical)\nIf \\(Q_n\\)‚Äã is the sample average, the estimator is unbiased, that is \\[\n\\mathbb{E}[Q_n] = q_* \\quad \\text{for all } n \\in \\mathbb{N}.\n\\] Which is easy to show. Its mean squared error \\(\\mathrm{MSE}(Q_n) := \\mathbb{E}[(Q_n - q_*)^2]\\) is decreasing (Lemma¬†2.4): \\[\n\\mathrm{MSE}(Q_n) = \\frac{\\sigma^2}{n}.\n\\]\nIf the \\(Q_n\\) are calculated using a constant step size, they are biased (there is always a small dependency on \\(Q_1\\)): \\[\n\\begin{split}\n\\mathbb{E}[Q_{n+1}] &=  (1-\\alpha)^n Q_1 + q\\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i}   \\\\\n&= (1-\\alpha)^n Q_1 + q (1 - (1 - \\alpha)^n)\n\\end{split}\n\\tag{2.5}\\]\nAnd even though they are asymptotically unbiased, i.e., \\(\\lim_{n\\to\\infty} \\mathbb{E}[Q_{n}] = q\\), their mean squared error is bounded away from zero (Lemma¬†2.5): \\[\n\\mathrm{MSE}(Q_n) &gt; \\sigma^2 \\frac{\\alpha}{2-\\alpha}.\n\\]\nIt‚Äôs hard for me to translate these stochastic results in statistic behaviour, but I think that means that constant step size will end up wiggling around the true value.\nLet‚Äôs define the constant step size agent to compare it with the sample average method later.\n\n# === the constant step bandit agent ===\nclass ConstantStepBanditAgent:\n    def __init__(self, Q1, Œ±, Œµ, seed=None):\n        self.rng = np.random.default_rng(seed)\n        self.num_actions = len(Q1)\n        self.Q1 = Q1\n        self.Œ± = Œ±\n        self.Œµ = Œµ\n        self.reset()\n\n    def reset(self):\n        self.Q = self.Q1.copy()\n\n    def act(self, bandit):\n        # Œµ-greedy action selection\n        if self.rng.random() &lt; self.Œµ:\n            action = self.rng.integers(self.num_actions)\n        else:\n            action = np.argmax(self.Q)\n\n        # take action\n        reward = bandit.pull_arm(action)\n\n        # update value estimate\n        self.Q[action] += self.Œ± * (reward - self.Q[action])\n\n        return (action, reward)\n\n\nExercise 2.4 If the step-size parameters, \\(\\alpha_n\\), are not constant, then the estimate \\(Q_n\\) is a weighted average of previously received rewards with a weighting different from that given by Equation¬†2.4. What is the weighting on each prior reward for the general case, analogous to Equation¬†2.4, in terms of the sequence of step-size parameters.\n\n\nSolution 2.4. The update rule for non-constant step size has \\(\\alpha_n\\) depending on the step. \\[\nQ_{n+1} = Q_n + \\alpha_n \\Big[ R_n - Q_n \\Big]\n\\tag{2.6}\\]\nIn this case the weighted average is given by \\[\nQ_{n+1} = \\left( \\prod_{j=1}^n 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^n \\alpha_i \\left( \\prod_{j=i+1}^n 1 - \\alpha_j \\right) R_i\n\\tag{2.7}\\]\nThis explicit form can be verified inductively. For \\(n=0\\), we get \\(Q_1\\) on both sides.\nFor the induction step we have \\[\n\\begin{split}\nQ_{n+1} &= Q_n + \\alpha_n \\Big[ R_n - Q_n \\Big] \\\\\n&= \\alpha_n R_n + (1 - \\alpha_n) Q_n \\\\\n&= \\alpha_n R_n + (1 - \\alpha_n) \\Big[ \\left( \\prod_{j=1}^{n-1} 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^{n-1} \\alpha_i \\left( \\prod_{j=i+1}^{n-1} 1 - \\alpha_j \\right) R_i \\Big] \\\\\n&= \\left( \\prod_{j=1}^n 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^n \\alpha_i \\left( \\prod_{j=i+1}^n 1 - \\alpha_j \\right) R_i\n\\end{split}\n\\]\nNote that the more general Equation¬†2.7 is still a weighted average. We could prove it by induction or use a little trick. If we set \\(Q_1 = 1\\) and \\(R_n = 1\\) for all \\(n\\) in the recurrence relation Equation¬†2.6 we see that each \\(Q_n = 1\\). If we do the same in the explicit formula Equation¬†2.7 we see that each \\(Q_n\\) is equal to the sum of the weights. Therefore, the weights sum up to \\(1\\).\n\n\nExercise 2.5 Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the \\(q_\\star(a)\\) start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the \\(q_\\star(a)\\) on each step). Prepare plots like Figure Figure¬†2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, \\(\\alpha = 0.1\\). Use \\(\\epsilon = 0.1\\) and longer runs, say of 10,000 steps\n\n\nSolution 2.5. Alright, let‚Äôs do a little experiment, just as they told us.\n\n\nCode\n# === battle between sample average and constant step ===\nconfig = Config(\n    bandit_q_mu=0,\n    bandit_q_sd=0,\n    bandit_q_drift=True,\n    bandit_q_drift_mu=0,\n    bandit_q_drift_sd=0.01,\n    exp_steps=10_000,\n    exp_runs=100,\n)\n\nagent0 = SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms, dtype=float), Œµ=0.1, seed=config.exp_seed)\nagent1 = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), Œ±=0.1, Œµ=0.1, seed=config.exp_seed\n)\nagents = [agent0, agent1]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\"sample averages (Œµ=0.1)\", \"constant step-size (Œ±=0.1, Œµ=0.1)\"]\nplot_average_reward(avg_rwd, labels=labels)\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot surprisingly, we can see how much the sample average agent struggles to keep up. For longer episode lengths, the problem only gets worse. Eventually, it will be completely out of touch with the world, like an old man unable to keep up with the times.\nHowever, it remains unclear exactly how rapidly the sample average method will deteriorate to an unacceptable level. The results from the final exercise of this chapter (Exercise¬†2.11) indicate that this deterioration may take a significant number of steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#optimistic-initial-values",
    "href": "chapters/02-multi-armed-bandits.html#optimistic-initial-values",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.6 Optimistic Initial Values",
    "text": "2.6 Optimistic Initial Values\nWe need the following figure comparing an optimistic greedy agent and a realistic \\(\\varepsilon\\)-greedy agent.\n\nCode\n# === realism vs optimism ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1_000\n)\nagent_optimistic_greedy = ConstantStepBanditAgent(\n    Q1=np.full(config.bandit_num_arms, 5.0, dtype=float), Œ±=0.1, Œµ=0.0, seed=config.exp_seed\n)\nagent_realistic_Œµ_greedy = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), Œ±=0.1, Œµ=0.1, seed=config.exp_seed\n)\n\nagents = [agent_optimistic_greedy, agent_realistic_Œµ_greedy]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"optimistic,greedy (Q1=5, Œµ=0, Œ±=0.1)\",\n    \"realistic,Œµ-greedy (Q1=0, Œµ=0.1, Œ±=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.3: The effect of optimistic initial action-value estimates. (Sutton and Barto 2018, fig. 2.3)\n\n\n\n\nExercise 2.6 (Mysterious Spikes) The results shown in Figure¬†2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n\n\nSolution 2.6. We have the hard-earned luxury that we can zoom in on Figure¬†2.3:\n\n\nCode\n# === realism vs optimism zoomed in ===\nconfig = Config(\n    exp_steps=30,\n    exp_runs=5_000,\n)\nagent_optimistic_greedy = ConstantStepBanditAgent(\n    Q1=np.full(config.bandit_num_arms, 5.0, dtype=float),\n    Œ±=0.1,\n    Œµ=0.0,\n    seed=config.exp_seed,\n)\nagent_realistic_Œµ_greedy = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), Œ±=0.1, Œµ=0.1, seed=config.exp_seed\n)\n\nagents = [agent_optimistic_greedy, agent_realistic_Œµ_greedy]\navg_rwd, opt_pct = bandit_experiment(agents, config)\n\nlabels = [\n    \"optimistic,greedy (Q1=5, Œµ=0, Œ±=0.1)\",\n    \"realistic,Œµ-greedy (Q1=0, Œµ=0.1, Œ±=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe spike occurs at step 11. Essentially, the optimistic method samples all actions once (poor performance), and then selects the action with the best result (good performance, with a success rate of over 40%). However, regardless of the outcome (which likely pales in comparison to the current Q-values, which are still likely greater than 4), the method returns to exploring all 10 actions again. This leads to poor performance once more. Around step 22, there is another spike for similar reasons, but this time smaller and more spread out.\n\n\nExercise 2.7 (Unbiased Constant-Step-Size Trick) In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of onstant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of \\[\n\\beta_n := \\alpha / \\bar{o}_n\n\\] to process the \\(n\\)-th reward for a particular action, where \\(\\alpha &gt; 0\\) is a conventional constant step size, and \\(\\bar{o}_n\\) is a trace of one that starts at \\(0\\): \\[\n\\bar{o}_n := \\bar{o}_{n-1} + \\alpha (1 - \\bar{o}_{n-1}), \\text{ for } n \\geq 1, \\text{ with } \\bar{o}_0 := 0\n\\] Carry out an analyises like that in Equation¬†2.4 to show that \\(Q_n\\) is an exponential recency-weighted average without initial bias.\n\n\nSolution 2.7. My first question when I saw this was, ‚ÄúWhat‚Äôs up with that strange name \\(\\bar{o}_n\\)?‚Äù‚Äù I guess it could be something like ‚Äúthe weighted average of ones‚Äù, maybe? Well, whatever. Let‚Äôs crack on.\nWhen we rewrite \\(\\bar{o}_{n+1}\\) as a recurrence relation for \\(\\frac{\\bar{o}_{n+1}}{\\alpha}\\) \\[\n\\frac{\\bar{o}_{n+1}}{\\alpha} = 1 + (1 - \\alpha) \\frac{\\bar{o}_n}{\\alpha}\n\\] we see that it is just the recurrence relation for a geometric series as in Equation¬†2.14 for \\(\\gamma = 1-\\alpha\\). Thus we get \\[\n\\bar{o}_n = \\alpha\\sum_{i=0}^{n-1} (1 - \\alpha)^{i} = 1 - (1-\\alpha)^n\n\\] and \\(\\beta_n = \\frac{\\alpha}{1-(1-\\alpha)^n}\\)\n(btw. this is such a complicated way to define \\(\\beta_n\\) and I don‚Äôt understand why actually.)\nIn particular we have that \\(\\beta_1 = 1\\), which makes the influence of \\(Q_1\\) disappears after the first reward \\(R_1\\) is received: \\(Q_2 = Q_1 + 1 [ R_1 - Q_1] = R_1\\). Great!\nScaling the \\(\\alpha\\) by the \\(\\bar{o}_n\\) has an additional nicer effect. I don‚Äôt quite understand how, but we can calculate it.\nFrom Exercise¬†2.4 we know \\[\nQ_{n+1} = Q_1 \\prod_{j=1}^n (1-\\beta_j)\n+ \\sum_{i=1}^n  R_i \\beta_i \\prod_{j=i+1}^n (1- \\beta_j )\n\\]\nThere is a nice form for these products \\[\n\\prod_{j=i}^n (1 - \\beta_j) = (1-\\alpha)^{n-j+1} \\frac{\\bar{o}_{i-1}}{\\bar{o}_n}\n\\]\nsince they are telescoping using \\[\n\\begin{split}\n1- \\beta_j &= 1 - \\frac{\\alpha}{\\bar{o}_j} = \\frac{\\bar{o}_j - \\alpha}{\\bar{o}_j}\\\\\n&= \\frac{\\alpha + (1-\\alpha) \\bar{o}_{j-1}}{\\bar{o}_j} = (1-\\alpha)\\frac{\\bar{o}_{j-1}}{\\bar{o}_j}.\n\\end{split}\n\\]\nThis gives the following closed form for \\(Q_{n+1}\\) \\[\nQ_{n+1} = \\frac{\\alpha}{1 - (1-\\alpha)^n}\\sum_{i=1}^n R_i (1-\\alpha)^{n-i}\n\\]\nWe can see that the weight given to any reward \\(R_i\\)‚Äã decreases exponentially.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#upper-confidence-bound-action-selection",
    "href": "chapters/02-multi-armed-bandits.html#upper-confidence-bound-action-selection",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.7 Upper-Confidence-Bound Action Selection",
    "text": "2.7 Upper-Confidence-Bound Action Selection\nWe have the opportunity to introduce a new agent here. The update rule remains the same (I assume sample average), but the action selection is more informed compared to \\(\\varepsilon\\)-greedy algorithms.\n\\[\nA_t := \\mathrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{ \\frac{\\ln t}{N_t(a)} }\\right]\n\\tag{2.8}\\]\nwhere \\(N_t(a)\\) is the number of times that action has been selected, c &gt; 0 controls the exploration (similar to \\(\\varepsilon\\)).\nIf an action has not been selected even once, i.e., \\(N_t(a)=0\\), then \\(a\\) is considered to be a maximizing action. (In our case, this means that in the first few steps, all actions have to be selected once, and only after that does the UCB-based action selection kick in.)\nBy the way, I have no idea where the UCB formulation comes from, but at least it looks fancy (and reasonable), and it doesn‚Äôt look too hard to implement it:\n\n# === the ucb bandit agent ===\nclass UcbBanditAgent:\n    def __init__(self, num_actions, c, seed=None):\n        self.num_actions = num_actions\n        self.c = c  # exploration parameter\n        self.reset()\n        self.rng = np.random.default_rng(seed)\n\n    def reset(self):\n        self.t = 0\n        self.Q = np.zeros(self.num_actions, dtype=float)\n        self.counts = np.zeros(self.num_actions, dtype=int)\n\n    def act(self, bandit):\n        self.t += 1\n\n        # upper-Confidence-Bound Action Selection\n        if self.t &lt;= self.num_actions:\n            # not all actions have been tried yet\n            action = self._choose_untaken_action()\n        else:\n            ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / (self.counts))\n            action = np.argmax(ucb_values)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update count and value estimate\n        self.counts[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.counts[action]\n\n        return (action, reward)\n\n    def _choose_untaken_action(self):\n        return self.rng.choice(np.where(self.counts == 0)[0])\n\nLet‚Äôs recreate the figure illustrating UCB action selection performance, which we‚Äôll need for the next exercise.\n\nCode\n# === ucb agent performance ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1_000,\n)\nagent_ucb = UcbBanditAgent(num_actions=config.bandit_num_arms, c=2, seed=config.exp_seed)\nagent_Œµ_greedy = SampleAverageBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), Œµ=0.1, seed=config.exp_seed\n)\n\nagents = [agent_ucb, agent_Œµ_greedy]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"ucb (c=2, Œ±=0.1)\",\n    \"Œµ-greedy (Œµ=0.1, Œ±=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.4: Average performance of UCB action selection. (Sutton and Barto 2018, fig. 2.4)\n\n\n\n\nExercise 2.8 (UCB Spikes) In Figure¬†2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if \\(c = 1\\), then the spike is less prominent.\n\n\nSolution 2.8. I think the answer is similar to the answer to Exercise 2.6. The first \\(10\\) steps the UCB algorithm tries out all actions. Then on step \\(11\\) it will select the one that scored highest, which is quite a decent strategy. But then because of the \\(N_t‚Äã(a)\\) in the denominator it goes back to exploring. The higher \\(c\\) is the more it does so:\n\n\nCode\nexp_runs = 2_000\nconfig = Config(exp_steps=15, exp_runs=exp_runs)\n\ncs = [1 / 2, 1, 2, 3]\nagents = [\n    UcbBanditAgent(num_actions=config.bandit_num_arms, c=c, seed=config.exp_seed)\n    for c in cs\n]\navg_rwd, opt_pct = bandit_experiment(agents, config)\n\nplot_optimal_action_percent(opt_pct, labels=[f\"c={c}\" for c in cs])\nplt.tight_layout()\nplt.title(\"Optimal-action frequency for UCB (average {exp_runs} runs)\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#gradient-bandit-algorithms",
    "href": "chapters/02-multi-armed-bandits.html#gradient-bandit-algorithms",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.8 Gradient Bandit Algorithms",
    "text": "2.8 Gradient Bandit Algorithms\nThis introduces a novel method that is not value-based; instead, it directly aims to select the best actions. The agent maintains numerical preferences, denoted by \\(H_t‚Äã(a)\\), rather than estimates of the action values.\nFor action selection, gradient bandit uses the softmax distribution: \\[\n\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}.\n\\tag{2.9}\\]\nShifting all preferences by a constant \\(C\\) doesn‚Äôt affect \\(\\pi_t(a)\\): \\[\n\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} = \\frac{e^Ce^{H_t(a)}}{\\sum_{b=1}^k e^Ce^{H_t(b)}} = \\frac{e^{H_t(a)+C}}{\\sum_{b=1}^k e^{H_t(b)+C}}\n\\]\nFor learning, gradient bandit uses the following rule: \\[\n\\begin{split}\nH_{t+1}(a) &:= H_t(a) + \\alpha (R_t - \\bar{R}_t) (\\mathbb{I}_{a = A_t} - \\pi_t(a))\n\\end{split}\n\\tag{2.10}\\]\nNow, let‚Äôs implement this gradient bandit algorithm:\n\n# === the gradient agent ===\nclass GradientBanditAgent:\n    def __init__(self, H1, Œ±, baseline=True, seed=None):\n        self.num_actions = len(H1) \n        self.Œ± = Œ± \n        self.H1 = np.asarray(H1, dtype=np.float64)  # initial preferences\n        self.baseline = baseline # apply average reward baseline\n        self.reset()\n        self.rng = np.random.default_rng(seed)\n\n    def reset(self):\n        self.H = self.H1.copy()  \n        self.avg_reward = 0 \n        self.t = 0  # step count\n\n    def act(self, bandit):\n        self.t += 1\n\n        # select action using softmax\n        action_probs = GradientBanditAgent.softmax(self.H)\n        action = self.rng.choice(self.num_actions, p=action_probs)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update average reward\n        if self.baseline:\n            self.avg_reward += (reward - self.avg_reward) / self.t\n\n        # update action preferences\n        advantage = reward - self.avg_reward # avg_reward = 0 if baseline = false\n        one_hot_action = np.eye(self.num_actions)[action]\n        self.H += self.Œ± * advantage * (one_hot_action - action_probs)\n\n        return action, reward\n\n    @staticmethod\n    def softmax(x):\n        # shift vector by max(x) to avoid hughe numbers.\n        # This is basically using the fact that softmax(x) = softmax(x + C)\n        exp_x = np.exp(x - np.max(x))\n        return exp_x / np.sum(exp_x)\n\nSutton and Barto (Sutton and Barto 2018, 37) emphasize the importance of the baseline \\(\\bar{R}_t\\)‚Äã in the update forumla and show that performance drops without it. In the derivation of the update as a form of stochastic gradient ascent, the baseline can be chosen arbitrarily (see Section 2.8.1). Whether or not a baseline is used, the resulting updates are unbiased estimators of the gradient. I assume, the baseline serves to reduce the variance of the estimator, although I have no idea about the maths behind it.\nHere we recreat Figure 2.5 (Sutton and Barto 2018), which shows how drastically the running average baseline can improve performance.\n\nCode\n# === gradient bandit performance ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=50,\n    bandit_q_mu=4,\n)\n\nalphas = [0.1, 0.4]\nagents = [GradientBanditAgent(H1=np.zeros(config.bandit_num_arms), Œ±=Œ±, seed=config.exp_seed) for Œ± in alphas] + [GradientBanditAgent(H1=np.zeros(config.bandit_num_arms), Œ±=Œ±, baseline=False, seed=config.exp_seed) for Œ± in alphas]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [f\"Œ± = {Œ±}, with baseline\" for Œ± in alphas] + [f\"Œ± = {Œ±}, without baseline\" for Œ± in alphas]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.5: Average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the \\(q_*(a)\\) are chosen to be near \\(+4\\) rather than near zero. (we averaged over 50 runs because it gives this cool jaggedy looking graph). (Sutton and Barto 2018, fig. 2.5)\n\n\n\n\nExercise 2.9 Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.\n\n\nSolution 2.9. The logistic function is defined as \\[\n\\sigma(x) := \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}.\n\\] If we map the two preferences \\(H(a_1), H(a_2)\\) to a single value \\(\\Delta = H(a_1) - H(a_2)\\) then \\[\n\\pi(a_1) = \\frac{e^{H(a_1)}}{e^{H(a_1)} + e^{H(a_2)}} = \\frac{e^{H(a_1)-H(a_2)}}{e^{H(a_1)-H(a_2)} + 1} = \\sigma(\\Delta)\n\\] and similarly \\[\n\\pi(a_2) = \\sigma(-\\Delta).\n\\]\n\n\n2.8.1 the bandit gradient algorithm as stochastic gradient ascent\nThis next subsection is devoted to the (I imagine) infamous brown box (Sutton and Barto 2018, 38‚Äì40), which marks a significant leap in theoretical complexity. It shows how the update rule arises as a form of stochastic gradient ascent.\nWe‚Äôll retrace their steps as a self-contained argument and flag two subtle points: the role of the baseline and the randomness in the preference vector.\n\n1. quick recap of gradient ascent\nLet \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\) be a differentiable function. We want to produce points \\(\\mathbf{x}_0, \\mathbf{x}_1, \\dots\\) that maximise \\(f\\). Gradient ascent updates the current point \\(\\mathbf{x}^{(t)}\\) in the direction of the gradient: \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\; \\nabla f(\\mathbf{x})\\big|_{\\mathbf{x}^{(t)}}\n\\] where \\(\\alpha &gt; 0\\) is the step size.\nIn one dimension, this becomes: \\[\nx_i^{(t+1)} = x_i^{(t)} + \\alpha \\; \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\bigg|_{x_i^{(t)}}\n\\]\nIn stochastic gradient ascent, we aim to maximise the expected value of a random vector \\(\\mathbf{R}\\) (a vector whose values are random variables), whose distribution depends on a parameter vector \\(\\mathbf{x}\\). That is, the underlying probability space \\((\\Omega, \\mathrm{Pr}_{\\mathbf{x}})\\) is parameterised by \\(\\mathbf{x}\\). Here \\(f\\) is \\(\\mathbb{E}_{\\mathbf{x}}[\\mathbf{R}]\\) where we have explicitly indicated the dependence of the expected value on \\(\\mathbf{x}\\). So this is still a deterministic gradient ascent step‚Äîalthough the true gradient is unknown to the algorithm and must later be estimated via sampling: \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\cdot \\nabla \\mathbb{E}_{\\mathbf{x}}[\\mathbf{R}]\\big|_{\\mathbf{x}^{(t)}}\n\\]\nOur goal is to cast the gradient bandit update in this framework.\n\n\n2. setting up the problem\nIn the gradient bandit algorithm, the parameters we adjust are the action preferences \\(H_t‚Äã(a)\\). These determine the policy via the softmax distribution: \\[\n\\pi_H(a) = \\frac{e^{H(a)}}{\\sum_{b \\in \\mathcal{A}} e^{H(b)}}\n\\]\nThis shows how our parameter \\(H\\) determines the probability space: it determines the probability distribution for \\(A_t\\) \\(\\pi_{H_t}(a) := \\mathrm{Pr}_{H_t}(A_t = a)\\) the probabilities for rewards given an action are determined by the system and independent of the parameters \\(q_*(a) := \\mathbb{E}[R_t \\mid A_t = a]\\).\nWith this set-up, it is clear how \\(\\mathbb{E}_{H_t}[R_t]\\) is a function on \\(H_t\\) \\[\n\\begin{split}\n\\mathbb{E}_{H_t}[R_t] &= \\sum_{b} \\mathrm{Pr}_{H_t}(A_t = b) \\cdot \\mathbb{E}[R_t \\mid A_t = b] \\\\\n&= \\sum_{b} \\pi_{H_t}(b) q_*(b)\n\\end{split}\n\\] (if you are unsure about this, check Theorem¬†2.4).\nIn this context, each action \\(a\\in\\mathcal{A}\\) corresponds to a coordinate in our parameter vector \\(H\\), so the gradient update in one dimension \\(a \\in \\mathcal{A}\\) becomes: \\[\nH_{t+1}(a) = H_t(a) + \\alpha \\frac{\\partial \\mathbf{E}[R_t]}{\\partial H(a)}\\bigg|_{H_t(a)}\n\\tag{2.11}\\]\n\n\n3. calculating the gradient\nNow look at the row of the gradient for \\(a \\in \\mathcal{A}\\): \\[\n\\begin{split}\n\\frac{\\partial \\mathbb{E}_{H}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)} &=\n\\sum_{b} q_*(b) \\cdot \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)} \\\\\n&= \\sum_{b} (q_*(b) - B_t) \\cdot \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)}\n\\end{split}\n\\] We could add here any scalar (called the baseline) as \\(\\sum_b \\pi_H(b) = 1\\) and thus \\(\\sum_{b} \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Big|_{H'(a)} = 0\\). Note that for this argument to work \\(B_t\\) cannot depend on \\(b\\).\nTo simplify that further we use the softmax derivative, (which is derived in Sutton and Barto): \\[\n\\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)}\n= \\pi_{H_t}(b) (\\mathbb{I}_{a = b} - \\pi_{H_t}(a))\n\\]\nSo we have \\[\n\\begin{split}\n\\frac{\\partial \\mathbb{E}_{H}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)} &=\n\\sum_{b} (q_*(b) - B_t) \\cdot  (\\mathbb{I}_{a = b} - \\pi_{H_t}(a)) \\pi_{H_t}(b) \\\\\n&= \\mathbb{E}_{H_t}[(q_*(A_t)- B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})] \\\\\n&= \\mathbb{E}_{H_t}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})].\n\\end{split}\n\\] We get the second equality by applying the law of the unconscious statistician in reverse (Theorem¬†2.1), and yes the expression inside the expectation is all just a deterministic function of \\(A_t\\). In the final equality, we substituted \\(R_t\\) for \\(q_*(A_t)\\) using that \\(q_*(A_t) = \\mathbb{E}_{H_t}[R_t]\\), and the law of iterated expectations justifies the substitution under the outer expectation.\nBefore going to the next step, you might want to check the plausibility of \\[\n\\frac{\\partial \\mathbb{E}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)}\n= \\mathbb{E}_{H_t}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})].\n\\tag{2.12}\\] On the left we have basically a number, the value of the derivative at some point, and on the right we have an expected value with a parameter \\(B_t\\). So the parameter \\(B_t\\) must cancel out somehow. Which it does indeed, you can check this for yourself.\n\n\n4. one-step update and baseline trade-off\nBy plugging Equation¬†2.12 into Equation¬†2.11 we get the deterministic gradient ascent update: \\[\nH_{t+1}(a) = H_t(a) + \\alpha \\mathbb{E}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})]\n\\]\nReplacing the expectation by the single sample \\(A_t, R_t\\) yields the stochastic gradient update: \\[\nH_{t+1}(a) = H_t(a) + \\alpha  (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})\n\\tag{2.13}\\]\nTo get Equation¬†2.10 we have to substitute \\(\\bar{R}_t\\) for \\(B_t\\). Which requires some discussion first.\nSutton and Barto make clear that \\(\\bar{R}_t\\) depend on \\(R_t\\):\n\n\\(\\bar{R}_t\\) is the average of all the rewards up through and including time \\(t\\) (Sutton and Barto 2018, 37).\n\nIf we use that in Equation¬†2.13 for \\(B_t\\) we are not using an unbiased estimator anymore. This is tightly coupled with the fact that when we introduced \\(B_t\\) we required it not to depend on \\(b\\) which does later play the role of \\(A_t\\), and \\(\\bar{R}_t\\) depends on \\(R_t\\) which depends on \\(A_t\\).\nMostl likely there is a good reason for using \\(\\bar{R}_t\\), but I don‚Äôt know the mathematical motivation. (As I said, maybe some variance reduction)\nHowever I‚Äôm saying the derivation of their update formula is wrong1 as they frame it as an unbiased estimator.\n\n\ncomment on the time parameter\nWe have keept the time parameter in the derivation to stick to the style of Sutton and Barto. We could have equally done the same derivation for \\(H'\\) (new) and \\(H\\) (old). However, conceptually keeping the time parameter is a bit shaky. Where does the \\(H_t\\) come from? If we think this through then \\(H_t\\) actually becomes part of the whole system (enviroment + agent) and thus is a random vector. And then it‚Äôs harder for me to think about how to analyse this to obtain the update formula. Once the update rule is fixed, we can then treat the entire system (agent and environment) as stochastic without any problems.\nIf correct or not Sutton-Barto derive this term for the gradient \\[\n\\mathbb{E}\\big[ (R_t - \\bar{R}_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t}(a)) \\big].\n\\] Which might look a bit fishy because maybe \\(\\mathbb{E}\\big[ R_t - \\bar{R}_t]\\) is always \\(0\\). But it is usually not, because the policy \\(\\pi\\) is not stationary, it get‚Äôs updated at every step and thus the policy‚Äôs evolution decouples the two expectations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#associative-search-contextual-bandits",
    "href": "chapters/02-multi-armed-bandits.html#associative-search-contextual-bandits",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.9 Associative Search (Contextual Bandits)",
    "text": "2.9 Associative Search (Contextual Bandits)\n\nExercise 2.10 Suppose you face a \\(2\\)-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions \\(1\\) and \\(2\\) are respectively \\(0.1\\) and \\(0.2\\) with probability \\(0.5\\) (case A), and \\(0.9\\) and \\(0.8\\) with probability \\(0.5\\) (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don‚Äôt know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?\n\n\nSolution 2.10. We are presented with two scenarios and the questions ‚ÄúWhat is the best strategy?‚Äù and ‚ÄúWhat is its expected reward?‚Äù for each scenario.\nIn the first scenario, we don‚Äôt know whether we are facing Case A or Case B at any given time step. The true values of the actions are as follows: \\[\n\\begin{split}\n\\mathbb{E}[R_t \\mid A_t = 1] &= \\mathrm{Pr}(\\text{Case A}) \\cdot 0.1 + \\mathrm{Pr}(\\text{Case B}) \\cdot 0.9\\\\\n&= 0.5 (0.1 + 0.9) = 0.5\\\\[3ex]\n\\mathbb{E}[R_t \\mid A_t = 2] &= \\mathrm{Pr}(\\text{Case A}) \\cdot 0.2 + \\mathrm{Pr}(\\text{Case B}) \\cdot 0.8\\\\\n&= 0.5 (0.2 + 0.8) = 0.5\n\\end{split}\n\\]\nSince both actions have the same expected reward of 0.5, it does not matter which action is chosen. Thus, any algorithm is optimal and has an expected of 0.5.\nIn the second scenario, we know whether we are facing Case A or Case B. The expected reward under the optimal strategy, which always chooses the action with the highest expected value, is: \\[\n\\mathbb{E}_{\\pi_*}[ R_t ] = \\overbrace{0.5 \\cdot 0.2}^{\\text{case A}} + \\overbrace{0.5 \\cdot 0.9}^{\\text{case B}} = 0.55\n\\]\nTo achieve this expected reward, we need to keep track of the two bandit problems separately and maximise their rewards. How to do this approximately is the topic of the whole chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#summary",
    "href": "chapters/02-multi-armed-bandits.html#summary",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.10 Summary",
    "text": "2.10 Summary\nLet‚Äôs recreate the parameter study (Sutton and Barto 2018, fig. 2.6). We compare these four agents across different parameter values:\n\n\n\nTable¬†2.1: Overview Bandit Agents\n\n\n\n\n\n\n\n\n\n\n\nAgent\nAction selection\nUpdate rule\nParameter varied\n\n\n\n\nŒµ-greedy\nSelects actions Œµ-greedily with respect to the action-value estimates\nSample-average update of the action-value estimates\n\\(\\varepsilon\\)\n\n\nGradient bandit\nSamples actions from the softmax distribution (Eq. Equation¬†2.9) over preferences\nGradient bandit update of the preferences (Eq. Equation¬†2.10)\n\\(\\alpha\\)\n\n\nUCB\nSelects actions using the upper confidence bound criterion (Eq. Equation¬†2.8) applied to the action-value estimates\nSample-average update of the action-value estimates\n\\(c\\)\n\n\nOptimistic greedy (constant step size)\nSelects actions greedily with respect to the action-value estimates. Starts with optimistic estimate \\(Q_0\\)\nConstant step-size update of the action-value estimates (step size Œ±)\n\\(Q_0\\)\n\n\n\n\n\n\n\nCode\nimport pickle\nimport matplotlib.pyplot as plt\n\nwith open(\"results/parameter_study_stationary.pkl\", \"rb\") as f:\n    p = pickle.load(f)\n\nplot_params = [\n    (\"eps_sample\", \"eps\", r\"$\\varepsilon$-greedy: $\\varepsilon$\"),\n    (\"grad_sample\", \"alpha\", r\"Gradient Bandit: $\\alpha$\"),\n    (\"ucb_sample\", \"c\", r\"UCB: $c$\"),\n    (\"optimistic\", \"q_init\", r\"Optimistic greedy, $\\alpha=0.1$: $Q_0$\"),\n]\n\nfor group, parameter, label in plot_params:\n    performance = p.get(\"res\").get(group)\n    setting = [spec.get(parameter) for spec in p.get(\"args\").get(\"groups\").get(group)]\n    plt.plot(setting, performance, label=label)\n\nax = plt.gca()\nax.set_xscale(\"log\", base=2)\n\nn_episodes = p.get(\"args\").get(\"n_episodes\")\nn_runs = p.get(\"args\").get(\"n_runs\")\n\nplt.ylabel(f\"Average reward over first {n_episodes:,} steps\")\nplt.xlabel(r\"$\\varepsilon$, $\\alpha$, $c$, $Q_0$\")\nplt.title(f\"Parameter study (Averaged over {n_runs:,} runs)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.6: Performance comparison over multiple agents (see Table¬†2.1). For each agent we sweep over the relevant parameter and plot the average reward over the first 1000 steps. This looks near identical to the original figure from the book (except that here the parameter ranges are a bit bigger) (Sutton and Barto 2018, fig. 2.6)\n\n\n\nUCB achieves the highest overall average reward and has a remarkable high and stable plateau for small \\(c\\). Surprisingly, the optimistic greedy agent performs second best, though its success likely hinges on careful tuning of \\(Q_0\\)‚Äã‚Äîa process that may be highly sensitive to the number of steps. When \\(Q_0\\)‚Äã is set low (like \\(2^{-7} \\approx 0.008\\)), the agent effectively acts like a constant-step-size greedy approach, which has the worst performance.\nThe data for the parameter study was precomputed using /scripts/parameter_study/study_stationary.py, which heavily relies on NumPy‚Äôs vectorized array operations for performance. This approach avoids the overhead of slow Python loops. Using the pure Python implementation of the agents from this chapter would be significantly slower for this task.\n\nExercise 2.11 Make a figure analogous to Figure¬†2.6 for the nonstationary case outlined in Exercise¬†2.5. Include the constant-step-size \\(\\varepsilon\\)-greedy algorithm with \\(\\alpha\\)= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.\n\n\nSolution 2.11. That last exercise is a bit of a banger to finish, but I‚Äôm pleased to present the results of the parameter study. First the candidates agents and their parameters:\n\n\n\nTable¬†2.2: Overview of bandit agents used in the non-stationary setting\n\n\n\n\n\n\n\n\n\n\n\nAgent\nAction selection\nUpdate rule\nParameter varied\n\n\n\n\nSample mean Œµ-greedy\n\\(\\varepsilon\\)-greedily over action-value estimates\nSample-average\n\\(\\varepsilon\\)\n\n\nConstant-step-size Œµ-greedy\n\\(\\varepsilon\\)-greedily over action-value estimates\nConstant step-size (\\(\\alpha = 0.1\\))\n\\(\\varepsilon\\)\n\n\nSample mean UCB\nUCB criterion over action-value estimates\nSample-average\n\\(c\\)\n\n\nConstant-step-size UCB\nUCB criterion over action-value estimates\nConstant step-size (\\(\\alpha = 0.1\\))\n\\(c\\)\n\n\nGradient bandit\nSoftmax sampling over preferences\nGradient bandit update of preferences\n\\(\\alpha\\)\n\n\nConstant-step-size baseline Gradient bandit\nSoftmax sampling over preferences\nGradient bandit update of preferences and constant-step-size updates for baseline \\(B_t\\) (\\(\\alpha_B = 0.1\\))\n\\(\\alpha\\)\n\n\n\n\n\n\nA note on the last agent: In the standard gradient bandit algorithm, the baseline \\(B_t\\) is the sample average of all previous rewards. Here, we modify this by using a constant step-size update for the baseline instead.\nSo here is the parameter sweep: all initial action values \\(q_*(a)\\) start at 0 and evolve via independent random walks, with increments drawn from \\(N(0, 0.01)\\) at each step. Rewards are then sampled from \\(N(q_*(a), 1)\\).\n\n\nCode\nimport pickle\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nwith open(\"results/parameter_study_nonstationary.pkl\", \"rb\") as f:\n    p = pickle.load(f)\n\nplot_params = [\n    (\"eps_sample\", \"eps\", r\"$\\varepsilon$-greedy, sample mean: $\\varepsilon$\"),\n    (\"eps_const\", \"eps\", r\"$\\varepsilon$-greedy with $\\alpha=0.1$: $\\varepsilon$\"),\n    (\"ucb_sample\", \"c\", r\"UCB, sample mean: $c$\"),\n    (\"ucb_const\", \"c\", r\"UCB with $\\alpha=0.1$: $c$\"),\n    (\"grad_sample\", \"alpha\", r\"gradient bandit: $\\alpha$\"),\n    (\"grad_const\", \"alpha\", r\"gradient bandit with $\\alpha_B=0.1$: $\\alpha$\"),\n]\n\nplt.figure(figsize=(12, 6))\n\nfor group, parameter, label in plot_params:\n    performance = p.get(\"res\").get(group)\n    setting = [spec.get(parameter) for spec in p.get(\"args\").get(\"groups\").get(group)]\n    plt.plot(setting, performance, label=label)\n\nax = plt.gca()\n\nax.set_xscale(\"log\", base=2)\nax.xaxis.set_major_locator(ticker.LogLocator(base=2, numticks=20))\n\n\nn_episodes = p.get(\"args\").get(\"n_episodes\")\nkeep_last = p.get(\"args\").get(\"keep_last\")\nn_runs = p.get(\"args\").get(\"n_runs\")\n\nplt.ylabel(f\"Average reward of last {keep_last:,} steps of {n_episodes,} total\")\nplt.xlabel(r\"a\")\nplt.title(f\"Parameter study non-stationary setting (Averaged over {n_runs:,} runs)\")\nplt.legend(ncol=3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAnother surprise: \\(varepsilon\\)-greedy is the winner. This simple algorithm performs quite well in this study. UCB also does well, but its effective parameter range is quite narrow (and surprisingly high at around \\(c=2^7=128\\)). For the gradient bandit, the baseline update method doesn‚Äôt seem to matter much. However, what surprises me most is that the sample mean \\(\\varepsilon\\)-greedy performs as well as, or even better than, the gradient bandit, as it has a broader sweet spot. It appears that the gradient bandit isn‚Äôt exploring enough.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#sec-appendix-multi-arm-bandits",
    "href": "chapters/02-multi-armed-bandits.html#sec-appendix-multi-arm-bandits",
    "title": "2¬† Multi-armed Bandits",
    "section": "2.11 Appendix üîç",
    "text": "2.11 Appendix üîç\nHere are some more details on concepts that came up during this chapter.\n\n2.11.1 Distribution\nEvery random variable \\(X\\) has a distribution, denoted \\(p_X\\), which maps each possible value to its probability: \\[\np_X(x) := \\mathrm{Pr}(X = x).\n\\]\nOften, we say \\(X\\) is distributed according to \\(f\\) for a function \\(f \\colon \\mathcal{X} \\to [0,1]\\), which means that \\(f(x) = \\mathrm{Pr}(X = x)\\). We write this as: \\[\nX \\sim f.\n\\]\nTwo random variables \\(X\\) and \\(Y\\) have the same distribution if \\(p_X = p_Y\\).\nThe distribution of \\(X\\) turns \\((\\mathcal{X}, p_X)\\) into a probability space, where \\(p_X\\) is called the pushforward measure.\n\n\n2.11.2 Independent and Identically Distributed Random Variables\nA very important concept: IID. A collection of random variables \\(X_1, \\dots, X_n\\) is independent and identically distributed (IID) if all random variables have the same probability distribution, and all are mutually independent.\nFormally, this means:\n\n\\(\\mathrm{Pr}(X_i = x) = \\mathrm{Pr}(X_j = x)\\)\n\\(\\mathrm{Pr}(X_i = x,\\, X_j = x') = \\mathrm{Pr}(X_i = x) \\cdot \\mathrm{Pr}(X_j = x')\\)\n\nfor all distinct indices \\(i \\neq j\\).\n\n\n2.11.3 Lotus\nThe following theorem is widely known as the ‚Äúlaw of the unconscious statistician‚Äù. It is fundamental in many calculations as it allows us to compute the expected value of functions of random variables by only knowing the distributions of the random variables.\n\nTheorem 2.1 Let \\(X \\colon \\Omega \\to \\mathcal{X}\\) be a random variable, and let \\(g \\colon \\mathcal{X} \\to \\mathbb{R}\\) be a real-valued function on the result space.\nThen the expected value of \\(g\\) with respect to the pushforward distribution \\(p_X\\) is the same as the expected value of the random variable \\(g(X) := g \\circ X\\) on \\(\\Omega\\): \\[\n\\mathbb{E}[g(X)] = \\sum_{x \\in \\mathcal{X}} g(x)\\, p_X(x)\n\\]\n\n\nProof. Pretty sure this proof could be beautifully visualised: summing over columns is the same as summing over rows. But indicator functions \\(\\mathbb{I}\\) do the trick too.\n\\[\n\\begin{split}\n\\sum_{x \\in \\mathcal{X}} g(x) p_X(x)\n&= \\sum_{x \\in \\mathcal{X}} g(x) \\mathrm{Pr}(X = x) \\\\\n&= \\sum_{x \\in \\mathcal{X}} g(x) \\left( \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) \\mathbb{I}_{X = x} \\right) \\\\\n&= \\sum_{x \\in \\mathcal{X}, \\omega \\in \\Omega} g(x) \\mathrm{Pr}(\\omega) \\mathbb{I}_{X = x} \\\\\n&= \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) \\left( \\sum_{x \\in \\mathcal{X}} g(x) \\mathbb{I}_{X = x} \\right) \\\\\n&= \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) g(X) \\\\\n&= \\mathbb{E}[g(X)]\n\\end{split}\n\\]\n\n\n\n2.11.4 Multiplication Rule of Conditional Probabilities\nThe multiplication rule of conditional probabilities is great for manipulating unknown distributions into known distributions.\n\nTheorem 2.2 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R}\\) be random variables. Then \\[\n\\mathrm{Pr}[R = r, A = a] =  \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\] for \\(a \\in \\mathcal{A}\\) and \\(r \\in \\mathcal{R}\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}[R = r, A = a]\n&= \\mathrm{Pr}(A = a) \\frac{\\mathrm{Pr}[R = r, A = a]}{\\mathrm{Pr}(A = a)} \\\\\n&= \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\end{split}\n\\]\n\n\nTheorem 2.3 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R}\\) be random variables. Then \\[\n\\mathrm{Pr}[R =r] = \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\] for \\(r \\in \\mathcal{R}\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}[R =r]\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}[R = r, A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\end{split}\n\\]\n\n\nTheorem 2.4 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R} \\subseteq \\mathbb{R}\\) be random variables. Then \\[\n\\mathbb{E}[R] = \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathbb{E}[R \\mid A = a]\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathbb{E}[R]\n&= \\sum_{r \\in \\mathcal{R}} r \\mathrm{Pr}(R = r) \\\\\n&= \\sum_{r \\in \\mathcal{R}} r \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\sum_{r \\in \\mathcal{R}} r \\mathrm{Pr}[R = r \\mid A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathbb{E}[R \\mid A = a]\n\\end{split}\n\\]\n\n\n\n2.11.5 Variance\nThe variance \\(\\mathrm{Var}(X)\\) of a random variable is defined as: \\[\n\\mathrm{Var}(X) := \\mathbb{E}[(X-\\mu)^2]\n\\]\nwhere \\(\\mu = \\mathbb{E}[X]\\) is the mean of \\(X\\).\nIt can be easily shown that \\[\n\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2.\n\\]\n\n\n2.11.6 Independent Variables\nTwo random variables \\(X,Y\\) are independent if \\[\n\\mathrm{Pr}(X = x, Y = y) = \\mathrm{Pr}(X = x) \\cdot \\mathrm{Pr}(Y = y).\n\\]\nIn this case, the conditioned probabilities are equal to the ordinary probabilities\n\nLemma 2.1 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathrm{Pr}(X = x \\mid Y = y) = \\mathrm{Pr}(X = x).\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}(X = x \\mid Y = y) &= \\frac{\\mathrm{Pr}(X = x, Y = y)}{\\mathrm{Pr}(Y = y)} \\\\\n&= \\frac{\\mathrm{Pr}(X = x) \\mathrm{Pr}(Y = y)}{\\mathrm{Pr}(Y = y)} \\\\\n&= \\mathrm{Pr}(X = x)\n\\end{split}\n\\]\n\n\nLemma 2.2 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathbb{E}(XY) = \\mathbb{E}(X) \\cdot \\mathbb{E}(Y)\n\\]\n\n\nProof. We are cheating a bit here (but not doing anything wrong) and apply LOTUS on two random variables at once. \\[\n\\begin{split}\n\\mathbb{E}[XY] &= \\sum_{x,y} x\\cdot y \\; \\mathrm{Pr}(X = x, Y = y) \\\\\n&= \\left(\\sum_{x} x \\mathrm{Pr}(X = x)\\right) \\cdot \\left(\\sum_{y} y \\mathrm{Pr}(Y = y)\\right) \\\\\n&= \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]\n\\end{split}\n\\]\n\nWe can use this lemma to prove ‚Äúlinearity‚Äù for independent variables.\n\nLemma 2.3 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Var}(X + Y) &= \\mathbb{E}[(X+Y)^2] - \\mathbb{E}[X+Y]^2 \\\\\n&= (\\mathbb{E}[X^2] + 2 \\mathbb{E}[XY] + \\mathbb{E}[Y^2]) - (\\mathbb{E}[X]^2 + 2 \\mathbb{E}[X]\\mathbb{E}[Y] + \\mathbb{E}[Y]^2) \\\\\n&= (\\mathbb{E}[X^2] - \\mathbb{E}[X]^2) + (\\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2) \\\\\n&= \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n\\end{split}\n\\]\n\n\nTheorem 2.5 The population mean \\(\\bar{X}_n\\) of IID real-valued random variables \\(X_1, \\dots, X_n\\) has variance \\[\n\\mathrm{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of the \\(X_i\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Var}(\\bar{X}_n) &= \\mathrm{Var}(\\frac{1}{n}\\sum_{i=1}^n X_i) \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n \\mathrm{Var}(X_i)\\\\\n&= \\frac{1}{n^2} n \\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\n\n\n\n2.11.7 Estimators\nEstimators are functions used to infer the value of a hidden parameter from observed data.\nI don‚Äôt want to create too much theory for estimators. Let‚Äôs look at the \\(Q_n\\) and \\(R_i\\) from Section 2.4.\nThe \\(Q_n\\) are somehow based on the \\(R_1, \\dots, R_{n-1}\\) and called estimators for \\(q_*\\).\nThere are some common metrics for determining the quality of an estimator.\n\nBias\nThe bias of \\(Q_n\\) is \\[\n\\mathrm{Bias}(Q_n) = \\mathbb{E}[Q_n] - q_*.\n\\]\nIf this is \\(0\\) then \\(Q_n\\) is unbiased. If the bias disappears asymptotically, then \\(Q_n\\)‚Äã is asymptotically unbiased.\n\n\nmean squared error\nIt is used to indicate how far, on average, the collection of estimates are from the single parameter being estimated. \\[\n\\mathrm{MSE}(Q_n) = \\mathbb{E}[ (Q_n - q_*)^2]\n\\]\nThe mean squared error can be expressed in terms of bias and variance. \\[\n\\mathrm{MSE}(Q_n) = \\mathrm{Var}(Q_n) + \\mathrm{Bias}(Q_n)^2\n\\]\nIn particular, for unbiased estimators, the mean squared error is just the variance.\n\nLemma 2.4 Let \\(Q_n\\) be the sample average of \\(R_1, \\dots, R_n\\). Then \\[\n\\mathrm{MSE}(Q_n) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of the \\(X_i\\).\n\n\nProof. The sample average is unbiased. Thus, its mean squared error is its variance given in Theorem¬†2.5.\n\n\nLemma 2.5 Let \\(Q_n\\) be the constant step size weighted average of the \\(R_1, \\dots, R_n\\). Then, the Mean Squared Error of \\(Q_{n+1}\\) is given by: \\[\n\\mathrm{MSE}(Q_{n+1}) = \\sigma^2 \\frac{\\alpha}{2-\\alpha} + (1 - \\alpha)^{2n} [(Q_n - q_*)^2 - \\sigma^2\\frac{\\alpha}{2-\\alpha}],\n\\] where \\(\\sigma^2\\) is the variance of the \\(R_i\\).\nIn particular, the MSE is bounded from below by \\[\n\\lim_{n\\to\\infty}\\mathrm{MSE}(Q_n) = \\sigma^2 \\frac{\\alpha}{2-\\alpha}.\n\\]\n\n\nProof. The weighted average \\(Q_{n+1}\\)‚Äã is defined as: \\[\nQ_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i\n\\]\n\nstep 1: compute the expected value\nThis has been done in Equation¬†2.5 \\[\n\\mathbb{E}(Q_{n+1}) = (1-\\alpha)Q_1 + (1 - (1-\\alpha)^n)q_*\n\\]\n\n\nstep 2: compute the bias of\nUsing \\(\\mathrm{Bias}(Q_{n+1}) = \\mathbb{E}[Q_{n+1}] - q_*\\) we get \\[\n\\begin{split}\n\\mathrm{Bias}(Q_{n+1}) &= (1-\\alpha)Q_1 + (1 - (1-\\alpha)^n)q_* - q_* \\\\\n&= (1-\\alpha)^n [Q_n - q_*].\n\\end{split}\n\\]\n\n\nstep 3: compute the variance\n\\[\n\\begin{split}\n\\mathrm{Var}(Q_{n+1}) &= \\mathrm{Var}\\big((1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i \\big) \\\\\n&= \\sum_{i=1}^n \\alpha^2 \\big((1-\\alpha)^{2}\\big)^{n-i} \\sigma^2 \\\\\n&= \\sigma^2\\alpha^2 \\sum_{i=0}^{n-1} \\big((1-\\alpha)^{2}\\big)^{i} \\\\\n&= \\sigma^2\\alpha^2 \\frac{1 - (1-\\alpha)^{2n}}{1 - (1-\\alpha)^2} \\\\\n&= \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\big(1 - (1-\\alpha)^{2n}\\big).\n\\end{split}\n\\] Here it‚Äôs crucial that the \\(R_i\\) are independent.\n\n\n2.11.7.0.1 step 4: compute the mean squared error\nNow we can use \\(\\mathrm{MSE}(Q_{n+1}) = \\mathrm{Var}(Q_{n+1}) + \\mathrm{Bias}(Q_{n+1})^2\\) \\[\n\\begin{split}\n\\mathrm{MSE}(Q_{n+1}) &= \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\big(1 - (1-\\alpha)^{2n}\\big) + \\Big( (1-\\alpha)^n [Q_n - q_*] \\Big)^2 \\\\\n&= \\sigma^2 \\frac{\\alpha}{2-\\alpha} + (1 - \\alpha)^{2n} [(Q_n - q_*)^2 - \\sigma^2\\frac{\\alpha}{2-\\alpha}]\n\\end{split}\n\\]\n\n\n\n\n\n2.11.8 Geometric Series\nIn the context of reinforcement learning, the concept of discounting naturally requires the notion of geometric series. This series is defined as, \\[\nS(n+1) := \\sum_{i=0}^n \\gamma^i,\n\\]\nwhere \\(\\gamma \\in \\mathbb{R}\\). By convention, an empty sum is considered to be 0, thus \\(S(0)=0\\).\nIf \\(\\gamma = 1\\), then the geometric series simplifies to \\(S(n+1) = n+1\\). So let‚Äôs assume \\(\\gamma \\neq 1\\) from now on.\nBy pulling out the term for \\(i=0\\) and factoring out a \\(\\gamma\\), we can derive a recurrence relation for the geometric series \\[\nS(n+1) = 1 + \\gamma S(n) \\quad \\text{and} \\quad S(0) = 0\n\\tag{2.14}\\]\nWhen we even add a clever \\(0 = \\gamma^{n+1} - \\gamma^{n+1}\\), we get this equation for \\(S(n)\\) \\[\nS(n) = (1 - \\gamma^{n+1}) + \\gamma S(n).\n\\]\nFrom this, we can deduce the closed-form expression for the geometric series: \\[\n\\sum_{i=0}^n \\gamma^i  = \\frac{1 - \\gamma^{n+1}}{1 - \\gamma}\n\\tag{2.15}\\]\nBy omitting the first term (starting from \\(i = 1\\)), we obtain: \\[\n\\sum_{i=1}^n \\gamma^i =  \\frac{\\gamma - \\gamma^{n+1}}{1 - \\gamma}\n\\tag{2.16}\\]\nThe infinite geometric series converges, if and only if, \\(|\\gamma| &lt; 1\\). Using the previous formulas, we can derive their limits: \\[\n\\sum_{i=0}^\\infty \\gamma^i = \\frac{1}{1-\\gamma}\n\\tag{2.17}\\] \\[\n\\sum_{i=1}^\\infty \\gamma^i = \\frac{\\gamma}{1-\\gamma}\n\\tag{2.18}\\]\n\n\n2.11.9 Recurrence Relations\nAs it turns out, the basic geometric series we‚Äôve explored isn‚Äôt quite enough to handle discounting and cumulative discounted returns in reinforcement learning. While the geometric series solves the homogeneous linear recurrence relation given by Equation¬†2.14, dealing with cumulative discounted returns introduces a non-homogeneous variation, where the constants 1s are replaced by a some \\(r_i\\)‚Äã, leading to the recurrence relation: \\[\nQ(n+1) := r_n + \\gamma Q(n) \\quad \\text{and} \\quad Q(0) := 0\n\\]\n\nTheorem 2.6 The function \\[\nQ(n+1) = \\sum_{i=0}^n \\gamma^{n-i} r_i.\n\\] is the solution to the recurrence relation \\[\nQ(n+1) := r_n + \\gamma Q(n) \\quad \\text{and} \\quad Q(0) := 0\n\\]\n\n\nProof. It‚Äôs easy to verify that this fulfils the recursive definition: \\[\n\\begin{split}\nQ(n+1) &= r_n + \\sum_{i=0}^{n-1} \\gamma^{n-i} r_i \\\\\n&= r_n + \\gamma\\sum_{i=0}^{n-1} \\gamma^{n-1-i} r_{i} \\\\\n&= r_n + \\gamma Q(n).\n\\end{split}\n\\]\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#footnotes",
    "href": "chapters/02-multi-armed-bandits.html#footnotes",
    "title": "2¬† Multi-armed Bandits",
    "section": "",
    "text": "I‚Äôm saying this not very loudly. Maybe I‚Äôm somewhere wrong.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html",
    "href": "chapters/03-finite-markov-decision-processes.html",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "",
    "text": "3.1 The Agent‚ÄìEnvironment Interface\nWe are already anticipating Exercise¬†3.5 and will give the formulations for a Markov Decision Process (MDP) for continuing and episodic tasks.\nA continuing trajectory looks like this: \\[\nS_0, A_0, R_1, S_1, A_1, R_2, S_2 A_2, R_3, \\dots,\n\\] and an episodic trajectory looks like this: \\[\nS_0, A_0, R_1, S_1, A_1, \\dots R_{T-1}, S_{T-1}, A_{T-1}, R_T, S_T.\n\\] Note that the indexes of actions and rewards has changed from the previous chapter. Now, the reward for an action \\(A_t\\) is \\(R_{t+1}\\), not \\(R_t\\) as it was defined for the armed bandits.\nAn MDP is completely described by its dynamics: \\[\np(s', r |s,a) := \\mathrm{Pr}(S_t = s', R_t = r \\mid S_{t-1} = s, A_{t-1} = a)\n\\tag{3.1}\\]\ngiving the probability that, from state \\(s \\in \\mathcal{S}\\) under action \\(a \\in \\mathcal{A}(s)\\), the environment transitions to state \\(s' \\in \\mathcal{S}^+\\) (\\(\\mathcal{S}^+\\) denotes the state space with any possible terminal states) and gives reward \\(r \\in \\mathcal{R}\\).\nIn particular when \\(s\\) and \\(a\\) are fixed \\(p(s', r | s,a)\\) is a discrete probability density, i.e., \\[\np(\\cdot, \\cdot | s,a)\\colon \\mathcal{S}^+ \\times \\mathcal{R} \\to [0,1]\n\\] and \\[\n\\sum_{s' \\in \\mathcal{S}^+, r \\in \\mathcal{R}} p(s',r | s,a) = 1.\n\\tag{3.2}\\]\nI want to add some more words about MDP and other Markov Chains that will be important for us.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#sec-the-agent-environment-interface",
    "href": "chapters/03-finite-markov-decision-processes.html#sec-the-agent-environment-interface",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "",
    "text": "3.1.1 markov chains\nVery generally, Markov chains are processes modelled with sequences of random variables \\(X_1, X_2, \\dots\\), where the conditional probabilities have a finite horizon. We will describe Markov Chains with increasing complexity until we end up at MDPs.\n\nmarkov process (MP)\nMPs model systems that evolve randomly over discrete time steps. They are a sequence of random variables \\(S_0, S_1, \\dots\\), where \\(S_t\\) is the state of the system at time \\(t\\). In the past the system was in the states \\(S_0, \\dots, S_{t-1}\\) and its future is \\(S_{t+1}\\).\nThe defining property of a Markov chain is that the future is independent of the past given the present state of the process. This is expressed as: \\[\n\\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s, (S_{t'} = s_{t'})_{t' &lt; t}) = \\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s)\n\\]\nUsually we require the environment to be stationary, i.e., the transition probabilities are independent of \\(t\\): \\[\n\\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s) = \\mathrm{Pr}(S_{t'+1} = s' \\mid S_t' = s)\n\\]\nSo, in our case a Markov Process1 is completely described by\n\nstate space \\(\\mathcal{S}\\) and\ntransition probabilities: \\(p(s' | s) := P(S_{t+1}=s‚Ä≤‚à£ S_t=s)\\).\n\n\n\nmarkov reward process (MRP)\nA Markov Reward Process adds a reward structure to a Markov Process. What are we rewarded for? Simply for observing the process diligently and keeping our feet still as there is no interaction with the environment yet.\nHere, we have a sequence of random variables \\(R_0, S_0, R_1, S_1, R_2, \\dots\\). Basically it‚Äôs a sequence of random vectors \\((R_i, S_i)\\), where \\(S_i\\) tracks the state and the \\(R_i\\) give us some numerical information about the system. (Sutton and Barto usually omit the 0-th reward, which occurs before anything really happens‚Äîessentially a reward for starting the environment. It doesn‚Äôt change much, of course, but I like the symmetry it brings.)\nA Markov reward process (MRP) is therefore specified by:\n\nfinite state space \\(\\mathcal{S}\\)\nfinite reward space \\(\\mathcal{R} \\subseteq \\mathbb{R}\\)\n\\(p(s', r | s) := \\mathrm{Pr}(S_{t+1}=s', R_{t+1} = r‚à£ S_t = s)\\).\n\nHere \\(p(\\cdot, \\cdot | s)\\) is a probability measure on the product space \\(\\mathcal{S} \\times \\mathcal{R}\\), in particular \\(\\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s',r|s) = 1\\)\n\n\nmarkov decision process (MDP)\nNow we add interaction to the environment.\nThe trajectory looks like this: \\[\nR_0, S_0, A_0, R_1, S_1, A_1, \\dots ,\n\\] where \\(R_i\\) take values in the reward space \\(\\mathcal{R}\\), \\(S_i\\) values in the state space \\(\\mathcal{S}\\), and \\(A_i\\) in the action space \\(\\mathcal{A}\\).\nThe full dynamic of this process is an interwoven interaction between environment and agent. It looks a bit like this: \\[\n(R_0, S_0) \\overset{\\text{agent}}{\\to} A_0  \\overset{\\text{env}}{\\to}(R_1, S_1) \\overset{\\text{agent}}{\\to}  A_1 \\overset{\\text{env}}{\\to} \\dots\n\\] So, going from \\(S_t, A_t\\) to the next state-reward pair is given by the environment \\[\np(s', r | s, a) := \\mathrm{Pr}(S_{t+1}=s', R_{t+1} = r‚à£ S_t = s, A_t = a).\n\\] and going from a state reward pair to an action is is given by the agent \\[\n\\pi_t(a|s) = \\mathrm{Pr}(A_t = a | S_t = s).\n\\] If the agent is stationary, we can drop the \\(t\\). \\[\n\\pi(a|s) = \\mathrm{Pr}(A_t = a | S_t = s)\n\\]\n\nExercise 3.1 Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n\n\nSolution 3.1. TBD\n\n\nExercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n\n\nSolution 3.2. No, I can‚Äôt think of any clear exceptions. There‚Äôs only the challenge of how to model MDP for goals that we don‚Äôt know how to specify properly in the reward signal, e.g., human happiness. I can‚Äôt come up with a reward signal that wouldn‚Äôt be vulnerable to reward hacking, like ‚Äúnumber pressed by user on screen‚Äù, ‚Äútime smiling‚Äù, ‚Äúendorphins level in brain‚Äù.\n\n\nExercise 3.3 Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out‚Äîsay, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in‚Äîsay, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n\n\nSolution 3.3. TBD\n\n\nExercise 3.4 Give a table analogous to that in Example 3.3, but for \\(p(s' , r |s, a)\\). It should have columns for \\(s, a, s' , r\\) and \\(p(s' , r |s, a)\\), and a row for every 4-tuple for which \\(p(s', r |s, a) &gt; 0\\).\n\n\nSolution 3.4. \n\n\n\ns\na\ns‚Äô\nr\np(s‚Äô,r | s,a)\n\n\n\n\nhigh\nwait\nhigh\nr_wait\n1\n\n\nhigh\nsearch\nhigh\nr_search\nŒ±\n\n\nhigh\nsearch\nlow\nr_search\n1 - Œ±\n\n\nlow\nwait\nlow\nr_wait\n1\n\n\nlow\nsearch\nlow\nr_search\nŒ≤\n\n\nlow\nsearch\nhigh\n-3\n1 - Œ≤\n\n\nlow\nrecharge\nhigh\n0\n1",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#goals-and-rewards",
    "href": "chapters/03-finite-markov-decision-processes.html#goals-and-rewards",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.2 Goals and Rewards",
    "text": "3.2 Goals and Rewards",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#returns-and-episodes",
    "href": "chapters/03-finite-markov-decision-processes.html#returns-and-episodes",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.3 Returns and Episodes",
    "text": "3.3 Returns and Episodes\nThe expected (discounted) return is defined as: \\[\nG_t := \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}.\n\\tag{3.3}\\] For episodic tasks we have the convention that \\(R_{t} = 0\\) when \\(t &gt; T\\), and thus, in particular, \\(G_T = 0\\).\nIn an undiscounted, episodic task, this becomes \\[\nG_t := \\sum_{k=0}^{T-t-1}  R_{t+k+1}\n\\tag{3.4}\\]\nAnd as for the armed bandit this recurrence relation tying immediate reward with future return will be used often: \\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\tag{3.5}\\]\n\nExercise 3.5 The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of Equation¬†3.2\n\n\nSolution 3.5. We already described Section 3.1 for continuing and episodic tasks. So, Equation¬†3.2 is already in the right form.\n\n\nExercise 3.6 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n\n\nSolution 3.6. The reward at time \\(t\\) would be \\[\nG_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t + k+1} =  -\\gamma^{T - t - 1},\n\\] where \\(T\\) is the length of that episode.\nIn the continuing formulation there can be multiple failures in the future so the return is of the form \\(-\\gamma^{K_1} - \\gamma^{K_2} - \\dots\\). Here there can always just be one failure.\n\n\nExercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes‚Äîthe successive runs through the maze‚Äîso you decide to treat it as an episodic task, where the goal is to maximize expected total reward Equation¬†3.4. After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n\n\nSolution 3.7. In this setup the reward basically says: ‚ÄúFinish the maze eventually.‚Äù. So when the robot has learned to finish a maze somehow, it can‚Äôt perform better regarding this reward.\n\n\nExercise 3.8 Suppose \\(\\gamma = 0.5\\) and the following sequence of rewards is received \\(R_1 = 1, R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2\\), with \\(T = 5\\). What are \\(G_0, G_1 \\dots, G_5\\)? Hint: Work backwards.\n\n\nSolution 3.8. We can use the recurrence relation Equation¬†3.5 for the reward: $\n\n\n\nt\n\\(R_{t+1}\\)\n\\(\\gamma G_{t+1}\\)\n\\(G_t\\)\n\n\n\n\n5\n0\n0\n0\n\n\n4\n2\n0\n2\n\n\n3\n3\n1\n4\n\n\n2\n6\n2\n8\n\n\n1\n2\n8\n10\n\n\n0\n1\n5\n6\n\n\n\nWe recall that, by convention, \\(R_t := 0\\) for \\(t &gt; T\\)\n\n\nExercise 3.9 Suppose \\(\\gamma = 0.9\\) and the reward sequence is \\(R_1 = 2\\) followed by an infinite sequence of 7s. What are \\(G_1\\) and \\(G_0\\)?\n\n\nSolution 3.9. \\[\nG_1 = \\sum_{k=0}^\\infty 0.9^k R_{2+k} = 7 \\sum_{k=0}^\\infty 0.9^k = 7 / 0.1 = 70\n\\] and \\[\nG_0 = R_1 + \\gamma G_1 =  2 + 0.9 \\cdot G_1 = 2 + 0.9 \\cdot 70 = 65\n\\]\n\n\nExercise 3.10 Prove the second equality in (3.10).\n\n\nSolution 3.10. See Section 2.11.8 for a proof.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#unified-notation-for-episodic-and-continuing-tasks",
    "href": "chapters/03-finite-markov-decision-processes.html#unified-notation-for-episodic-and-continuing-tasks",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.4 Unified Notation for Episodic and Continuing Tasks",
    "text": "3.4 Unified Notation for Episodic and Continuing Tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#policies-and-value-functions",
    "href": "chapters/03-finite-markov-decision-processes.html#policies-and-value-functions",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.5 Policies and Value Functions",
    "text": "3.5 Policies and Value Functions\nThe policy distribution, together with the MDP dynamics, completely specify the distribution over trajectories. We write \\(\\mathrm{Pr}_\\pi\\) and \\(\\mathbb{E}_\\pi\\) to indicate which policy is used.\nWe want to evaluate stationary policies \\(\\pi\\). For example, something like \\(\\mathbb{E}_\\pi[R_{t+1} \\mid S_t = s]\\).\n\nExercise 3.11 If the current state is \\(S_t\\), and actions are selected according to the stochastic policy \\(\\pi\\), then what is the expectation of \\(R_{t+1}\\) in terms of \\(\\pi\\) and the four-argument function \\(p\\) (Equation¬†3.1)?\n\n\nSolution 3.11. It‚Äôs clearer to rephrase the exercise as ‚Äúgiven that the current state \\(S_t\\) is \\(s\\)‚Äù. So, let‚Äôs proceed with that.\nWe‚Äôll solve this two ways: first, intuitively; then, using our theory machine.\nIntuitively, when \\(S_t = s\\) then we know that \\(A_t\\) is distributed according to \\(\\pi(\\cdot | s)\\) and then from \\(S_t\\) and \\(A_t\\) we can get the next \\(S_{t+1}, R_{t+1}\\) via the MDP dynamics measure. So let‚Äôs put this together. The agent selects \\(a\\) with probability \\(\\pi(a \\mid s)\\), and then the environment transitions to \\((s', r)\\) with probability \\(p(s', r | s,a)\\). We don‚Äôt care about the \\(s'\\) right now. So, we get reward \\(r\\) with probability \\(\\sum_{s'} p(s',r | s,a)\\). Thus we have \\[\n\\mathbb{E}_{\\pi}[R_{t+1} \\mid S_t = s]\n= \\sum_{a} \\pi(a|s) \\sum_{r} r \\left(\\sum_{s'} p(s',r | s,a)\\right)\n\\] Or in a nicer format \\[\n\\mathbb{E}_{\\pi}[R_{t+1} \\mid S_t = s]\n= \\sum_{a} \\pi(a|s) \\sum_{r,s'} r \\; p(s',r | s,a)\n\\]\nNow let us derive this using LOTUS (Theorem¬†2.1) the law of total expectation (Theorem¬†2.3) \\[\n\\begin{split}\n\\mathbb{E}_{\\pi}&[R_{t+1} \\mid S_t = s] =\n\\sum_{r} r \\; \\mathrm{Pr}_{\\pi}[R_{t+1} = r \\mid S_t = s] \\\\\n&= \\sum_r r \\sum_{a, s'} \\mathrm{Pr}_{\\pi}[R_{t+1} = r, S_{t+1} = s' \\mid A_t = a, S_t = s] \\mathrm{Pr}_{\\pi}[A_t = a \\mid S_t = s] \\\\\n&=  \\sum_{r,a,s'} p(s',r | a,s) \\pi(a|s)\n\\end{split}\n\\]\n\nThe value functions quantify how desirable it is to be in a given state (or to take a given action in a state) under a policy:\n\nstate value function: \\[\nv_\\pi(s) := \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]\n\\tag{3.6}\\]\naction-value function: \\[\nq_{\\pi}(s,a) := \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]\n\\tag{3.7}\\]\n\n\nExercise 3.12 Give an equation for \\(v_\\pi\\) in terms of \\(q_\\pi\\) and \\(\\pi\\).\n\n\nSolution 3.12. The quick and easy answer is: \\[\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n\\tag{3.8}\\]\n\n\nExercise 3.13 Give an equation for \\(q_\\pi\\) in terms of \\(v_\\pi\\) and the four-argument \\(p\\)\n\n\nSolution 3.13. Again quick and easy: \\[\nq_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n\\tag{3.9}\\]\n\nWe can get the Bellman equations for \\(v_\\pi\\) and \\(q_\\pi\\) by writing Equation¬†3.6 and Equation¬†3.7 in their ‚Äúone-step look-ahead‚Äù form \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s]\n\\tag{3.10}\\] \\[\nq_\\pi(s,a) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a]\n\\tag{3.11}\\]\nWe then write down the expectations with the MDP‚Äôs transition dynamics and the policy‚Äôs action probabilities: \\[\nv_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_\\pi(s')]\n\\tag{3.12}\\] \\[\nq_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma \\sum_{a'}\\pi(a'|s')q_{\\pi}(s'|a')]\n\\tag{3.13}\\]\nEquivalently, one can derive these by combining Equation¬†3.8 with Equation¬†3.9.\nThis is typically stated as a recursive relationship between the state‚Äôs value and the next state‚Äôs value. (I wouldn‚Äôt call this ‚Äòrecursive‚Äô myself, especially in continuing tasks where there‚Äôs no base case)\nWhat I found helpful for understanding was writing the Bellman equations as a system of linear equations. We see the value function as a vector \\(v_\\pi\\) (indexed by \\(\\mathcal{S}\\)) and then we can write the Bellman equations like: \\[\n\\mathbf{v}_\\pi = \\mathbf{r}_{\\pi} + \\gamma \\mathbf{P}_\\pi \\mathbf{v}_\\pi,\n\\]\nwhere \\(\\mathbf{r}_\\pi\\) is the expected reward, and \\(\\mathbf{P}_\\pi\\) is the transition matrix under policy \\(\\pi\\). Specifically, \\[\n(\\mathbf{R}_\\pi)_s = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)r\n\\]\nand \\[\n(\\mathbf{P}_\\pi)_{s,s'} = \\sum_{a} \\pi(a|s) \\sum_{r} p(s',r|s,a).\n\\]\nOne can check that the Bellman equation for \\(v_\\pi\\) (Equation¬†3.12) is indeed one row of this vector equation. For episodic tasks, we use the convention that that \\((\\mathbf{R}_\\pi)_s = 0\\), \\((\\mathbf{P}_\\pi)_{s,s} =1\\), and \\((\\mathbf{v}_\\pi)_{s} = 0\\) for terminal states \\(s\\).\n\nExample 3.1 This is (Sutton and Barto 2018, example 3.5: Gridworld).\nFirst, Apologies for this ugly gridworld diagram below - this was the best I could manage with Graphviz.\nHere‚Äôs a quick recap of the gridworld setup. The states are the cells in an \\(5 \\times 5\\)-grid. The actions are up, down, left, right each normally with a reward of +0. When bumping into a wall the position does not change and the reward is -1. When moving any direction from A or B the reward is +10 respectively +5 and the agent gets beamed to A‚Äô respectively B‚Äô.\n\n\n\n\n\n\n\nG\n\n\n\ngrid\n\n\nA\n\n ¬†\n\nB\n\n ¬†\n\n ¬†\n\n\n\n\n\n ¬†\n\n\n\nB'\n\n\n ¬†\n\n\n\n\n\n\nA'\n\n\n\n\n\n\ngrid:01-&gt;grid:41\n\n\n+10\n\n\n\ngrid:03-&gt;grid:23\n\n\n+5\n\n\n\n\n\n\n\n\nLet‚Äôs solve the Bellman equation for the Gridworld in python. Here is some code that sets up the Gridworld class and a nice plotter for the value function. It‚Äôs good code but no need to read the fine print.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom enum import Enum\nfrom typing import Callable, Dict, Tuple\n\n\nclass Action(Enum):\n    UP = (-1, 0)\n    DOWN = (1, 0)\n    LEFT = (0, -1)\n    RIGHT = (0, 1)\n\n\nclass SpecialCell:\n    def __init__(self, reward: float, teleport: Tuple[int, int]):\n        self.reward = reward\n        self.teleport = teleport\n\n\nclass Gridworld:\n    \"\"\"\n    A simple Gridworld MDP with teleporting special cells.\n    \"\"\"\n\n    def __init__(\n        self,\n        size: int,\n        special_cells: Dict[Tuple[int, int], SpecialCell],\n        default_reward: float = 0.0,\n        boundary_penalty: float = -1.0,\n    ):\n        self.size = size\n        self.n_states = size * size\n        self.special_cells = special_cells\n        self.default_reward = default_reward\n        self.boundary_penalty = boundary_penalty\n        self.actions = list(Action)\n        self.n_actions = len(self.actions)\n\n    def state_index(self, position: Tuple[int, int]) -&gt; int:\n        i, j = position\n        return i * self.size + j\n\n    def index_state(self, idx: int) -&gt; Tuple[int, int]:\n        return divmod(idx, self.size)\n\n    def is_boundary(self, position: Tuple[int, int]) -&gt; bool:\n        i, j = position\n        return not (0 &lt;= i &lt; self.size and 0 &lt;= j &lt; self.size)\n\n    def expected_rewards(self, policy: Callable[[int, Action], float]) -&gt; np.ndarray:\n        \"\"\"\n        Compute expected immediate rewards R[s] under a given policy.\n        \"\"\"\n        R = np.zeros(self.n_states)\n        for s in range(self.n_states):\n            pos = self.index_state(s)\n            for action in self.actions:\n                prob = policy(s, action)\n                if pos in self.special_cells:\n                    reward = self.special_cells[pos].reward\n                else:\n                    new_pos = (pos[0] + action.value[0], pos[1] + action.value[1])\n                    reward = (\n                        self.boundary_penalty\n                        if self.is_boundary(new_pos)\n                        else self.default_reward\n                    )\n                R[s] += prob * reward\n        return R\n\n    def transition_matrix(self, policy: Callable[[int, Action], float]) -&gt; np.ndarray:\n        \"\"\"\n        Compute state-to-state transition probabilities P[s, s'] under a policy.\n        \"\"\"\n        P = np.zeros((self.n_states, self.n_states))\n        for s in range(self.n_states):\n            pos = self.index_state(s)\n            for action in self.actions:\n                prob = policy(s, action)\n                if pos in self.special_cells:\n                    new_pos = self.special_cells[pos].teleport\n                else:\n                    raw = (pos[0] + action.value[0], pos[1] + action.value[1])\n                    # clip to remain in grid\n                    new_pos = (\n                        min(max(raw[0], 0), self.size - 1),\n                        min(max(raw[1], 0), self.size - 1),\n                    )\n                s_prime = self.state_index(new_pos)\n                P[s, s_prime] += prob\n        return P\n\n\ndef plot_value_grid(\n    V: np.ndarray,\n    size: int,\n    cmap: str = \"viridis\",\n    fmt: str = \".2f\",\n    figsize: Tuple[int, int] = (6, 6),\n):\n    \"\"\"\n    Plot the state-value grid with annotations and a colorbar.\n    \"\"\"\n    V = V.reshape(size, size)\n\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Display heatmap\n    cax = ax.matshow(V, cmap=cmap, origin=\"upper\")\n\n    # Annotate each cell\n    for (i, j), val in np.ndenumerate(V):\n        ax.text(j, i, format(val, fmt), ha=\"center\", va=\"center\", fontsize=12)\n\n    # Configure ticks\n    ax.set_xticks(range(size))\n    ax.set_yticks(range(size))\n    ax.set_xlabel(\"Column\")\n    ax.set_ylabel(\"Row\")\n    ax.set_title(\"State-Value Function under Random Policy\")\n\n    # Add colorbar\n    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.show()\n\n\nTo solve for \\(\\mathbf{v}\\) we let python compute \\[\n\\mathbf{v} = (\\mathbf{I} - \\gamma \\mathbf{P})^{-1} \\mathbf{r}.\n\\]\n(I left the \\(\\pi\\) indices implicit here)\nSo here is the meat of this computation. We let numpy solve the bellmann equations for grid world.\n\n# === solving gridworld ===\n# setup parameters\ngrid_size = 5\nŒ≥ = 0.9\n\n# setup gridworld\nspecials = {\n    (0, 1): SpecialCell(reward=10, teleport=(4, 1)),\n    (0, 3): SpecialCell(reward=5, teleport=(2, 3)),\n}\nenv = Gridworld(size=grid_size, special_cells=specials)\n\n\n# setup random policy\ndef random_policy(_: int, __: Action) -&gt; float:\n    \"\"\"Uniform random policy over all actions.\"\"\"\n    return 1 / len(Action)\n\n\n# obtain variables of Bellman equation\nR = env.expected_rewards(random_policy)\nP = env.transition_matrix(random_policy)\n\n# --- solve the Bellman equation ---\nI = np.eye(grid_size * grid_size)\nv = np.linalg.solve(I - Œ≥ * P, R)\n\nplot_value_grid(v, grid_size)\n\n\n\n\n\n\n\nFigure¬†3.1: This is like the right part of Figure 3.2 (Sutton and Barto 2018): the state-value function for the equiprobable random policy\n\n\n\n\n\n\n\nExercise 3.14 The Bellman equation (Equation¬†3.12) must hold for each state for the value function \\(v_\\pi\\) shown in Figure¬†3.1 of Example¬†3.1. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n\n\nSolution 3.14. We‚Äôll use the numbers accurate to two decimal places. Basically we have to show that for the middle state \\(s\\) we have \\[\n\\begin{split}\nv_\\pi(s) &\\approx 0.9 \\cdot 0.25 \\cdot \\big( v_\\pi(s + (1,0)) + v_\\pi(s + (-1,0)) \\\\\n&+ v_\\pi(s + (0,1)) + v_\\pi(s + (0,-1)) \\big),\n\\end{split}\n\\] (Here, we‚Äôre using vector notation to denote the directions to neighbouring states.)\nIndeed this is true \\[\n0.9 \\cdot 0.25 \\cdot (2.25 + 0.36 + (-0.35) + 0.74) = 0.669\n\\] which is approximately \\(0.67\\)\n\n\nExercise 3.15 In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using Equation¬†3.3, that adding a constant \\(c\\) to all the rewards adds a constant, \\(v_c\\), to the values of all states, and thus does not affect the relative values of any states under any policies. What is \\(v_c\\) in terms of \\(c\\) and \\(\\gamma\\)?\n\n\nSolution 3.15. Adding a constant \\(c\\) to all rewards in a continuing task adds a constant \\[\nv_c = \\frac{c}{ 1 ‚àí \\gamma}\n\\] to the value of every state. This can be shown as follows. \\[\n\\begin{split}\nG_t &= \\sum_{k=0}^{\\infty} \\gamma^k (R_{t+k+1} + c) \\\\\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} + \\sum_{k=0}^\\infty \\gamma^kc\\\\\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} + \\frac{c}{1-\\gamma}\n\\end{split}\n\\]\nThus, the relative ordering of state values is preserved (it doesn‚Äôt change which states are better than others). However, value ratios do change.\nFor completeness, we can also verify this result using the vector form of the Bellman equation. Suppose \\(\\mathbf{v}\\) is the original value function, and \\(\\mathbf{r}\\) the original reward vector. After adding a constant \\(c\\) to every reward, the new reward vector is \\(r+c\\mathbf{1}\\), where \\(\\mathbf{1}\\) is the vector of all ones. The new value function is \\[\n\\begin{split}\n\\mathbf{v}' &= (\\mathbf{I} - \\gamma \\mathbf{P})^{-1} (\\mathbf{r} + c\\mathbf{1}) \\\\\n&= (\\mathbf{I} - \\gamma \\mathbf{P})^{-1} \\mathbf{r} + c(\\mathbf{I} - \\gamma \\mathbf{P})^{-1} \\mathbf{1} \\\\\n&= \\mathbf{v} + \\frac{c}{1 - \\gamma} \\mathbf{1},\n\\end{split}\n\\] since \\((\\mathbf{I} - \\gamma \\mathbf{P}) \\frac{1}{1-\\gamma}\\mathbf{1}\n= \\frac{1}{1-\\gamma}(\\mathbf{1} - \\gamma \\mathbf{1}) = \\mathbf{1}\\).\n\n\nExercise 3.16 Now consider adding a constant \\(c\\) to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n\n\nSolution 3.16. In an episodic task, adding a constant \\(c\\) to all rewards the task in a meaningful way. The additive term depends on both the state and the policy through the expected remaining time in the episode, so it can change the relative values of states and also reverse the ranking of policies.\nFormally, for the undiscounted case, the value function under a modified reward \\(R'_t = R_t + c\\) becomes:\n\\[\n\\begin{split}\nv'_{\\pi}(s) &= \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{T-t-1} (R_{t+k+1} + c) \\mid S_t = s \\right] \\\\\n&= v_{\\pi}(s) + c \\cdot \\mathbb{E}_{\\pi}[T - t \\mid S_t = s],\n\\end{split}\n\\] where \\(T\\) is the random variable for the time step at which the episode ends.\nWe consider the easiest maze in the world with two states: \\(S\\) and \\(T\\) (terminal). In state \\(S\\), there are two actions:\n\n‚Äúcontinue‚Äù: returns to \\(S\\) with reward \\(r\\)\n\n‚Äústop‚Äù: transitions to \\(T\\) with reward \\(0\\)\n\nIf \\(r=‚àí1\\), then policies that choose ‚Äústop‚Äù with highen probability perform better. However, if we add \\(c=2\\) to all rewards, then this reverses to selecting ‚Äústop‚Äù with lower probability.\n\n\nExercise 3.17 What is the Bellman equation for action values, that is, for \\(q_\\pi\\)? It must give the action values \\(q_\\pi(s,a)\\) in terms of the action values \\(q_\\pi(s'a')\\), of possible successors to the state-action pair \\((s,a)\\).\n\n\nSolution 3.17. This is just Equation¬†3.13.\n\n\nExercise 3.18 The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n\n\n\n\n\n\n\nPolicyDiagram\n\n\n\ns\n\ns\n\n\n\na1\n\n\n\n\ns-&gt;a1\n\n\na‚ÇÅ\n\n\n\na2\n\n\n\n\ns-&gt;a2\n\n\na‚ÇÇ\n\n\n\na3\n\n\n\n\ns-&gt;a3\n\n\na‚ÇÉ\n\n\n\nvpi\nv_œÄ(s)\n\n\n\ns-&gt;vpi\n\n\n\n\nqpi\nq_œÄ(s, a), has prob. œÄ(a|s)\n\n\n\na3-&gt;qpi\n\n\n\n\n\n\n\n\n\nGive the equation corresponding to this intuition and diagram for the value at the root node, \\(v_\\pi(s)\\), in terms of the value at the expected leaf node, \\(q_{\\pi}(s, a)\\), given \\(S_t = s\\). This equation should include an expectation conditioned on following the policy, \\(\\pi\\). Then give a second equation in which the expected value is written out xplicitly in terms of \\(\\pi(a|s)\\) such that no expected value notation appears in the equation.\n\n\nSolution 3.18. The hardest part of this exercise is to understand the problem description. And in the end isn‚Äôt this just Exercise¬†3.12 again? \\[\n\\begin{split}\nv_{\\pi}(s) &= \\mathbb{E}_{\\pi}[q_\\pi(s, A_t) \\mid S_t = s] \\\\\n&= \\sum_{a} \\pi(a|s)q_{\\pi}(s,a)\n\\end{split}\n\\]\n\n\nExercise 3.19 The value of an action, \\(q_{\\pi}(s, a)\\), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state‚Äìaction pair) and branching to the possible next states:\n\n\n\n\n\n\n\nmdp\n\n\n\nsa\n\ns, a\n\n\n\ns1\n\ns‚ÇÅ'\n\n\n\nsa-&gt;s1\n\n\nr‚ÇÅ\n\n\n\ns2\n\ns‚ÇÇ'\n\n\n\nsa-&gt;s2\n\n\nr‚ÇÇ\n\n\n\ns3\n\ns‚ÇÉ'\n\n\n\nsa-&gt;s3\n\n\nr‚ÇÉ\n\n\n\nq\nq_œÄ(s, a)\n\n\n\nq-&gt;sa\n\n\n\n\nv3\nv_œÄ(s')\n\n\n\nv3-&gt;s3\n\n\n\n\n\n\n\n\n\nGive the equation corresponding to this intuition and diagram for the action value, \\(q_{\\pi}(s, a)\\), in terms of the expected next reward, \\(R_{t+1}\\), and the expected next state value, \\(v_{\\pi}(S_{t+1})\\), given that \\(S_{t} = s\\)\n\n\nSolution 3.19. Ok, let‚Äôs do it again. \\[\n\\begin{split}\nq_{\\pi}(s,a) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s, A_t = a] \\\\\n&= \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]\n\\end{split}\n\\]\nNote, I have left out the subscript \\(\\pi\\) for the expectation as the transition \\((S_t,A_t)\\to (S_{t+1}, R_{t+1})\\) is independent of the agent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#optimal-policies-and-optimal-value-functions",
    "href": "chapters/03-finite-markov-decision-processes.html#optimal-policies-and-optimal-value-functions",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.6 Optimal Policies and Optimal Value Functions",
    "text": "3.6 Optimal Policies and Optimal Value Functions\nIn the next chapter (Section 4.2), we will see that any MDP has an optimal deterministic policy. We‚Äôll take this fact for granted for now and focus on their value functions.\nA policy is deterministic if \\(\\pi(a|s) = 1\\) for exactly one action for each \\(s \\in \\mathcal{S}\\). We then also just write \\(\\pi(s)\\) for this action.\nA policy \\(\\pi_*\\) is optimal if \\(\\pi_* \\geq \\pi\\) for any other \\(\\pi\\). Here we use \\(\\geq\\), a partial order on all policies for the MDP, defiend by \\[\n\\pi \\leq \\pi' \\quad:\\Longleftrightarrow\\quad v_\\pi(s) \\leq v_{\\pi'}(s) \\; \\text{, for all }s \\in \\mathcal{S}.\n\\]\nThere might be more than one optimal policy, but they must have the same value function (otherwise, they wouldn‚Äôt be optimal). We can define the optimal state-value function as \\[\nv_*(s) := \\max_{\\pi} v_{\\pi}(s)\n\\]\nand the optimal action-value function as \\[\nq_*(s,a) := \\max_{\\pi} q_\\pi(s,a).\n\\]\nThese definitions are well-defined by the existence of optimal policies.\nThe conversion formula from \\(q_*\\) to \\(v_*\\) takes a special simple form (we also prove this in the next chapter in Theorem¬†4.2): \\[\nv_*(s) = \\max_{a} q_\\pi(a,s)\n\\]\nCombining this with Equation¬†3.9, we get the Bellman optimality equations for the state value: \\[\nv_*(s) = \\max_{a} \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_*(s')]\n\\]\nand the state-action value: \\[\nq_*(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma \\max_{a'}q_*(s',a')]\n\\]\nIn principal, these Bellman optimality equations can be solved and having access to either \\(v_*\\) or \\(q_*\\) gives us direct access to an optimal \\(\\pi_*\\) (for \\(v_* \\to \\pi_*\\) we need to know the dynamics of the MDP though; see Exercise¬†3.28)\n\nExercise 3.20 Draw or describe the optimal state-value function for the golf example.\n\n\nSolution 3.20. I‚Äôm not 100% sure about this solution as I don‚Äôt like the golf example too much and as a result haven‚Äôt thought it completely through:\nUse driver when not on the green. Use putter on the green.\n\n\nExercise 3.21 Draw or describe the contours of the optimal action-value function for putting, \\(q_*(s, \\mathrm{putter})\\), for the golf example.\n\n\nSolution 3.21. I‚Äôm even less sure here.\nInside the -2 contour of \\(v_\\mathrm{putt}\\) it‚Äôs like \\(v_{\\mathrm{putt}}\\). In the sand it‚Äôs -3. Everywhere else it‚Äôs like the \\(q_*(s,\\mathrm{driver})\\) but shifted by a putter distance outwards and the value is one less.\n\n\nExercise 3.22 Consider the continuing MDP shown on to the right. The only decision to be made is that in the top state, where two actions are available, \\(\\mathrm{left}\\) and \\(\\mathrm{right}\\). The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, \\(\\pi_{\\mathrm{left}}\\) and \\(\\pi_{\\mathrm{right}}\\). What policy is optimal if \\(\\gamma = 0\\)? If \\(\\gamma = 0.9\\)? If \\(\\gamma = 0.5\\)?\n\n\nSolution 3.22. Here is a table indicating which policy is best in which case.\n\n\n\n\\(\\gamma\\)\n\\(v_{\\pi_{\\mathrm{left}}}\\)\n\\(v_{\\pi_{\\mathrm{right}}}\\)\n\n\n\n\n0\n1\n0\n\n\n0.9\n1\n1.8\n\n\n0.5\n0.5\n0.5\n\n\n\n\n\nExercise 3.23 Give the Bellman equation for \\(q_*\\) for the recycling robot.\n\n\nSolution 3.23. \n\n\nExercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.\n\n\nSolution 3.24. The solution is \\[\n\\begin{split}\nv_*(s) &= 10 + 0\\gamma + 0\\gamma^2 + 0\\gamma^3 + 0\\gamma^4 + 10\\gamma^5 + \\dots \\\\\n&= 10 \\sum_{i=0}^\\infty \\gamma^{5i} = \\frac{10}{1-\\gamma^5}\n\\end{split}\n\\] Which is for \\(\\gamma = 0.9\\) approximately \\(24.41943\\).\n\n\nExercise 3.25 Give an equation for \\(v_*\\) in terms of \\(q_*\\).\n\n\nSolution 3.25. \\[\nv_*(s) = \\max_a q_*(s,a)\n\\]\n\n\nExercise 3.26 Give an equation for \\(q_*\\) in terms of \\(v_*\\) and the four-argument \\(p\\).\n\n\nSolution 3.26. \\[\nq_*(s,a) = \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_*(s)]\n\\]\n\n\nExercise 3.27 Give an equation for \\(\\pi_*\\) in terms of \\(q_\\pi\\).\n\n\nSolution 3.27. \\[\n\\pi_*(s) = \\underset{a}{\\mathrm{argmax}} \\; q_*(s,a)\n\\]\n\n\nExercise 3.28 Give an equation for \\(\\pi_*\\) in terms of \\(v_*\\) and the four-argument \\(p\\).\n\n\nSolution 3.28. \\[\n\\pi_*(s) = \\underset{a}{\\mathrm{argmax}} \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_*(s)]\n\\]\n\n\nExercise 3.29 Rewrite the four Bellman equations for the four value functions (\\(v_\\pi\\), \\(v_*\\), \\(q_\\pi\\), and \\(q_*\\)) in terms of the three argument function \\(p\\) (3.4) and the two-argument function \\(r\\) (3.5).\n\n\nSolution 3.29. \\[\n\\begin{split}\nv_\\pi(s) &= \\sum_{a} \\pi(a|s) \\Big[r(a,s) + \\gamma \\sum_{s'} p(s'|a,s) v_\\pi(s')\\Big] \\\\\nv_*(s) &= \\max_a r(a,s) + \\gamma \\sum_{s'} p(s'|a,s) v_*(s') \\\\\nq_\\pi(a,s) &= r(a,s) + \\gamma \\sum_{s'} p(s'|s,a) \\sum_{a'} \\pi(a'|s') q_\\pi(a',s') \\\\\nq_*(a,s) &= r(a,s) + \\gamma \\sum_{s'} p(s'|s,a) \\max_{a'} q_*(a',s')\n\\end{split}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#optimality-and-approximation",
    "href": "chapters/03-finite-markov-decision-processes.html#optimality-and-approximation",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.7 Optimality and Approximation",
    "text": "3.7 Optimality and Approximation",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#summary",
    "href": "chapters/03-finite-markov-decision-processes.html#summary",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "3.8 Summary",
    "text": "3.8 Summary\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#footnotes",
    "href": "chapters/03-finite-markov-decision-processes.html#footnotes",
    "title": "3¬† Finite Markov Decision Processes",
    "section": "",
    "text": "‚ÄúOur‚Äù Markov processes could be called more precisely stationary discrete-time Markov process.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html",
    "href": "chapters/04-dynamic-programming.html",
    "title": "4¬† Dynamic Programming",
    "section": "",
    "text": "4.1 Policy Evaluation\nThe Bellman equations for state-value Equation¬†3.12 and for action-value Equation¬†3.13 can be used as update rules to approximate \\(v_\\pi\\) and \\(q_\\pi\\): \\[\nv_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_{k}(s')]\n\\tag{4.1}\\] \\[\nq_{k+1}(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma \\sum_{a'}\\pi(a'|s')q_k(s',a')]\n\\tag{4.2}\\]\nThese equations form the basis for iterative policy evaluation. The algorithm below demonstrates how to approximate \\(v_\\pi\\), where updates are performed in ‚Äúsweeps‚Äù rather than ‚Äúchunk updates‚Äù. This constitutes the policy evaluations step,\\(\\pi \\overset{\\mathrm{Eval}}{\\to} v_{\\pi}\\), in the policy iteration algorithm (Section 4.3).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#policy-evaluation",
    "href": "chapters/04-dynamic-programming.html#policy-evaluation",
    "title": "4¬† Dynamic Programming",
    "section": "",
    "text": "Listing¬†4.1: Iterative Policy Evaluation, for estimating \\(V \\approx v_\\pi\\)\n\n\nInput: \\(\\pi\\), the policy to be evaluated\nParameters: \\(\\theta &gt; 0\\), determining accuracy of estimation\nInitialisation: \\(V(s)\\), for all \\(s \\in \\mathcal{S}\\) arbitrarily, and \\(V(\\mathrm{terminal}) = 0\\)\nLoop:\n¬†¬†¬†\\(\\Delta \\gets 0\\)\n¬†¬†¬†Loop for each \\(s \\in \\mathcal{S}\\):\n¬†¬†¬†¬†¬†¬†¬†\\(v \\gets V(s)\\)\n¬†¬†¬†¬†¬†¬†¬†\\(V(s) \\gets \\sum_a \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r + \\gamma V(s')]\\)\n¬†¬†¬†¬†¬†¬†¬†\\(\\Delta \\gets \\max(\\Delta, |v - V(s))\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\n\n\n\nExample 4.1 Here we explore Example 4.1 from Sutton and Barto (2018)\nHere is the quick summary:\n\nstates: non-terminal states are numbered 1 through 14. The two gray cells are treated as a single terminal state.\nactions: Four deterministic actions available in each state: up, down, left, right. Moving ‚Äúoff the grid‚Äù results in no state change.\nrewards: A reward of -1 is given for every transition until the terminal state is reached\nreturn: undiscounted\n\n\n\n\n\n\n\n\nG\n\n\n\ngrid\n\n\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n\n\n\n\n\n\n\n\n\nAnd here are the state-values for the random policy:\n\n\n\n\n\n\n\nG\n\n\n\ngrid\n\n\n0\n\n-14\n\n-20\n\n-22\n\n-14\n\n-18\n\n-20\n\n-20\n\n-20\n\n-20\n\n-18\n\n-14\n\n-22\n\n-20\n\n-14\n\n\n0\n\n\n\n\n\n\n\n\nNote that these values are (luckily) exact, which will be useful for the next exercises\n\n\nExercise 4.1 In Example¬†4.1, if \\(\\pi\\) is the equiprobable random policy, what is \\(q_\\pi(11, \\mathrm{down})\\)? What is \\(q_\\pi(7, \\mathrm{down})\\)?\n\n\nSolution 4.1. We can use the state-value function given in the example: \\[\n\\begin{split}\nq_\\pi(11, \\mathrm{down}) &= -1 + 0 = -1\\\\\nq_\\pi(7, \\mathrm{down}) &= -1 + v_\\pi(11) = -1 + (-14) = -15\n\\end{split}\n\\]\n\n\nExercise 4.2 In Example¬†4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is \\(v_\\pi(15)\\) for the equiprobable random policy?\nNow suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is \\(v_\\pi(15)\\) for the equiprobable random policy in this case?\n\n\nSolution 4.2. Since the MDP is deterministic and all transitions give the same reward, the undiscounted Bellman equation Equation¬†3.12 simplifies to: \\[\nv_{\\pi}(s) = r + \\sum_{a}\\pi(a|s') [v_{\\pi}(s')],\n\\]\nwhere \\(r = -1\\).\nThe first case is quite easy to compute. The transitions for all original states remain unchanged, so their values also remain unchanged. For the new state 15, we can write: \\[\nv_\\pi(15) = -1 + \\frac{1}{4}(v_\\pi(12) + v_\\pi(13) + v_\\pi(14) + v_\\pi(15))\n\\] which gives \\(v_\\pi(15) = -20\\).\nNow in the second case we might be up for a lot of work, as state 13 has a new transition: taking action ‚Äúdown‚Äù leads to state 15. This changes the dynamics of the MDP, so in principle the values might change. However, luckily the existing state-value function still satisfies the Bellman equation for state 13: \\[\nv_\\pi(13) = -1 + \\frac{1}{4}(v_\\pi(12) + v_\\pi(9) + v_\\pi(13) + v_\\pi(15))\n\\]\nSubstitute the known values we see that the equation holds \\[\nv_\\pi(13) = -20 = -1 + \\frac{1}{4}(-22 - 20 - 14 - 20)\n\\]\nSo \\(v_\\pi(13)\\) remains consistent with the new dynamics. Since all Bellman equations continue to hold with the same values, the state-value function does not change. So, \\(v_\\pi(15)=-20\\) also in this case.\n\n\nExercise 4.3 What are the equations analogous to Equation¬†3.10, Equation¬†3.12, and Equation¬†4.1 for the action-value function \\(q_\\pi\\) and its successive approximation by a sequence of functions \\(q_0, q_1, \\dots\\)?\n\n\nSolution 4.3. We have already stated these equations in tandem as Equation¬†3.11, Equation¬†3.13, and Equation¬†4.2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#sec-policy-improvement",
    "href": "chapters/04-dynamic-programming.html#sec-policy-improvement",
    "title": "4¬† Dynamic Programming",
    "section": "4.2 Policy Improvement",
    "text": "4.2 Policy Improvement\nAn optimal policy can always be chosen to be deterministic. This is quite intuitive: why would introducing randomness in action selection be beneficial if all you care about is maximising expected return? More rigorously, if you are choosing between two actions, \\(a_1\\) and \\(a_2\\), and you know their values \\(q_\\pi(s,a_1)\\) and \\(q_\\pi(s,a_2)\\), then it is clearly best to take the one with the higher value. A key tool for this kind of reasoning is the policy improvement theorem.\n\nTheorem 4.1 Let \\(\\pi\\) be any policy and \\(\\pi'\\) a deterministic policy. Then \\(\\pi \\leq \\pi'\\) if \\[\nv_\\pi(s) \\leq q_\\pi(s,\\pi'(s)),\n\\] for all \\(s \\in \\mathcal{S}\\).\n\n\nProof. From the assumption, we have: \\[\n\\begin{split}\nv_\\pi(s) &\\leq q_\\pi(s, \\pi'(s)) \\\\\n&= \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s, A_t = \\pi'(s)] \\\\\n&= \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] \\\\\n\\end{split}\n\\] (if you wonder about the indices in the expectation: the first expectation is completely determined by the MDP, in the second one we stipulate action selection according to \\(\\pi'\\).)\nNow, we can unroll this expression recursively: \\[\n\\begin{align}\nv_\\pi(s) &\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] \\\\\n&\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma \\mathbb{E}_{\\pi'}[R'_{t+2} + \\gamma v_\\pi(S'_{t+2}) \\mid S'_{t+1} = S_{t+1}] \\mid S_t = s] \\\\\n&= \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2}) \\mid S_t = s]\n\\end{align}\n\\]\nand so on. The last equality should be justified formally by the law of total expectation and the law of the unconscious statistician (Theorem¬†2.4 and Theorem¬†2.1).\nIterating this process a couple of times we get \\[\nv_\\pi(s) \\leq \\mathbb{E}_{\\pi'}\\bigg[\\sum_{i=0}^{N} \\gamma^i R_{t+1+i} + \\gamma^{N+1} v_\\pi(S_{t+1+N}) \\;\\bigg|\\; S_t = s \\bigg]\n\\]\nand in the limit (everything is bounded so this should be kosher) \\[\nv_\\pi(s) \\leq \\mathbb{E}_{\\pi'}\\bigg[\\sum_{i=0}^{\\infty} \\gamma^i R_{t+1+i} \\;\\bigg| \\; S_t = s \\bigg] = v_{\\pi'}(s).\n\\]\n\nThis result allows us to show that every finite MDP has an optimal deterministic policy.\nLet \\(\\pi\\) be any policy. Define a new deterministic policy \\(\\pi'\\) by \\[\n\\pi'(s)= \\underset{a \\in \\mathcal{A}}{\\mathrm{argmax}} q_{\\pi}(s,a)\n\\]\nBy the policy improvement theorem, we have \\(\\pi \\leq \\pi'\\). Now consider two deterministic policies, \\(\\pi_1\\) and \\(\\pi_2\\), and define their meet (pointwise maximum policy) as \\[\n(\\pi_1 \\vee \\pi_2)(s) =\n\\begin{cases}\\pi_1(s) &\\text{if } v_{\\pi_1}(s) \\geq v_{\\pi_2}(s) \\\\\n\\pi_2(s) &\\text{else}\n\\end{cases}\n\\]\nThen, again by the policy improvement theorem, we have \\(\\pi_1, \\pi_2 \\leq \\pi_1 \\vee \\pi_2\\).\nNow, since the number of deterministic policies is finite (as both \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) are finite), we can take the meet over all deterministic policies and obtain an optimal deterministic policy.\nThis leads directly to a characterisation of optimality in terms of greedy action selection.\n\nTheorem 4.2 A policy \\(\\pi\\) is optimal, if and only if, \\[\nv_\\pi(s) = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi}(s,a),\n\\tag{4.3}\\]\nfor all \\(s \\in \\mathcal{S}\\).\n\n\nProof. If \\(\\pi\\) is optimal then \\(v_\\pi(s) &lt; \\max_{a} q_\\pi(s,a)\\) would lead to a contradiction using the policy improvement theorem.\nFor the converse we do an argument very similar to the proof of Theorem¬†4.1. So similar in fact that I‚Äôm afraid that were doing the same work twice. Let \\(\\pi\\) satisfy Equation¬†4.3. We show that \\(\\pi\\) is optimal by showing that \\[\n\\Delta(s) = v_{\\pi_*}(s) - v_{\\pi}(s)\n\\] is \\(0\\) for all \\(s \\in \\mathcal{S}\\), where \\(\\pi_*\\) is any deterministic, optimal policy.\nWe can bound \\(\\Delta(s)\\) like so \\[\n\\begin{split}\n\\Delta(s) &= q_{\\pi_*}(s,\\pi_*(s)) - \\max_a q_{\\pi}(s,a) \\\\\n&\\leq q_{\\pi_*}(s,\\pi_*(s)) - q_\\pi(s,\\pi_*(s)) \\\\\n&= \\mathbb{E}_{\\pi_*}[ R_{t+1} + \\gamma v_{\\pi_*}(S_{t+1}) - (R_{t+1} + \\gamma v_{\\pi}(S_{t+1})) | S_{t} = s] \\\\\n&= \\mathbb{E}_{\\pi_*}[\\gamma \\Delta(S_{t+1}) | S_t = s]\n\\end{split}\n\\]\nIterating this and taking the limit gives \\[\n\\Delta(s) \\leq \\lim_{k \\to \\infty} \\mathbb{E}_{\\pi_*}[\\gamma^k \\Delta(S_{t+k}) \\mid S_t = s] = 0.\n\\]\n\nFor a policy \\(\\pi\\), if we define \\(\\pi'(s) := \\underset{a}{\\mathrm{argmax}}\\;q_\\pi(s,a)\\), we get an improved policy, unless \\(\\pi\\) was already optimal. This constitutes the policy improvement step,\\(v_\\pi \\overset{\\mathrm{Imp}}{\\to} \\pi'\\), in the policy iteration algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#sec-policy-iteration",
    "href": "chapters/04-dynamic-programming.html#sec-policy-iteration",
    "title": "4¬† Dynamic Programming",
    "section": "4.3 Policy Iteration",
    "text": "4.3 Policy Iteration\nThe policy iteration algorithm chains evaluation and improvement and converges to an optimal policy, for any initial policy \\(\\pi_0\\): \\[\n\\pi_0 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_0} \\overset{\\mathrm{Imp}}{\\to}\n\\pi_1 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_1} \\overset{\\mathrm{Imp}}{\\to}\n\\pi_2 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_2} \\overset{\\mathrm{Imp}}{\\to}\n\\dots\n\\overset{\\mathrm{Imp}}{\\to} \\pi_* \\overset{\\mathrm{Eval}}{\\to} v_{*}\n\\]\nAnd here is the pseudo code.\n\n\n\nListing¬†4.2: Policy Iteration (using iterative policy evaluation) for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nParameter:\n\\(\\theta\\): a small positive number determining the accuracy of estimation\n\n1. Initialisation:\n\\(V(s) \\in \\mathbb{R}\\), \\(\\pi(s) \\in \\mathcal{A}(s)\\) arbitrarily, \\(V(\\mathrm{terminal}) = 0\\)\n\n2. Policy Evaluation\nLoop:\n¬†¬†¬†¬†\\(\\Delta \\gets 0\\)\n¬†¬†¬†¬†Loop for each \\(s \\in \\mathcal{S}\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(v \\gets V(s)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(V(s) \\gets  \\sum_{s',r}p(s',r|s,\\pi(s))[r + \\gamma V(s')]\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(\\Delta \\gets \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\n3. Policy Improvement\n\\(\\text{policy-stable} \\gets \\mathrm{true}\\)\nFor each \\(s \\in \\mathcal{S}\\):\n¬†¬†¬†¬†\\(\\text{old-action} \\gets \\pi(s)\\)\n¬†¬†¬†¬†\\(\\pi(s) \\gets \\underset{a}{\\mathrm{argmax}} \\sum_{s',r} p(s', r |s,a)[r + \\gamma V(s')]\\)\n¬†¬†¬†¬†If \\(\\text{old-action} \\neq \\pi(s)\\), then \\(\\text{policy-stable} \\gets \\text{false}\\)\nIf \\(\\text{policy-stable}\\):\n¬†¬†¬†¬†return \\(V \\approx v_*\\) and \\(\\pi \\approx \\pi_*\\)\nelse:\n¬†¬†¬†¬†go to Policy Evaluation\n\n\n\nNote that the final policy improvement step does not change the policy and is basically just checking that the current policy is optimal. So this is basically an extra step to see that this chain is done: \\(\\pi_0 \\overset{\\mathrm{Eval}}{\\to} v_{\\pi_0} \\overset{\\mathrm{Imp}}{\\to}\n\\dots\n\\overset{\\mathrm{Imp}}{\\to} \\pi_* \\overset{\\mathrm{Eval}}{\\to} v_{*} \\overset{\\mathrm{Imp}}{\\to} \\text{Finished}\\)\n\nExample 4.2 Here we explore Example 4.2 from Sutton and Barto (2018) - Jack‚Äôs car rental.\nHere‚Äôs a quick summary:\n\ntwo locations, each with a maximum of 20 cars (more cars added to a location magically vanish into thin air)\nduring the day, a random amount of customers rent cars and then another random amount of customers return cars\nat the end of a day, up to 5 cars can be moved between the locations\neach car rented rewards 10\neach move car costs 2\nrentals and returns are Poisson distributed:\n\\(\\lambda_{1,\\text{rent}} =3\\), \\(\\lambda_{1,\\text{return}} =3\\), \\(\\lambda_{2,\\text{rent}} =4\\), \\(\\lambda_{2,\\text{return}} =2\\)\n\nWe solve Jack‚Äôs car rental using policy iteration. The core computation in policy iteration is getting a state-action value from the state values, the one-step lookahead, which is used both in policy evaluation and policy improvement. This is the main performance bottleneck: \\[\nQ(s,a) = \\sum_{s',r} p(s',r|s,a) [ r + \\gamma V(s')]\n\\]\nThis expression is good for theorizing about the algorithm but its direct implementation feels awkward and inefficient. In practice, it‚Äôs better to split the four-argument \\(p(s',r|s,a)\\) into expected immediate reward \\(r(s,a)\\) and the transition probability \\(p(s'|s,a)\\).\n\\[\n\\begin{split}\nQ(s,a) &= \\sum_{s',r}p(s',r|s,a)r + \\gamma\\sum_{s',r}p(s',r|s,a)V(s') \\\\\n&= r(s,a) + \\gamma \\sum_{s'}p(s'|s, a) V(s')\n\\end{split}\n\\]\nFurthermore, we can make the algorithm more natural for this problem by introducing afterstates (Sutton and Barto 2018, sec. 6.8). Although we don‚Äôt use them to their full potential (we don‚Äôt learn an afterstate value function).\nAn afterstate is the environment state immediately after the agent‚Äôs action, but before the environment‚Äôs stochastic dynamics. This formulation is particularly effective when actions have deterministic effects. In Jack‚Äôs car rental, moving cars deterministically leads to an afterstate \\(s \\oplus a\\), while the stochastic dynamics - rentals and returns - then determine the next state \\(s'\\).\nThe one-step lookahead using afterstates becomes: \\[\nQ(s,a) = c(a) + r(s \\oplus a) + \\gamma \\sum_{s'} p(s'|s\\oplus a)V(s'),\n\\tag{4.4}\\]\nwhere \\(c(a)\\) is the cost of the, \\(r(s \\oplus a)\\) is the expected immediate reward for the afterstate.\nThe following code is a nearly verbatim implementation of the policy iteration pseudocode, using Equation¬†4.4 for evaluation. (Select annotations to see inline explanations.)\n\n# === policy iteration for Jack's car rental ===\nfrom collections import namedtuple\nfrom typing import Dict, List\n1from scripts.jacks_car_rental.jacks_car_rental import (\n    JacksCarRental,\n    State,\n    Action,\n)\n\n2Policy = Dict[State, Action]\nValueFn = Dict[State, float]\n\n\n3def compute_state_action_value(\n    env: JacksCarRental, state: State, action: Action, value: ValueFn, Œ≥: float\n) -&gt; float:\n    \"\"\"\n    Compute the expected one‚Äêstep return\n    \"\"\"\n    after_state, cost = env.move(state, action)\n\n    future_return = 0.0\n    for state_new in env.state_space:\n        p = env.get_transition_probability(after_state, state_new)\n        future_return += p * value[state_new]\n\n    return cost + env.get_expected_revenue(after_state) + Œ≥ * future_return\n\n\ndef policy_evaluation(\n    env: JacksCarRental, œÄ: Policy, value: ValueFn, Œ∏: float, Œ≥: float\n):\n    \"\"\"\n    Approximates the ValueFn from a deterministic policy\n    \"\"\"\n    while True:\n        Œî = 0.0\n\n        for s in env.state_space:\n            v_old = value[s]\n            v_new = compute_state_action_value(env, s, œÄ[s], value, Œ≥)\n            value[s] = v_new\n            Œî = max(Œî, abs(v_old - v_new))\n\n        if Œî &lt; Œ∏:\n            break\n\n\ndef policy_improvement(\n    env: JacksCarRental, œÄ: Policy, value: ValueFn, Œ≥: float\n) -&gt; bool:\n    \"\"\"\n    Improve a policy according to the provided value‚Äêfunction\n\n    If no state's action changes, return True (policy is stable). Otherwise return False.\n    \"\"\"\n    stable = True\n\n    for s in env.state_space:\n        old_action = œÄ[s]\n4        best_action = max(\n            env.action_space,\n            key=lambda a: compute_state_action_value(env, s, a, value, Œ≥),\n        )\n\n        œÄ[s] = best_action\n        if best_action != old_action:\n            stable = False\n\n    return stable\n\n\nPolicyIterationStep = namedtuple(\"PolicyIterationStep\", [\"policy\", \"values\"])\n\n\n5def policy_iteration(env, Œ∏, Œ≥) -&gt; List[PolicyIterationStep]:\n    optimal = False\n    history = []\n\n    # init policy and value-function\n    œÄ: Policy = {s: 0 for s in env.state_space}\n    value: ValueFn = {s: 0.0 for s in env.state_space}\n\n    while not optimal:\n\n        # evaluation and save\n        policy_evaluation(env, œÄ, value, Œ∏, Œ≥)\n        history.append(PolicyIterationStep(œÄ.copy(), value.copy()))\n\n        # find better policy\n        optimal = policy_improvement(env, œÄ, value, Œ≥)\n\n    return history\n\n\n1\n\nImport the environment class and type aliases (State, Action) from the Jack‚Äôs car‚Äêrental module.\n\n2\n\nSince we are dealing with deterministic policies we can define Policy as a mapping.\n\n3\n\nImplements the one‚Äêstep lookahead with afterstates (see Equation¬†4.4).\n\n4\n\nChoose the action \\(a\\in \\mathcal{A}(s)\\) that maximises the one‚Äêstep lookahead; this is a concise way to do ‚Äú\\(\\mathrm{argmax}_a \\dots\\)‚Äù‚Äù in Python\n\n5\n\nThe only slight change to the pseudocode: instead of just returning the optimal policy and its state-value function we return the history of policies.\n\n\n\n\nHere is some more code just to be able to visualize our solution for Jack‚Äôs car rental problem‚Ä¶\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm\nfrom matplotlib import cm\n\nfrom scripts.jacks_car_rental.jacks_car_rental import (\n    JacksCarRentalConfig,\n)\n\n\ndef plot_policy(title: str, config: JacksCarRentalConfig, œÄ: dict):\n    max_cars = config.max_cars\n    max_move = config.max_move\n\n    # Build a (max_cars+1)√ó(max_cars+1) integer grid of ‚Äúaction‚Äù values\n    policy_grid = np.zeros((max_cars + 1, max_cars + 1), dtype=int)\n    for (cars1, cars2), action in œÄ.items():\n        policy_grid[cars1, cars2] = action\n\n    # X/Y coordinates for pcolormesh:\n    x = np.arange(max_cars + 1)\n    y = np.arange(max_cars + 1)\n    X, Y = np.meshgrid(x, y)\n\n    fig, ax = plt.subplots(figsize=(9, 9))\n\n    # Discrete actions range\n    actions = np.arange(-max_move, max_move + 1)\n    n_colors = len(actions)\n\n    # Create a ‚Äúcoolwarm‚Äù colormap with exactly n_colors bins\n    cmap = plt.get_cmap(\"coolwarm\", n_colors)\n\n    # For a discrete colormap, we want boundaries at x.5, so that integer values\n    # get mapped to their own color. Example: if max_move=2, actions = [-2, -1, 0, 1, 2],\n    # then boundaries = [-2.5, -1.5, -0.5, 0.5, 1.5, 2.5].\n    bounds = np.arange(-max_move - 0.5, max_move + 0.5 + 1e-6, 1)\n    norm = BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n\n    cax = ax.pcolormesh(\n        X,\n        Y,\n        policy_grid,\n        cmap=cmap,\n        norm=norm,\n        edgecolors=\"black\",\n        linewidth=0.4,\n    )\n\n    # Axis labels and title\n    ax.set_xlabel(\"Cars at Location¬†2\", fontsize=12)\n    ax.set_ylabel(\"Cars at Location¬†1\", fontsize=12)\n    ax.set_title(title, fontsize=14, pad=12)\n\n    # Square aspect ratio so each cell is a square:\n    ax.set_aspect(\"equal\")\n\n    # Ticks every 5 cars\n    step = 5\n    ax.set_xticks(np.arange(0, max_cars + 1, step))\n    ax.set_yticks(np.arange(0, max_cars + 1, step))\n\n    # Colorbar (horizontal, at the bottom)\n    cbar = fig.colorbar(\n        cax,\n        ax=ax,\n        orientation=\"horizontal\",\n        pad=0.08,\n        shrink=0.85,\n        boundaries=bounds,\n        ticks=actions,\n        label=\"Action‚Äâ(Car Movement)\",\n    )\n    cbar.ax.xaxis.set_label_position(\"bottom\")\n    cbar.ax.xaxis.tick_bottom()\n\n    fig.tight_layout(rect=[0, 0.03, 1, 1])\n\n    plt.show()\n\n\ndef plot_valueFn(title, config, val):\n    \"\"\"\n    3D surface plot of the value function\n    \"\"\"\n    max_cars = config.max_cars\n\n    # Build a (max_cars+1)√ó(max_cars+1) grid of value estimates\n    value_grid = np.zeros((max_cars + 1, max_cars + 1), dtype=float)\n    for (l1, l2), v in val.items():\n        value_grid[l1, l2] = v\n\n    # Meshgrid for locations on each axis\n    x = np.arange(max_cars + 1)\n    y = np.arange(max_cars + 1)\n    X, Y = np.meshgrid(x, y)\n\n    fig = plt.figure(figsize=(11, 7))\n    ax = fig.add_subplot(111, projection=\"3d\")\n\n    # Shaded surface plot\n    surf = ax.plot_surface(\n        X,\n        Y,\n        value_grid,\n        rstride=1,\n        cstride=1,\n        cmap=cm.viridis,\n        edgecolor=\"none\",\n        antialiased=True,\n    )\n\n    ax.set_xlabel(\"Cars at Location‚ÄØ2\", fontsize=12, labelpad=10)\n    ax.set_ylabel(\"Cars at Location‚ÄØ1\", fontsize=12, labelpad=10)\n    ax.set_title(title, fontsize=14, pad=12)\n    ax.view_init(elev=35, azim=-60)\n\n    fig.tight_layout()\n    plt.show()\n\n\n‚Ä¶and now we can solve it:\n\n# === Solving Jack's car rental ===\n\n# Hyperparameter for \"training\"\nŒ≥ = 0.9\nŒ∏ = 1e-5\n\n# config and environment\nconfig = JacksCarRentalConfig(max_cars=20)\nenv = JacksCarRental(config)\n\n# do policy iteration\nhistory = policy_iteration(env, Œ∏, Œ≥)\n\n# print last (optimal) policy and its value function\nplot_policy(\n    f\"Optimal Policy after {len(history)-1} iterations\", config, history[-1].policy\n)\nplot_valueFn(f\"Value function for optimal policy\", config, history[-1].values)\n\n\n\n\n\n\n\n\n\n\n(a) Heatmap of the optimal policy \\(\\pi_‚àó(s)\\). Each cell at coordinates \\((i,j)\\) shows the number of cars moved from location 1 to location 2.\n\n\n\n\n\n\n\n\n\n\n\n(b) 3D surface of the optimal state‚Äêvalue function \\(v_‚àó‚Äã(s)\\) corresponding to the policy in (a). The maximum value here is approximately \\(\\max‚Å°_s v_‚àó(s)\\approx 625\\); Sutton &‚ÄØBarto‚Äôs reported maximum is \\(\\approx 612\\).\n\n\n\n\n\n\nFigure¬†4.1: This is like the last two diagrmas in Figure 4.2 (Sutton and Barto 2018): the first diagrams shows the optimal policy - which looks to me identical to Sutton-Barto‚Äôs optimal policy. The second diagram shows its value function (the optimal value function), which seems to have a higher maximum as Sutton-Barto‚Äôs - I don‚Äôt know why.\n\n\n\n\nThe runtime of this policy iteration is quite reasonable - just a few seconds. But considering we‚Äôre only dealing with \\(441\\) states, it highlights how dynamic programming is limited to small MDPs.\nAnother thought: although we now have the optimal solution, the meaning of the value function is still somewhat abstract. Mathematically it‚Äôs clear, but I couldn‚Äôt, for example, say whether Jack can pay his monthly rent with this business.\n\n\nExercise 4.4 The policy iteration algorithm Listing¬†4.2 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the seudocode so that convergence is guaranteed.\n\n\nSolution 4.4. If ties in the argmax are determined randomly, this could maybe result in a soft-lock when there are enough states with ties in their evaluation. Here we should add a condition that the policy is only changed if the change results in an actual improvement. Often in application there is an order on the actions, and we could also choose the smallest. However, this might also have consequences for the exploration of the algorithms.\nSo maybe the best solution is to only change the policy action if a better action improves the value by more than some \\(\\epsilon&gt;0\\):\n\n\n\nListing¬†4.3: Modified step Policy Improvement in Policy Iteration Listing¬†4.2\n\n\nExtra Paramater:\n\\(\\epsilon\\): small parameter determining of policy is stable\n\nPolicy Improvement\n\\(\\text{policy-stable} \\gets \\mathrm{true}\\)\n\nfor each \\(s \\in \\mathcal{S}\\):\n¬†¬†¬†¬†\\(\\text{old-action} \\gets \\pi(s)\\)\n\n¬†¬†¬†¬†for each \\(a \\in \\mathcal{A}(s)\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(Q(a) \\gets \\sum_{s',r} p(s',r | s,a) [ r + \\gamma V(s') ]\\)\n\n¬†¬†¬†¬†\\(\\text{best-value} \\gets \\max_{a} Q(a)\\)\n\n¬†¬†¬†¬†if \\(\\text{best-value} - Q(\\text{old-action}) &gt; \\epsilon\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(\\pi(s) \\gets a^*\\) for which \\(Q(a_*) = \\text{best-value}\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(\\text{policy‚Äêstable} \\gets \\mathrm{false}\\)\n\nif policy‚Äêstable:\n¬†¬†¬†¬†return \\(V \\approx v_*\\) and \\(œÄ ‚âà œÄ_*\\)\nelse:\n¬†¬†¬†¬†go to Policy Evaluation\n\n\n\n\n\nExercise 4.5 How would policy iteration be defined for action values? Give a complete algorithm for computing \\(q_*\\), analogous to Listing¬†4.2 for computing \\(v_*\\). Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.\n\n\nSolution 4.5. The code depends on the convention that episodic tasks have an absorbing state that transitions only to itself and that generates only rewards of zero (Sutton and Barto 2018, sec. 3.4).\n\n\n\nListing¬†4.4: Policy Iteration (using iterative policy evaluation) for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nParameter:\n\\(\\theta\\): a small positive number determining the accuracy of estimation\n\nInitialisation:\n\\(Q(s,a) \\in \\mathbb{R}\\), \\(\\pi(s) \\in \\mathcal{A}(s)\\) arbitrarily, but \\(Q(\\mathrm{terminal},a) = 0\\)\n\nPolicy Evaluation\nLoop:\n¬†¬†¬†¬†\\(\\Delta \\gets 0\\)\n¬†¬†¬†¬†Loop for each \\(s \\in \\mathcal{S}\\) and \\(a \\in \\mathcal{A}(s)\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(q \\gets Q(s,a)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(Q(s,a) \\gets  \\sum_{s',r}p(s',r|s,a)[r + \\gamma Q(s',\\pi(s'))]\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(\\Delta \\gets \\max(\\Delta, |q - Q(s,a)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\nPolicy Improvement\n\\(\\text{policy-stable} \\gets \\mathrm{true}\\)\nFor each \\(s \\in \\mathcal{S}\\):\n¬†¬†¬†¬†\\(\\text{old-action} \\gets \\pi(s)\\)\n¬†¬†¬†¬†\\(\\pi(s) \\gets \\underset{a}{\\mathrm{argmax}}  \\; Q(s,a)\\)\n¬†¬†¬†¬†If \\(\\text{old-action} \\neq \\pi(s)\\), then \\(\\text{policy-stable} \\gets \\text{false}\\)\nIf \\(\\text{policy-stable}\\):\n¬†¬†¬†¬†return \\(Q \\approx q_*\\) and \\(\\pi \\approx \\pi_*\\)\nelse:\n¬†¬†¬†¬†go to Policy Evaluation\n\n\n\n\n\nExercise 4.6 Suppose you are restricted to considering only policies that are \\(\\varepsilon\\)-soft, meaning that the probability of selecting each action in each state, \\(s\\), is at least \\(\\varepsilon/|\\mathcal{A}(s)|\\). Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm Listing¬†4.2 for \\(v_*\\).\n\n\nSolution 4.6. First of all, we are now dealing with a policy \\(\\pi(a|s)\\) that assigns probabilities to actions instead of \\(\\pi(s)\\) that choses an action.\nFor step 3, policy improvement, we assign to every possible action \\(a\\) a propability \\(\\varepsilon\\), then the remainining propability \\(1 - \\varepsilon|\\mathbfcal{A}(s)|\\) to the actions that maximize the expression under the argmax.\nFor step 2, policy evaluation, we have to change the assignment to \\(V(s) \\gets \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma V(s')]\\)\nFor step 1, initialisation, we have to make sure that each action gets a probability of at least \\(\\varepsilon\\).\n\n\nExercise 4.7 Write a program for policy iteration and re-solve Jack‚Äôs car rental problem with the following changes. One of Jack‚Äôs employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem.\n\n\nSolution 4.7. We have already set up everyting in Example¬†4.2. I made sure in the implementation for the environment to include parameters for these changes.\n\n# === solving Jack's car rental again ===\n# Hyperparameter for \"training\"\nŒ≥ = 0.9\nŒ∏ = 1e-5\n\n# config and environment\nconfig = JacksCarRentalConfig(\n    max_cars=20, free_moves_from_1_to_2=1, max_free_parking=10, extra_parking_cost=4\n)\nenv = JacksCarRental(config)\n\n# do policy iteration\nhistory = policy_iteration(env, Œ∏, Œ≥)\n\n# print last optimal policy\nplot_policy(\n    f\"Optimal Policy after {len(history)-1} iterations\", config, history[-1].policy\n)\n\n\n\n\n\n\n\n\nInterestingly this somewhat more complex problem, does need 1 less iteration than the original.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#value-iteration",
    "href": "chapters/04-dynamic-programming.html#value-iteration",
    "title": "4¬† Dynamic Programming",
    "section": "4.4 Value Iteration",
    "text": "4.4 Value Iteration\nValue iteration is policy iteration with a single sweep of policy evaluation per iteration.\nIn policy iteration, policy improvement is given by: \\[\n\\pi_k(s) = \\mathrm{argmax}_{a} \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_k(s')]\n\\]\nThis picks an action \\(\\hat{a}\\) that gives the best one-step lookahead from the current value function.\nThen, policy evaluation, uses the one-step lookahead for the updates: \\[\nv_{k+1}(s) = \\sum_{s',r}p(s',r|s,\\pi_k(s)) [r + \\gamma v_k(s')]\n\\]\nBut if we‚Äôre only doing one sweep, we may as well just plug in \\(\\hat{a}\\) directly from the lookahead, which gives the maximum of this expression: \\[\nv_{k+1}(s) = \\max_a \\sum_{s',} p(s',r|s,a)[r + \\gamma v_k(s')]\n\\tag{4.5}\\]\nSo, value iteration performs both greedy action selection and value backup in one go.\nAnd here is the respective pseudocode:\n\n\n\nListing¬†4.5: Value Iteration, for estimating \\(\\pi \\approx \\pi_*\\)\n\n\nParameter:\n\\(\\theta\\): a small positive number determining the accuracy of estimation\n\nInitialisation:\n\\(V(s)\\) arbitrarily for \\(s \\in \\mathcal{S}\\), when episodic \\(V(\\mathrm{terminal}) = 0\\)\n\nValue Iteration\nLoop:\n¬†¬†¬†¬†\\(\\Delta \\gets 0\\)\n¬†¬†¬†¬†Loop for each \\(s \\in \\mathcal{S}\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(v \\gets V(s)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(V(s) \\gets  \\max_a \\sum_{s',r} p(s',p|s,a) [r + \\gamma V(s')]\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(\\Delta \\gets \\max(\\Delta, |v - V(s)|)\\)\nuntil \\(\\Delta &lt; \\theta\\)\n\nGreedy Policy Extraction\nOutput a deterministic policy \\(\\pi \\approx \\pi_*\\), such that\n\\(\\pi(s) = \\mathrm{argmax}_a \\sum_{s',r} p(s',p|s,a) [r + \\gamma V(s')]\\)\n\n\n\n\n4.4.1 gambler‚Äôs problem\nLet‚Äôs talk about Example 4.3 from Sutton and Barto (2018) - the Gambler‚Äôs Problem. It gets its own little subsection because there‚Äôs actually quite a bit to say about it.\nFirst, a quick summary of the MPD:\n\nthe idea is to win a target amount \\(N\\) of chips by betting on coin flips\nnon-terminal states are the current amount of chips (captial): \\(\\mathcal{S} = \\{0,1,\\dots,N\\}\\), and the terminal states are \\(0\\) and \\(N\\).\nactions are how mamny chips you wager (the stake): \\(\\mathcal{A}(s) = \\{1,\\dots, \\min(s, N-s)\\}\\) for \\(s \\in \\mathcal{S}\\)\nthe environment dynamics are \\[\np(s' \\mid a,s) = \\begin{cases}p_{\\mathrm{win}} &\\text{if }s' = s+a\\\\1-p_{\\mathrm{win}} &\\text{if }s' = s-a \\end{cases}\n\\]\nrewards are 0 except for reaching the goal state \\(N\\), which gives a rewards of +1\nthe task is episodic and undiscounted (\\(\\gamma = 1\\))\n\nThe reward structure is set up so that the state-value function gives the probability of eventually reaching the goal from a given state: \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] = 1 \\cdot \\mathrm{Pr}_{\\pi}(S_T = N \\mid S_t = s)\n\\]\nSo under any policy \\(\\pi\\), the value of a state is just the probability of hitting \\(N\\) before hitting \\(0\\).\n\n4.4.1.1 general remarks\nThere‚Äôs a lot of content about this problem floating around online. Maybe that‚Äôs because it appears quite early in the book, or because it‚Äôs so simple to implement. Or maybe it‚Äôs because the problem hides a few trip wires. For one thing, if you implement it yourself without really understanding what‚Äôs going on, your correct solution might look completely wrong (see Figure¬†4.2 (b)).\nA lot of the articles I‚Äôve come across either lack substance or seem a bit confused - which is totally fair, but I personally prefer reading something more insightful when digging into a problem.\nSo here, I‚Äôm trying to give this a bit of extra depth.\n\n\n4.4.1.2 no 0 stakes\nAnother issue you might stumble across is that the original example allows wagers of size 0. Which seems innocuous, but it actually complicates things when thinking about deterministic policies.\nSince the problem is undiscounted, we have: \\[\nq_*(s,0) = v_*(s) = \\max_a q_*(s,a)\n\\]\nSo technically, wagering nothing could be considered an optimal action. However, any deterministic policy following such an action will not complete an episode.\nFurthermore, zero stakes can also mess with value iteration. If we initialise the non-terminal states with a value greater than their optimal value, the algorithm will not update them, because it will just set \\(v(s) = q(s,0)\\).\nIt mostly leads to problems, so let‚Äôs leave them just out here.\n\n\n4.4.1.3 one-step lookahead and environment\nWe‚Äôll follow the design proposed by Sutton and Barto (2018): no rewards, and two terminal states:\n\nruin: 0 capital, with value \\(0\\)\nwin: 100 captial, with value \\(1\\)\n\nThe terminal states are not updated during value iteration, of course.\nGiven that, the one-step lookahead for stake \\(s\\) and wager \\(a\\) is especially simple to compute: \\[\nQ(s,a) = p_\\mathrm{win} \\cdot V(s+a) + (1-p_\\mathrm{win}) \\cdot V(s-a).\n\\]\nIn code, it‚Äôs the environment‚Äôs responsibility to calculate the one-step lookaheads for a given state-value function (one_step_lookaheads), as well as to initialise the value function (make_initial_values). This gives us some nice encapsulation.\n\nfrom typing import List\n\nValueFn = List[float]\nState = int\n\n\nclass EnvGamblersProblem:\n    def __init__(self, p_win: float, goal: int):\n        self.p = p_win\n        self.goal = goal\n        self.non_terminal_states = list(range(1, goal))\n\n    def make_initial_values(self, non_terminal_value: float) -&gt; ValueFn:\n        v = [0.0]  # terminal: ruin\n        v.extend([non_terminal_value] * (self.goal - 1))  # non-terminals\n        v.append(1.0)  # terminal: win\n        return v\n\n    def one_step_lookaheads(self, s: State, v: ValueFn):\n        \"\"\"returns a list of the q-values for state s.\n        q[i] contains the value of betting an amount of i+1\"\"\"\n        p = self.p\n        goal = self.goal\n        return [\n            p * v[s + a] + (1 - p) * v[s - a] for a in range(1, min(s, goal - s) + 1)\n        ]\n\n\n\n4.4.1.4 solving the problem\nNow we can implement value iteration, as described in Listing¬†4.5. We split the process into the two parts: value iteration and policy extraction.\nI‚Äôve modified the value iteration function so that it returns all intermediate value functions - the final one is the optimal state-value function.\n\ndef value_iteration_gamblers_problem(\n    env: EnvGamblersProblem, Œ∏: float, init_value=0.0\n) -&gt; List[ValueFn]:\n    # **Init**\n    v = env.make_initial_values(init_value)\n\n    # **Value iteration**\n    value_functions = []\n    while True:\n        value_functions.append(v.copy())\n        Œî = 0.0\n        for s in env.non_terminal_states:\n            v_old = v[s]\n            v_new = max(env.one_step_lookaheads(s, v))\n            v[s] = v_new\n            Œî = max(Œî, abs(v_old - v_new))\n        if Œî &lt; Œ∏:\n            break\n\n    return value_functions\n\n\ndef get_greedy_policy(v: ValueFn, env: EnvGamblersProblem):\n    # ** Greedy Policy Extraction **\n    policy = {}\n    for s in env.non_terminal_states:\n        action_values = env.one_step_lookaheads(s, v)\n1        greedy_action_idx = action_values.index(max(action_values))\n        policy[s] = greedy_action_idx + 1  # convert idx -&gt; stake\n    return policy\n\n\n1\n\nthis isn‚Äôt the most efficient way to compute the argmax of a list, but it‚Äôs fine for a simple problem like this.\n\n\n\n\nNow, using the code below, we can solve the gambler‚Äôs problem for \\(N = 100\\) and \\(p_\\mathrm{win} = 0.4\\).\n\n# set up environment\nenv = EnvGamblersProblem(p_win=0.4, goal=100)\n\n# run value_iteration\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\n\n# optain optimal value function (result of last sweep)\nv_star = value_functions[-1]\n\n# optain optimal policy (greedy w.r.t to v_star)\npolicy_star = get_greedy_policy(v_star, env)\n\nWe plot the value function sweeps and a (somewhat strangely shaped) optimal policy below in Figure¬†4.2.\nWe‚Äôre not going to go into the mathematical analysis of the exact solution here, but if you‚Äôre curious about exact formulas for the value function, check out the section about ‚ÄòBold Strategy‚Äô for the game ‚ÄòRed and Black‚Äô - which is apparently the mathematicans way of calling the gambler‚Äôs problem - in Siegrist (2023). 1\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple\n\n\ndef plot_value_function(\n    value_functions: List[Tuple[str, ValueFn]],\n    title: str = \"Value Function Sweeps\",\n):\n    plt.figure(figsize=(10, 6))\n\n    for label, value_function in value_functions:\n        plt.plot(range(0, len(value_function)), value_function, label=label)\n\n    plt.xlabel(\"State (Capital)\")\n    plt.ylabel(\"Value\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n# sweeps to plot\nsweeps = (1, 2, 3, len(value_functions) - 1)\nvalue_functions_with_labels = [\n    (\n        f\"{\"final \" if i == len(value_functions) -1 else \"\" }sweep {i}\",\n        value_functions[i],\n    )\n    for i in sweeps\n]\nplot_value_function(\n    value_functions_with_labels,\n    title=r\"Value Function Sweeps for $N=100$ and $p_\\mathrm{win}=0.4$\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) This shows the value function over several sweeps of value iteration. The final sweep is the optimal value function. The first few sweeps (1-3) look very similar to what Sutton and Barto (2018) show. Interestingly, though, in their plot it seems to take more sweeps, even sweep 32 still noticeably differs from the final result.\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple, Dict\n\n\ndef plot_policy(policy: Dict[int, int], title: str = \"Policy\"):\n    plt.figure(figsize=(10, 6))\n\n    states = sorted(policy.keys())\n    actions = [policy[s] for s in states]\n    plt.scatter(states, actions)\n\n    plt.xlabel(\"State (Capital)\")\n    plt.ylabel(\"Action\")\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nplot_policy(policy_star, title=r\"An optimal policy for $N=100$ and p_\\mathrm{win$ = 0.4$\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) This shows one optimal policy. It looks quite different to the one shown by Sutton and Barto (2018). That‚Äôs because many states have multiple optimal actions. The greedy action selection just picks one, and the result is ultimatively decided by floating-point imprecisions.\n\n\n\n\n\n\n\nFigure¬†4.2: This is the analogue of Figure 4.3 from Sutton and Barto (2018). It shows the solution to the gambler‚Äôs problem for \\(N=100\\) and \\(p_\\mathrm{win} = 0.4\\).\n\n\n\n\n\n4.4.1.5 optimal actions\nSo, Figure¬†4.2 (b) looks jagged because, in some states, there are multiple optimal actions. The choice of which one is used isn‚Äôt governed by any particular logic in the implementation - it‚Äôs effectively decided by floating-point imprecision.\nHere we want to answer this somewhat understated question from Sutton and Barto (2018, 84):\n\nThis policy [shown in Figure 4.3] is optimal, but not unique. In fact, there is a whole family of optimal policies, all corresponding to ties for the argmax action selection with respect to the optimal value function. Can you guess what the entire family looks like?\n\nThe short answer is: no, I can‚Äôt. It‚Äôs not something that‚Äôs easy to guess, in my view.\nHowever, Siegrist (2023) gives an account of some well-developed theory addressing this question in. The section about bold play in the game Red and Black proves the existence of optimal strategies.\nOne such optimal strategy pattern applicable for all \\(p_\\mathrm{win} &lt; 0.5\\) and \\(N\\) is bold play, where you always bet the maximum amount possible: \\[\nB(s) := \\max \\mathcal{A}(s) =  \\min(\\{s, N-s\\})\n\\]\nActually, for any \\(p_\\mathrm{win} &lt; 0.5\\), all optimal actions are the same - only the underlying value functions change. The shape of \\(B(s)\\) is triangular, and if \\(N\\) is odd, it is the unique optimal policy, as seen in Figure¬†4.3 (c).\nIf \\(N\\) is even, then there exists a second-order bold strategy \\(B_2\\), which effectively applies divide and conquer to the problem (although I don‚Äôt have a really good intuition why it works). Create two subproblems, one from \\(0\\) to \\(N/2\\), and another from \\(N/2\\) to \\(N\\), each treated with their own bold strategy. It looks a bit like this (where \\(\\left\\lfloor \\frac{N}{4} \\right\\rfloor\\) means \\(N/4\\) rounded down):\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the x values\nx = np.linspace(0, 1, 1000)\n\n\n# Define the B_2(x)\ndef B2(x):\n    return np.where(\n        x &lt; 0.25, x, np.where(x &lt; 0.5, 0.5 - x, np.where(x &lt; 0.75, -0.5 + x, 1 - x))\n    )\n\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(x, B2(x), color=\"blue\")\n\n# Highlight key points\nax.plot([0.5], [0.5], \"o\", color=\"blue\")\nax.plot(\n    [0.5], [0], \"o\", mfc=\"white\", mec=\"blue\", zorder=5, clip_on=False\n)  # Draw over axis\n\n# Add dashed grid lines for key points\nax.axhline(0.25, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nax.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nax.axvline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nax.plot([0.25, 0.5], [0.25, 0.5], \"gray\", linestyle=\"--\", linewidth=0.5)\nax.plot([0.75, 0.5], [0.25, 0.5], \"gray\", linestyle=\"--\", linewidth=0.5)\n\n# Add axis labels and ticks\nax.set_xticks([0, 0.5, 1])\nax.set_xticklabels([\"0\", r\"$\\frac{N}{2}$\", r\"$N$\"])\nax.set_yticks([0.25, 0.5])\nax.set_yticklabels([r\"$\\left\\lfloor \\frac{N}{4} \\right\\rfloor$\", r\"$\\frac{N}{2}$\"])\n\nax.set_xlabel(r\"$s$\")\nax.set_ylabel(r\"$B_2(s)$\", rotation=0, labelpad=15)\n\n# Style the plot\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"left\"].set_position(\"zero\")\nax.spines[\"bottom\"].set_position(\"zero\")\nax.set_xlim(0, 1)\nax.set_ylim(0, 0.55)\n\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf \\(N\\) is divisible by \\(4\\), we can dividive and conquer again to get \\(B_3\\). This is exactly what we see in the original problem for \\(N = 100\\) (see Figure¬†4.3 (a)).\nBasically, if \\(N\\) is divisible by \\(2^\\ell\\) then \\(B_\\ell\\) is an optimal stategy. In the limit, this family gives rise to a kind of fractal pattern of stacked diamonds (see Figure¬†4.3 (b)) - although these diamonds are missing their bottom tips, which would correspond to wagering 0.\nSo my finally answer to the inital question is. The family of optimal policies consists of any selection of actions from the bold-strategy hierarchy: \\(B\\) (one big triangle), \\(B_2\\) (two triangles), and \\(B_3\\) (four triangles).\n\n\n\n\n\n\nCode\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n\ndef get_greedy_actions(v, env, Œµ):\n    optimal_actions = {}\n    for s in env.non_terminal_states:\n        q_values = env.one_step_lookaheads(s, v)\n        max_value = max(q_values)\n        bests = []\n        for a, q in enumerate(q_values):\n            if max_value - q &lt; Œµ:\n                bests.append(a + 1)\n        optimal_actions[s] = bests\n\n    return optimal_actions\n\n\n@dataclass\nclass MultiPolicy:\n    actions: Dict[int, List[int]]  # state -&gt; list of stakes\n    name: str = None\n    marker: str = \"o\"\n    size: int = None\n\n\ndef plot_multi_policy(multi_policies: List[MultiPolicy], title: str):\n    plt.figure(figsize=(10, 6))\n\n    draw_legend = False\n\n    for pol in multi_policies:\n        pts = [(s, a) for s, al in pol.actions.items() for a in al]\n        states, acts = zip(*pts)\n        if pol.name:\n            draw_legend = True\n        plt.scatter(\n            states,\n            acts,\n            marker=pol.marker,\n            s=pol.size,\n            label=pol.name,\n        )\n\n    plt.xlabel(\"State (Capital)\")\n    plt.ylabel(\"Action (Stake)\")\n    if draw_legend:\n        plt.legend()\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\n\nbest_minimal_actions = {}\nfor s in best_actions:\n    best_min = min(best_actions[s])\n    best_minimal_actions[s] = [best_min]\n\noptimals = [\n    MultiPolicy(best_actions, name=\"optimal actions\"),\n    MultiPolicy(best_minimal_actions, name=\"third-order bold strategy\", marker=\"x\", size=30),\n]\nplot_multi_policy(optimals, title=\"Optimal Actions for N=100\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The optimal actions for the gambler‚Äôs problem of example 4.3 of Sutton and Barto (2018) for \\(p_\\mathrm{win} = 0.4\\) and \\(N = 100\\). The actions chosen by ‚Äòthird-order bold strategy‚Äô is the optimal policy shown by Sutton and Barto (2018).\n\n\n\n\n\n\nCode\ngoal = 5 * 32\nenv = EnvGamblersProblem(p_win=0.4, goal=goal)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-14)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\nplot_multi_policy([MultiPolicy(best_actions, size=12)], title=f\"Optimal actions for N={goal}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The optimal actions for the gambler‚Äôs problem for \\(N = 5\\cdot 32\\) and \\(p_\\mathrm{win} &lt; 0.5\\). It consists of the bold strategies up to order \\(6\\).\n\n\n\n\n\n\nCode\ngoal = 101\nenv = EnvGamblersProblem(p_win=0.4, goal=goal)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\nplot_multi_policy([MultiPolicy(best_actions)], title=f\"Optimal actions for N={goal}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) For \\(N = 101\\) only bold play is optimal. The same is true for all odd \\(N\\) and \\(p_\\mathrm{win} &lt; 0.5\\).\n\n\n\n\n\n\n\nFigure¬†4.3: Showcase of all optimal actions for \\(p &lt; 0.5\\) for various \\(N\\).\n\n\n\n\n\n4.4.1.6 gambler‚Äôs ruin\nCheck out the value function in Figure¬†4.2 (a). It looks like, even though a single coin flip is stacked against the gambler with \\(p_\\mathrm{win} = 0.4\\), if they start with a large capital, say 80 chips, they still reach the goal of \\(N = 100\\) about 70% of the time. Doesn‚Äôt sound too bad. But of course, if they succeed, they only win 20 chips. If they lose, they lose 80 chips.\nOne general result that captures this asymmetry is gambler‚Äôs ruin, which states essentially that when the odds are stacked against the gambler, there is no strategy to turn the odds in their favour.\nWe can make this concrete by calculating the expected monetary return when following the optimal strategy, starting with \\(s\\) chips: \\[\n\\mathbb{E}_{\\pi_*}[S_T - S_0 \\mid S_0 = s] = p_{\\mathrm{goal}}(N-s) + (1-p_{\\mathrm{goal}})(-s),\n\\]\nwhere \\[\np_{\\mathrm{goal}}(s) = \\mathrm{Pr}_{\\pi_*}(S_T = N \\mid S_0 = s) = v_*(s).\n\\]\nLet‚Äôs compute and plot it for \\(p = 0.4\\) and \\(N = 100\\):\n\n\nCode\ngoal = 100\np_win = 0.4\nenv = EnvGamblersProblem(p_win=p_win, goal=goal)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nexpected_profit = [\n    value * (env.goal - s) - (1 - value) * s for s, value in enumerate(v_star)\n]\n\n\nplt.figure(figsize=(10, 6))\n\nplt.plot(range(0, len(expected_profit)), expected_profit)\nplt.xlabel(\"Startign Capital\")\nplt.ylabel(\"Expected profit\")\nplt.title(\n    r\"Profit when following an optimal strategy for $N = 100$ and $p_\\mathrm{win} = 0.4$\"\n)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nEven though the plot shows a nice (fractal-looking) w shape, you‚Äôre expected to lose chips, no matter what your initial capital is.\nIf you have to play, then the best is to start with either very low or very high capital - this is not financial advice though ü•∏.\nAs a risk-averse person, my actual advice is: start with \\(0\\) or \\(100\\) chips - i.e., don‚Äôt play at all.\n\n\n4.4.1.7 gambler‚Äôs fortune\nWhen we stack the game in favour of the gambler \\(p_{\\mathrm{win}} &gt; 0.5\\) everything becomes somewhat easier.\nThere is just one optimal strategy, timid play, that is, always bet exactly one coin \\[\n\\pi_*(s) = 1.\n\\]\nThe value function in this case has a clean analytical form: \\[\nv(s) = \\frac{1 - \\left(\\frac{1-p}{p}\\right)^s}{1 - \\left(\\frac{1-p}{p}\\right)^N}\n\\]\nAnd‚Ä¶ well, that‚Äôs basically it. Winnig all the time doesn‚Äôt require any creative policies.\n\n\n4.4.1.8 floating point imprecissions\nThere‚Äôs one interesting thing about the gambler‚Äôs fortune case. The high win probabilities can lead to floating point imprecisions.\nWhen we run value iteration with \\(N = 100\\) and \\(p_\\mathrm{win} = 0.6\\), these imprecisions can bleed into the computed optimal policy.\n\nCode\nenv = EnvGamblersProblem(p_win=0.6, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nplot_value_function([(\"optimal value function\", v_star)])\nbest_actions = get_greedy_actions(v_star, env, 1e-10)\nplot_multi_policy([MultiPolicy(best_actions)], title=\"bla\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe value function approaches \\(1\\) very quickly. That means from a certain point onward, the \\(q(s, a)\\) values are all indistinguishably close to \\(1\\)‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶which leads to floating point imprecisions during optimal action selection. The code ends up including some sub-optimal actions.\n\n\n\nTwo thoughts on this:\n\nPractically speaking, this isn‚Äôt really a problem. When all candidate actions yield a value practically indistinguishable from the max, it doesn‚Äôt matter which one we take. They‚Äôre all effectively optimal.\nTheoretically, it‚Äôs a useful reminder: a computation is not a proof. It can return wrong answers. Our algorithms produce approximations of the optimal policy, and here it‚Äôs harmless, but we should keep it in mind.\n\nThis kind of issue always arises when the \\(q\\)-values are close together. You can also see it with very low win probabilities:\n\n\nCode\nenv = EnvGamblersProblem(p_win=0.01, goal=128)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-8)\nplot_multi_policy([MultiPolicy(best_actions)], title=r\"$p_\\mathrm{win} = 0.01$ and $N=100$\")\n\n\n\n\n\n\n\n\n\nNote: For illustration, I‚Äôve deliberately used ‚Äòlarge‚Äô \\(\\varepsilon\\) values (\\(10^{-10}\\) for \\(p_\\mathrm{win} = 0.6\\) and \\(10^{-8}\\) for \\(p_\\mathrm{win} = 0.01\\)) when deciding which actions to treat as equally good, that is, if \\(|q(s,a) - q(s,a')| &lt; \\varepsilon\\), we consider both actions equally good.\n\nExercise 4.8 Why does the optimal policy for the gambler‚Äôs problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?\n\n\nSolution 4.8. As discussed in Section 4.4.1.5, there is no single optimal policy.\nFor capital 50, there‚Äôs only one optimal action - bet it all. This reflects the general strategy of bold play, which intuitively limits the number of steps (and thus the compounding risk) needed to reach the goal.\nFor capital 51, though, there are two optimal actions: continue being bold (bet 49), or - as shown in the policy from Sutton and Barto (2018) - just bet 1.\nThat both are equally good and optimal follows from the maths. But in my opinion, any simple intuitive explanation is just a posteriori justification of the mathematical facts.\n\n\nExercise 4.9 Implement value iteration for the gambler‚Äôs problem and solve it for \\(p_\\mathrm{win} = 0.25\\) and \\(p_{\\mathrm{win}} = 0.55\\). In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital of \\(0\\) and \\(100\\), giving them values of \\(0\\) and \\(1\\) respectively. Show your results graphically, as in Figure¬†4.2. Are your results stable as \\(\\theta \\to 0\\)?\n\n\nSolution 4.9. Here are the computed solutions - both the state-value functions and the optimal policies. For the policies, we show all optimal actions.\nThere are no surprises here; everything behaves as previously discussed.\n\nCode\nenv = EnvGamblersProblem(p_win=0.25, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-10)\nplot_value_function([(\"optimal value function\", v_star)], title=r\"optimal value function for $p_\\mathrm{win} = 0.25$ and $N=100$\")\nplot_multi_policy(\n    [MultiPolicy(best_actions)], title=r\"Optimal actions for $p_\\mathrm{win} = 0.25$\"\n)\n\nenv = EnvGamblersProblem(p_win=0.55, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-12)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-10)\nplot_value_function([(\"optimal value function\", v_star)], title=r\"optimal value function for $p_\\mathrm{win} = 0.55$ and $N=100$\")\nplot_multi_policy(\n    [MultiPolicy(best_actions)], title=r\"$Optimal actions for p_\\mathrm{win} = 0.55$\"\n)\n\n\n\n\n\n\n\nState value function for \\(p_\\mathrm{win} = 0.25\\) and \\(N = 100\\)\n\n\n\n\n\n\n\nOptimal actions for \\(p_\\mathrm{win} = 0.25\\) and \\(N = 100\\)\n\n\n\n\n\n\n\n\n\nState value function for \\(p_\\mathrm{win} = 0.55\\) and \\(N = 100\\)\n\n\n\n\n\n\n\nOptimal actions for \\(p_\\mathrm{win} = 0.55\\) and \\(N = 100\\)\n\n\n\n\n\n\nAll used \\(\\theta = 10^{-10}\\)\n\n\n\nThese solutions are not stable as \\(\\theta \\to 0\\) (pushing \\(\\theta &lt; 10^{-18}\\) seems to be numerically fragile).\nIf we go that low, we get residual differences in the values, which can make some optimal actions appear non-optimal:\n\n\nCode\nenv = EnvGamblersProblem(p_win=0.25, goal=100)\nvalue_functions = value_iteration_gamblers_problem(env, 1e-18)\nv_star = value_functions[-1]\nbest_actions = get_greedy_actions(v_star, env, 1e-16)\nplot_multi_policy(\n    [MultiPolicy(best_actions)], title=r\"$p_\\mathrm{win} = 0.25$ and $N=100$ with $\\theta = 10^{-18}\"\n)\n\n\n\n\n\n\n\n\n\nThat said, unlike the issues in Section 4.4.1.8, this doesn‚Äôt lead to wrong policies - they still contain only optimal actions.\n\n\nExercise 4.10 What is the analog of the value iteration update Equation¬†4.5 for action values, \\(q_{k+1}(s, a)\\)?\n\n\nFrom the greedy policy update: \\[\n\\pi_k(s) = \\mathrm{argmax}_a q_k(s,a)\n\\] and the action-value Bellman update: \\[\nq_{k+1}(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma Q(s', \\pi_k(s'))]\n\\] we get the action-value version of the value iteration update: \\[\nq_{k+1}(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\max_{a'} Q(s',a')]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#asynchronous-dynamic-programming",
    "href": "chapters/04-dynamic-programming.html#asynchronous-dynamic-programming",
    "title": "4¬† Dynamic Programming",
    "section": "4.5 Asynchronous Dynamic Programming",
    "text": "4.5 Asynchronous Dynamic Programming\nNothing to add here,",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#generalized-policy-iteration",
    "href": "chapters/04-dynamic-programming.html#generalized-policy-iteration",
    "title": "4¬† Dynamic Programming",
    "section": "4.6 Generalized Policy Iteration",
    "text": "4.6 Generalized Policy Iteration\nnor here,",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#efficiency-of-dynamic-programming",
    "href": "chapters/04-dynamic-programming.html#efficiency-of-dynamic-programming",
    "title": "4¬† Dynamic Programming",
    "section": "4.7 Efficiency of Dynamic Programming",
    "text": "4.7 Efficiency of Dynamic Programming\nor here,",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#summary",
    "href": "chapters/04-dynamic-programming.html#summary",
    "title": "4¬† Dynamic Programming",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nand not even here.\n\n\n\n\nSiegrist, Kyle. 2023. ‚ÄúProbability, Mathematical Statistics, Stochastic Processes.‚Äù https://www.randomservices.org/random/index.html.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/04-dynamic-programming.html#footnotes",
    "href": "chapters/04-dynamic-programming.html#footnotes",
    "title": "4¬† Dynamic Programming",
    "section": "",
    "text": "They use a continuous version of the gambler‚Äôs problem, but the continous value function \\(F\\) can be translated by \\(v_\\ast(s) = F(\\frac{x}{N})\\). That‚Äôs because the optimal strategy, ‚Äòbold play‚Äô, can also be used in the discrete caes.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html",
    "href": "chapters/05-monte-carlo-methods.html",
    "title": "5¬† Monte Carlo Methods",
    "section": "",
    "text": "5.1 Monte Carlo Prediction\nIn this chapter ‚ÄòMonte Carlo‚Äô means sampling complete episodes. Monte Carlo methods have the following properties:\nFor prediction, we have two variants of Monte Carlo methods:\nBoth variants converge to \\(v_\\pi(s)\\) if every \\(s\\) is visited infinitely often. The algorithm is stochastic, that‚Äôs why we loop forever in the pseudocode to get asymptotic guarantee. In real code, we have to have some stop condition. For example, a simple ‚Äúhigh number of iterations and then fingers crossed‚Äù.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#monte-carlo-prediction",
    "href": "chapters/05-monte-carlo-methods.html#monte-carlo-prediction",
    "title": "5¬† Monte Carlo Methods",
    "section": "",
    "text": "every-visit MC: maybe more intuitive but theoretical harder to analyse.\nfirst-visit MC: seems a bit wasteful at first but returns are independent. It can be beneficial to count a state only once, otherwise huge visit counts to a state in a single episode can have big impacts on the value estimate.\n\n\n\n\n\nListing¬†5.1: First-visit MC prediction, for estimating \\(V \\approx v_\\pi\\)\n\n\nInput: \\(\\pi\\), the policy to be evaluated\nInitialisation: \\(V(s)\\), for all \\(s \\in \\mathcal{S}\\) arbitrarily\n¬†¬†¬†¬†\\(\\mathrm{Returns}(S_t) \\gets\\) empty list, for all \\(s \\in \\mathcal{S}\\)\n\nLoop forever:\n¬†¬†¬†¬†Generate an episode following \\(\\pi\\): \\(S_0,A_0, R_1, S_1, A_1, \\dots, R_T, S_T\\)\n¬†¬†¬†¬†\\(G \\gets 0\\)\n¬†¬†¬†¬†Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(G \\gets \\gamma G + R_{t+1}\\)\n¬†¬†¬†¬†¬†¬†¬†¬†If \\(S_t\\) is not in \\(\\{S_0,S_1, \\dots, S_{t-1}\\}\\):\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†Append \\(G\\) to \\(\\mathrm{Returns}(S_t)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(V(S_t) \\gets \\mathrm{average}(\\mathrm{Returns}(S_t))\\)\n\n\n\n\nExample 5.1 This is Example 5.1: Blackjack from Sutton and Barto (2018).\nGet your infinite deck ready, we‚Äôre heading to the casinos.\nHere is a not-so-short summary of the Blackjack variant used in the source. I tried to keep it concise but also precise, the devil lives in the details here.\n\nplayer competes against the dealer\nboth parties collect cards and if one‚Äôs hand value exceeds 21 they automatically lose the game (go bust).\nthe deck is ‚Äúinfinite‚Äù, i.e., the chances for any card are always like in a freshly shuffled deck.\nthe numbered cards go from 1 to 10, the value of face cards is 10 except ace which is valued as 1 or 11 if that doesn‚Äôt result in a bust.\ngame begins with two cards dealt to the player and dealer each. The player can see the first card of the dealer but not the second.\nright after the hands have been dealt, if the player or the dealer have 21 they have a natural and immediately win, or it‚Äôs a draw if both have 21.\nif there are no naturals the player can hit‚Äîrequest a new card‚Äîuntil he sticks‚Äîstop and let the dealer collect cards.\nthe dealer plays to this fixed strategy, they will hit when they have less than 17, and stick otherwise.\nif neither the player nor the dealer go bust during their turns, the party with the highest total wins, or it‚Äôs a draw.\n\nThe rule about naturals that I use here seems to be different from what‚Äôs stated in Sutton and Barto (2018) which reads as ‚Äúthe dealer‚Äôs second card is only checked if the player has a natural‚Äù. However, I don‚Äôt know which rule they used in their implementation though. This is actually an important point for the distribution of the cards. Following ‚Äòmy‚Äô rules, if the game has properly started‚Äîno party has a natural‚Äîand the dealer‚Äôs showing card is an ace, it is guaranteed that the dealer‚Äôs second card‚Äîthe one face down‚Äîwill not be a 10.\nThis is how we set up Blackjack as an MDP:\n\nEach game is an episode with reward: +1 (win), -1 (loss), 0 (draw).\nThe state is represented as \\[\n(\\text{player's total}, \\text{player has usable ace}, \\text{dealer's showing card}),\n\\] where a usable ace means that the hand has one ace that counts as 11 (soft hand).\nW.l.o.g. we can assume that the player‚Äôs total is at least 12 as for any value below, there is no chance to go bust and we disregard any incredibly stupid policies that would stick in these kind of situations.\nAll that stuff about naturals are not part of the dynamics of the MDP, the game basically ends before it has properly started. They influence the starting states though, as described above.\n\nThe strategy we want to evaluate using MC prediction is quite greedy. The agent only sticks on a 20 or 21 otherwise hits. I implemented the Blackjack environment separately and just use it here.\n\n# === a quite greedy Blackjack policy ===\nfrom scripts.environment.black_jack import BlackJack, Action, State\n\nenv = BlackJack()\n\nœÄ_at_least_20 = {}\nfor state in env.state_space:\n    if state.player_total &lt; 20:\n        œÄ_at_least_20[state] = Action.HIT\n    else:\n        œÄ_at_least_20[state] = Action.STICK\n\nNow let‚Äôs set up the code for Listing¬†6.1. We need to make a few changes:\n\nInclude a stop condition‚Äîa simple step counter.\nCalculate the averages for the state-value approximation just once at the end.\nTo implement first visit, start with a count of all the states. Then, for each \\(S_t\\), decrease the counter for that state until it‚Äôs one, then we know this is the first visit.\nI added a \\(R_0\\) which corresponds to naturals (episode starts in a terminal state). However, we ignore these as we only calculate rewards for non-terminal states. This means a fraction of the generated episodes won‚Äôt give any experience.\n\n\n# === MC prediction for Blackjack ===\nfrom collections import Counter, defaultdict\nimport statistics\nimport random\n\nfrom scripts.environment.black_jack import BlackJack, Action, State, Reward\n\n\ndef generate_episode(env, œÄ) -&gt; tuple[list[State], list[Reward], list[Action]]:\n    states = []  # S_0, ..., S_T\n    rewards = []  # R_0, ..., R_T\n    actions = []  # A_0, ..., A_{T-1}\n\n    state, reward, terminated = env.reset()\n\n    states.append(state)\n    rewards.append(reward)\n\n    while not terminated:\n        action = œÄ[state]\n        actions.append(action)\n\n        state, reward, terminated = env.step(action)\n\n        states.append(state)\n        rewards.append(reward)\n\n    return states, rewards, actions\n\n\ndef mc_prediction(env, œÄ, n_episodes):\n    returns = defaultdict(list)\n\n    for _ in range(n_episodes):\n        states, rewards, actions = generate_episode(env, œÄ)\n\n        G = 0.0\n        visit_counter = Counter(states)\n        for i in reversed(range(len(actions))):\n            G += rewards[i + 1]\n            state = states[i]\n\n            if visit_counter[state] == 1:\n                returns[state].append(G)\n            else:\n                visit_counter[state] += -1\n\n    V = {}\n    for state in env.state_space:\n        if len(returns[state]) != 0:\n            V[state] = statistics.mean(returns[state])\n\n    return V\n\n\niterations = 1  # todo: this should be 5_000_000\nvalue = mc_prediction(env, œÄ_at_least_20, iterations)\n\nHere we can see the state-value function as heatmap.\n\nCode\n## plotting\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# def plt_grid_for_blackjack\n# Prepare axes\nplayer_sums = np.arange(12, 22)  # 12‚Äì21\ndealer_cards = np.arange(2, 12)  # 2‚Äì11 (11==Ace)\n\n\n# Helper to build a matrix for usable_ace = True/False\ndef build_grid(value_dict, usable_ace_flag):\n    grid = np.zeros((len(player_sums), len(dealer_cards)))\n    for i, ps in enumerate(player_sums):\n        for j, dc in enumerate(dealer_cards):\n            # Create a State object with the current parameters\n            state = State(\n                player_total=ps,\n                is_soft=usable_ace_flag,\n                dealer_revealed=dc,\n            )\n            # Look up the value using the State object\n            grid[i, j] = value_dict.get(state, np.nan)\n    return grid\n\n\n# Create the two grids\ngrid_no_ace = build_grid(value, False)\ngrid_with_ace = build_grid(value, True)\n\n# Plot with usable ace\nplt.figure()\nplt.imshow(\n    grid_with_ace, origin=\"lower\", aspect=\"auto\", vmin=-1, vmax=1, cmap=\"coolwarm\"\n)\n\nfor i in range(grid_no_ace.shape[0]):\n    for j in range(grid_no_ace.shape[1]):\n        plt.text(\n            j,\n            i,\n            f\"{grid_with_ace[i, j]:.2f}\",\n            ha=\"center\",\n            va=\"center\",\n            color=\"black\",\n            fontsize=6,\n        )\n\n\nplt.colorbar()\nplt.xticks(ticks=np.arange(len(dealer_cards)), labels=dealer_cards)\nplt.yticks(ticks=np.arange(len(player_sums)), labels=player_sums)\nplt.xlabel(\"Dealer's Showing Card\")\nplt.ylabel(\"Player's Total\")\nplt.title(f\"MC Value Function (With Usable Ace), {iterations:,} Iterations\")\n\nplt.tight_layout()\nplt.show()\n\n# Plot 1: no usable ace\nplt.figure()\nplt.imshow(grid_no_ace, origin=\"lower\", aspect=\"auto\", vmin=-1, vmax=1, cmap=\"coolwarm\")\n\nfor i in range(grid_no_ace.shape[0]):\n    for j in range(grid_no_ace.shape[1]):\n        plt.text(\n            j,\n            i,\n            f\"{grid_no_ace[i, j]:.2f}\",\n            ha=\"center\",\n            va=\"center\",\n            color=\"black\",\n            fontsize=6,\n        )\n\nplt.colorbar()\nplt.xticks(ticks=np.arange(len(dealer_cards)), labels=dealer_cards)\nplt.yticks(ticks=np.arange(len(player_sums)), labels=player_sums)\nplt.xlabel(\"Dealer's Showing Card\")\nplt.ylabel(\"Player's Total\")\nplt.title(f\"MC Value Function (No Usable Ace), {iterations:,} Iterations\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.1: This is like Figure 5.1 from Sutton and Barto (2018). It shows the approximate state-value function for the Blackjack policy that sticks only for 20 or 21, computed by MC policy evaluation.\n\n\n\n\n\nExercise 5.1 Consider the diagrams in Figure¬†5.1. Why does the estimated value function jump up for the last two rows in the top1? Why does it drop off for dealer‚Äôs showing card is 11?2 Why are the lowermost values higher in the upper diagrams than in the lower?\n\n\nSolution 5.1. Let‚Äôs answer each question separately:\n\nthe jump at the top: this come from the fact that for 20 or 21 the player sticks and for those values the odds of beating the dealer are quite high. The lower values are all negative meaning, that the player is likely to loose because ‚Äì I think ‚Äì with this strategy the agent is quite likely to go bust. The negative values indicate the expected loss for the player, not just that the player is likely to lose.\nthis drop off that is not visible in my diagram: in the original diagram the dropoff is between ‚ÄúDealer‚Äôs showing card‚Äù is 2 and Ace. But there is no drop off visible in my diagram. I have the theory that their code does not check for dealer naturals which would make a dealer‚Äôs showing ace more dangerous.\nAll cell values in the upper diagram are higher for player‚Äôs total less than 20 as the agent has a better chance to reach 20 or 21 without going bust with an ace up its hand. The rows for player‚Äôs total of 20 and 21 should return the same values as the agent chooses to stick in both cases, and the expected outcome is the same.\n\n\n\nExercise 5.2 Suppose every-visit MC was used instead of first-visit MC on the blackjack task. Would you expect the results to be very different? Why or why not?\n\n\nSolution 5.2. The results would be exactly the same as there is no way to visit a state a second time in blackjack (player‚Äôs total goes up or usable ace flag becomes false, and usable ace flag can only become true once).\n\n\nExample 5.2 This is example 5.2: Soap Bubble from Sutton and Barto (2018).\nTake a closed wire frame and dip it into soapy water. We do not want to blow any bubbles; we only want to predict the shape the soap film spans over the frame. If the wire frame is not too wild, we can represent it by prescribing a height function \\(h\\) on the boundary of some planar region. The plot below shows how this can look.\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n# --- Parameters and calculations ---\n# Parameters\nradius = 2\noffset = 6\nn = 35  # resolution (should be 200)\np = 35  # patching (should be 200)\n\n# mesh\nx = np.linspace(-radius, radius, n)\ny = np.linspace(-radius, radius, n)\nx, y = np.meshgrid(x, y)\n# disk\nmask = np.sqrt(x**2 + y**2) &lt;= radius\n# circle\ntheta = np.linspace(0, 2 * np.pi, n)\nx_circle = radius * np.cos(theta)\ny_circle = radius * np.sin(theta)\n# outline and film height\nz_circle = x_circle**2 - y_circle**2\nz = np.where(mask, x**2 - y**2, np.nan)\n\n\n# --- Create figure ---\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# outline shadow\nax.contourf(\n    x,\n    y,\n    np.where(mask, 0, np.nan),\n    zdir=\"z\",\n    alpha=0.3,\n)\n# wire\nax.plot(\n    x_circle,\n    y_circle,\n    z_circle + offset,\n    color=\"black\",\n    linewidth=2.5,\n)\n# soap film\nsurf = ax.plot_surface(\n    x,\n    y,\n    z + offset,\n    cmap=\"prism\",\n    linewidth=0,\n    alpha=0.75,\n    antialiased=True,\n    shade=False,\n    rcount=p,\n    ccount=p,\n)\n\n\n# no tick marks\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nax.set_zlabel(\"\")\n\nax.view_init(elev=20, azim=110)\nax.set_title(\"A soapy Pringle\", pad=20)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.2: This shows a Pringle-shaped soap film. The black outline is the wire frame; the rainbow surface is the film itself. The figure is modelled by a height function on the grey disk in the \\(xy\\)-plane. The underlying physics in Sutton and Barto (2018), which I used for this plot, is not quite right, so this is only an approximation.\n\n\n\n\nThe question is: given the height on the boundary, what is the height \\(h\\) at interior points? Sutton and Barto (2018) set up the problem like this:\n\nSuppose a wire frame forming a closed loop is dipped into soapy water to form a soap surface or bubble conforming to the wire frame at its edges. If the geometry of the wire frame is irregular but known, how can you compute the shape of the surface? The shape has the property that the total force on each point exerted by neighboring points is zero (or else the shape would change). This means that the height at any point is the average of the heights at points in a small circle around that point.\n\nThat last sentence as stated is not correct for soap films. It is true for elastic sheets like rubber, for which the restoring force is proportional to local displacement (to first order). In that case, the height function \\(h\\) satisfies the mean-value property: its value at a point equals asymptotically the average of values on small circles around that point. This is equivalent to \\(h\\) being a harmonic function: \\[\n\\Delta h = \\frac{\\partial^2 h}{\\partial x^2} + \\frac{\\partial^2 h}{\\partial y^2} = 0.\n\\tag{5.1}\\]\nSoap films differ. Their molecules do not ‚Äústretch out.‚Äù What contracts is surface tension, which makes them shrink into minimal surfaces. For more information about the physics behind this, I recommend Oprea (2000). Here, I will state the relevant result: A soap surface given as the graph \\(z = h(x, y)\\) satisfies the nonlinear minimal-surface equation (Oprea 2000, Proposition 3.2.3) \\[\nh_{xx}(1 + h_y^2) - 2h_x h_y h_{xy} + h_{yy}(1 + h_x^2) = 0,\n\\tag{5.2}\\] where \\(h_x, h_{xy}, \\dots\\) are the partial derivatives \\(\\frac{\\partial h}{\\partial x}, \\frac{\\partial^2 h}{\\partial x \\partial y}\\), etc.\nThis is generally not equivalent to being harmonic, so the mean-value property does not hold for a soap film in general. However, there is an approximation. When higher powers of the partial derivatives are small, a soap film is well approximated by a harmonic function (Oprea 2000, Exercise 4.4.9). This is why the picture in Figure¬†5.2 ‚Äî which simply plots the harmonic function \\(x^2 - y^2\\) inside a disk ‚Äî is only an approximation and not an exact minimal surface. Because of this distinction, I replace the ‚Äúsoap film problem‚Äù with the ‚Äúrubber sheet problem‚Äù in these notes.\nWe discretize the equation for harmonic functions so we can solve it numerically. On a rectangular grid, we enforce the discrete mean-value property at interior nodes: for each interior node \\((x, y)\\), \\[\nh(x, y) = \\frac{1}{4} \\sum_{(x', y') \\in B(x, y)} h(x', y'),\n\\] where \\(B(x, y) = \\{(x', y'): |x - x'| + |y - y'| = 1\\}\\).\nEquivalently, \\(h\\) at an interior site equals the expected boundary value reached by a simple (nearest-neighbor) random walk started at that site. Thus, we can use Monte Carlo methods for solving the rubber sheet problem: run many independent random walks from \\((x, y)\\) until they first hit the boundary, record the boundary value where each walk exits, and average those values to estimate \\(h(x, y)\\).\nAs a concrete example we‚Äôll take this \\(7\\times7\\) grid with radius \\(3\\):\n\n# === the rubber sheet boundary ===\n# -3 -2 -1  0  1  2  3\n# -3                 3\n# -3                 3\n# -3        x        3\n# -3                 3\n# -3                 3\n# -3 -2 -1  0  1  2  3\nradius = 3\nboundary = np.zeros((2 * radius + 1, 2 * radius + 1))\nboundary_horizontal = np.arange(-radius, radius + 1)\nboundary[0, :] = boundary_horizontal  # top edge\nboundary[-1, :] = boundary_horizontal  # bottom edge\nboundary[:, 0] = -radius  # left edge\nboundary[:, -1] = radius  # right edge\ncenter = (3, 3)\n\nboundary\n\narray([[-3., -2., -1.,  0.,  1.,  2.,  3.],\n       [-3.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [-3.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [-3.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [-3.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [-3.,  0.,  0.,  0.,  0.,  0.,  3.],\n       [-3., -2., -1.,  0.,  1.,  2.,  3.]])\n\n\nBy symmetry the true centre value is \\(0\\). Let‚Äôs see how close a limited Monte-Carlo simulation gets to the true value. First we set up the random-walk sampler that returns the height of the exit cell:\n\nimport random\nfrom enum import Enum, auto\n\n\nclass Direction(Enum):\n    UP = auto()\n    DOWN = auto()\n    LEFT = auto()\n    RIGHT = auto()\n\n\nMOVE_VECTORS = {\n    Direction.UP: (-1, 0),\n    Direction.DOWN: (1, 0),\n    Direction.LEFT: (0, -1),\n    Direction.RIGHT: (0, 1),\n}\n\n\ndef not_on_boundary(i, j, n_rows, n_cols):\n    return i &gt; 0 and i &lt; n_rows - 1 and j &gt; 0 and j &lt; n_cols - 1\n\n\ndef single_walk(grid, start_point, rng):\n    directions = list(Direction)\n\n    i, j = start_point\n    n_rows, n_cols = grid.shape\n\n    while not_on_boundary(i, j, n_rows, n_cols):\n        direction = rng.choice(directions)\n        di, dj = MOVE_VECTORS[direction]\n        i += di\n        j += dj\n\n    return grid[i, j]\n\n\ndef monte_carlo_rubber_sheet(grid, point, n_samples, rng):\n    return [single_walk(grid, point, rng) for _ in range(n_samples)]\n\nAnd now run and average the samples:\n\nrng = np.random.default_rng(0)\n\nn_samples = 1000\nsamples = monte_carlo_rubber_sheet(boundary, center, n_samples, rng)\nprint(f\"MC estimate after {n_samples} samples: {sum(samples)/n_samples}\")\n\nMC estimate after 1000 samples: 0.017\n\n\nThis is an okay approximation. Note that for Monte Carlo, 1000 runs is not much as it‚Äôs inherently slow. One way to assess the quality of the method is by examining the standard error, which is the standard deviation of the estimator itself, i.e., the Monte Carlo simulation considered as a random variable (for a fixed sample size \\(n\\)). We can see the standard error of this estimator in action if we run the upper experiment a couple of times. It‚Äôs quite high, so the estimates fluctuate a lot.\n\n\nCode\nfor i in range(5):\n    samples = monte_carlo_rubber_sheet(boundary, center, n_samples, rng)\n    print(f\"MC estimate after {n_samples} samples: {sum(samples)/n_samples}\")\n\n\nMC estimate after 1000 samples: -0.039\nMC estimate after 1000 samples: -0.006\nMC estimate after 1000 samples: -0.032\nMC estimate after 1000 samples: -0.045\nMC estimate after 1000 samples: 0.058\n\n\nWhat I mean by ‚ÄòMonte Carlo is slow‚Äô is that the standard error typically scales with \\(1/\\sqrt{n}\\). That is, if we want to cut the standard error in half, we have to run four times more simulations. I go more into detail about this in appendix Section 5.8.1.\nThere are two final remarks I want to make about Monte Carlo in general:\n\nA single value without error bounds is usually not really usable. A simple way to get a priori assurances is using confidence intervals. I discuss them quickly in the appendix Section 5.8.2.\nA clever way to decrease the standard error is by importance sampling, which will is also crucial for off-policy learning discussed in Section 5.5.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#monte-carlo-estimation-of-action-values",
    "href": "chapters/05-monte-carlo-methods.html#monte-carlo-estimation-of-action-values",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.2 Monte Carlo Estimation of Action Values",
    "text": "5.2 Monte Carlo Estimation of Action Values\nOne way to estimate the action-values is to view each state as its own little armed-bandit where we choose one action and obtain a reward according to the policy. For this we would need exploring starts, the ability to start the episode in any state action pair. This view misses one point though, that by generating an episode starting from one given state-action pair we see the rewards for all state-action pairs that follow.\n\nExercise 5.3 What is the backup diagram for Monte Carlo estimation of \\(q_\\pi\\)?\n\n\nSolution 5.3. I‚Äôm not 100% sure about this one. I think it‚Äôs just the backup diagram for \\(v_\\pi\\) but starting at an action node. The unsure part for me is that \\(q_\\pi\\) depends on an action and a state, so the state must be understood implicitly in the backup diagram. We could also demand that theoretically the actions for each state are distinct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#monte-carlo-control",
    "href": "chapters/05-monte-carlo-methods.html#monte-carlo-control",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.3 Monte Carlo Control",
    "text": "5.3 Monte Carlo Control\nHere is the algorithm for the first-visit Monte Carlo control algorithm with exploring starts\n\n\n\nListing¬†5.2: Monte Carlo ES (Exploring Starts), for estimating \\(\\pi \\approx \\pi_\\ast\\)\n\n\nInitialisation:\n¬†¬†¬†¬†\\(\\pi(s) \\in \\mathcal{A}(s)\\), for all \\(s \\in \\mathcal{S}\\) arbitrarily\n¬†¬†¬†¬†\\(Q(s,a) \\in \\mathbb{R}\\), for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\) arbitrarily\n¬†¬†¬†¬†\\(\\mathrm{Returns}(s,a) \\gets \\mathrm{empty list}\\), for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\)\n\nLoop forever:\n¬†¬†¬†¬†Choose \\(S_0 \\in \\mathcal{S}, A_0 \\in \\mathcal{A}(S_0)\\) randomly such that all pairs have probability \\(&gt; 0\\)\n¬†¬†¬†¬†Generate an episode following \\(\\pi\\): \\(S_0,A_0, R_1, S_1, A_1, \\dots, R_T, S_T\\)\n¬†¬†¬†¬†\\(G \\gets 0\\)\n¬†¬†¬†¬†Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(G \\gets \\gamma G + R_{t+1}\\)\n¬†¬†¬†¬†¬†¬†¬†¬†If \\((S_t, A_t) \\not\\in \\{(S_0,A_0), \\dots, (S_{t-1},A_{t-1})\\}\\):\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†Append \\(G\\) to \\(\\mathrm{Returns}(S_t,A_t)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(Q(S_t,A_t) \\gets \\mathrm{average}(\\mathrm{Returns}(S_t,A_t))\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(\\pi(S_t) \\gets \\arg \\max_a Q(S_t,a)\\)\n\n\n\nIt‚Äôs actually quite ingenious how this algorithm circumvents the problem that MC prediction Listing¬†6.1 does not terminate, by letting the approximate \\(Q\\) be a blend of all returns from all policies the algorithm has passed through so far. On the other hand, in my experience this algorithm tends to live in asymptopia‚Äîit can take an unpractical large number of episodes to converge to the optimal policy. In the practical world it can also result in rather flip-floppy behaviour. For example, in Example¬†5.3 there are actions with low signal-to-noise ratios: their \\(q\\)-values are close and have high variance. In this case the algorithm will often switch rapidly between these action values. It will eventually, in the limit‚Äîwhenever that may be‚Äîsettle on the correct one, but for solving Blackjack I couldn‚Äôt get there by sheer brute force. For obtaining good enough policies this is not a real problem‚Äîwhen the \\(q\\)-values are close, it is largely unimportant which one gets chosen.\nThe general method also suffers from the usual problems that come with MC methods. When should we stop? How do we know that we have enough episodes? This is a substantial question in its own right, but we‚Äôre dealing with the foundations here, so we‚Äôll leave it for now.\n\nExercise 5.4 The pseudocode for Monte Carlo ES is inefficient because, for each state-action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more efficient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state‚Äìaction pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this.\n\n\nSolution 5.4. In the initialsation we don‚Äôt need \\(\\mathrm{Returns}\\) any more and use instead\n\n¬†¬†¬†¬†\\(\\mathrm{Count}(s,a) \\gets 0\\)\n¬†¬†¬†¬†\\(\\mathrm{Avg}(s,a) \\gets 0.0\\)\n\nand in the last lines instead of appending \\(G\\) to \\(\\mathrm{Returns}\\) and calculate the average we do this\n\n¬†¬†¬†¬†\\(\\mathrm{Count}(S_t,A_t) \\gets \\mathrm{Count}(S_t,A_t) + 1\\)\n¬†¬†¬†¬†\\(\\mathrm{Avg}(S_t,A_t) \\gets \\mathrm{Avg}(S_t,A_t) + \\frac{G - \\mathrm{Avg}(S_t,A_t)}{\\mathrm{Count}(S_t,A_t)}\\)\n\n\n\n\nExample 5.3 This is example 5.3: Solving Blackjack from Sutton and Barto (2018).\nNow, we attempt to find the optimal policy for blackjack from Example¬†5.1. Spoiler alert: we won‚Äôt find it here. Using crude Monte Carlo methods would require an unreasonable huge amount of runs to find the optimal strategy. What we can do, however, is find a good strategy and discuss how to verify the optimal strategy from Sutton and Barto (2018).\nLet‚Äôs dive into the implementation. The code is nearly a verbatim copy of the algorithm given in Listing¬†5.2, with two changes:\n\nthe code uses the changes from Exercise¬†5.4 for updating the averages of the \\(Q\\)-values.\nthe code uses every-visit Monte Carlo, unlike the algorithm which is given for first-visit Monte Carlo. However, this doesn‚Äôt matter here as Blackjack is acyclic.\n\n\nimport random\nfrom collections import Counter\nimport scripts.environment.black_jack as black_jack\n\n\ndef generate_episode_es(env, œÄ, start_state, start_action):\n    \"\"\"Generate an episode starting from start_state and start_action following policy œÄ.\"\"\"\n    states = [start_state]  # S_0, ..., S_T\n    rewards = [env.set_state(start_state)]  # R_0, ..., R_T\n    actions = [start_action]  # A_0, ..., A_{T-1}\n\n    # take start action\n    state, reward, terminated = env.step(start_action)\n    states.append(state)\n    rewards.append(reward)\n\n    # follow policy\n    while not terminated:\n        action = œÄ[state]\n        actions.append(action)\n\n        state, reward, terminated = env.step(action)\n\n        states.append(state)\n        rewards.append(reward)\n\n    return states, rewards, actions\n\n\ndef mc_es(env, n_episodes, rng):\n    \"\"\"Monte Carlo control with Exploring Starts.\n\n    Returns:\n        œÄ: The learned policy.\n        q: The learned action-value function.\n    \"\"\"\n    env.rng = rng\n\n    q = {\n        (state, action): rng.random()\n        for state in env.state_space\n        for action in env.action_space\n    }\n    œÄ = {\n        state: max(env.action_space, key=lambda action: q[(state, action)])\n        for state in env.state_space\n    }  # this essentially chooses œÄ randomly\n    c = {(state, action): 0 for state in env.state_space for action in env.action_space}\n\n    for _ in range(n_episodes):\n        # make an episode\n        start_state = rng.choice(env.state_space)\n        start_action = rng.choice(env.action_space)\n\n        states, rewards, actions = generate_episode_es(\n            env, œÄ, start_state, start_action\n        )\n\n        # every visit\n        G = 0\n        state_actions = [(states[i], actions[i]) for i in range(len(actions))]\n        for i in reversed(range(len(actions))):\n            G += rewards[i + 1]\n            sa = state_actions[i]\n\n            # update q\n            c[sa] += 1\n            q[sa] += (G - q[sa]) / c[sa]\n\n            # update œÄ\n            state = sa[0]\n            œÄ[state] = max(env.action_space, key=lambda a: q[(state, a)])\n\n    return œÄ, q\n\nSo let‚Äôs run it!\n\nCode\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niterations = 1_000_000  # should be 1,000,000\nrng = random.Random(0)\nenv = black_jack.BlackJack()\nœÄ, q = mc_es(env, iterations, rng)\n\n# --- Plotting ---\n# Generate data\nplayer_totals = list(range(21, 11, -1))\ndealer_showing = list(range(2, 12))\n\n# Cell text\naction_to_string = {black_jack.Action.HIT: \"H\", black_jack.Action.STICK: \"S\"}\ncell_text_ace, cell_text_no_ace = [], []\nfor pt in player_totals:\n    row = [action_to_string[œÄ[black_jack.State(pt, True, dc)]] for dc in dealer_showing]\n    cell_text_ace.append(row)\n    row = [\n        action_to_string[œÄ[black_jack.State(pt, False, dc)]] for dc in dealer_showing\n    ]\n    cell_text_no_ace.append(row)\n\n# Cell colours\naction_to_facecolor = {\"S\": \"#c6dbef\", \"H\": \"#fdd0a2\"}\ncell_colours_ace = [\n    [action_to_facecolor[element] for element in row] for row in cell_text_ace\n]\ncell_colours_no_ace = [\n    [action_to_facecolor[element] for element in row] for row in cell_text_no_ace\n]\n\n# Plot\ncolWidths = [1 / 25] * 10\nscale = 1.5\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\nfig.suptitle(f\"MC control Blackjack strategy\\n(after {iterations:,} iterations)\")\n\n# With Ace\nax1.set_axis_off()\ntbl1 = ax1.table(\n    cellText=cell_text_ace,\n    cellColours=cell_colours_ace,\n    colWidths=colWidths,\n    rowLabels=player_totals,\n    colLabels=dealer_showing,\n    loc=\"center\",\n    cellLoc=\"center\",\n)\ntbl1.scale(scale, scale)\nax1.set_title(\"Usable Ace\")\n\n# Without Ace\nax2.set_axis_off()\ntbl2 = ax2.table(\n    cellText=cell_text_no_ace,\n    cellColours=cell_colours_no_ace,\n    colWidths=colWidths,\n    rowLabels=player_totals,\n    colLabels=dealer_showing,\n    loc=\"center\",\n    cellLoc=\"center\",\n)\ntbl2.scale(scale, scale)\nax2.set_title(\"No usable Ace\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.3: This shows the policy found by MC control for Blackjack. The row labels show the player sum, the column label the dealer‚Äôs showing card, and the cells if the policy‚Äôs action is H(it) or S(tick).\n\n\n\nAfter 1 million episodes, we get a pretty good approximation of the optimal policy shown in Figure 5.2 from Sutton and Barto (2018), but it‚Äôs not quite right. Smoothing out the kinks in the policy would require many more runs.\nEven if we could run the code for much longer, there remains the question of how to ensure that the resulting policy \\(\\pi\\) is optimal. We could use Monte Carlo prediction to estimate the \\(q\\)-values and then check if the policy takes only actions with the highest \\(q\\)-values. However, how can we know if these values are accurate enough? For this, we need a statistical procedure to distinguish the means of action values, even if they are close to each other. Since Monte Carlo prediction can be viewed as a collection of armed bandits, we can use action elimination and stopping conditions for multi-armed bandits to obtain the optimal action. A good reference for the theoretical side can be found in Even-Dar, Mannor, and Mansour (2006).\nI‚Äôve done calculating and verifying the optimal solution in a side project, it‚Äôs the same given by Sutton and Barto (2018). I won‚Äôt go into any more details though, as it‚Äôs only tangent to our aim for finding good policies for complicated environments.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#monte-carlo-control-without-exploring-starts",
    "href": "chapters/05-monte-carlo-methods.html#monte-carlo-control-without-exploring-starts",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.4 Monte Carlo Control without Exploring Starts",
    "text": "5.4 Monte Carlo Control without Exploring Starts\nExploring starts has two problems:\n\nnot always possible\nexplores all possible states-action pairs, even though optimal behaviour might only require exploring a small subset of them.\n\nAnd two ways to get exploration without exploring starts are:\n\nsoft policies\noff-policy prediction\n\nWe discuss soft policies now and off-policy prediction in the next section Section 5.5.\nA policy is \\(\\varepsilon\\)-soft if \\(\\pi(a|s) \\geq \\frac{\\varepsilon}{\\mathcal{A}(s)}\\) for all \\(s \\in \\mathcal{S}\\) and \\(a \\in \\mathcal{A}(s)\\). The role of the deterministic policies in this class is taken by the \\(\\epsilon\\)-greedy policies, policies that are as deterministic as \\(\\varepsilon\\)-soft policies are allowed. We can think of them as deterministic policies behind a randomizer. With probability \\(\\varepsilon\\) the action gets randomly chosen and with probability \\(1-\\varepsilon\\) the deterministic action is taken. This gives every action a probability of \\(\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\) except the greedy action which has a probability of \\(1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\).\nHere is the algorithm for approximating an optimal \\(\\varepsilon\\)-soft policy.\n\n\n\nListing¬†5.3: On-policy first-visit MC control (for \\(\\epsilon\\)-soft policies), estimates \\(\\pi \\approx \\pi_*\\).\n\n\nInput: small \\(\\varepsilon &gt; 0\\)\nInitialisation: \\(\\pi \\gets\\) an arbitrary \\(\\varepsilon\\)-soft policy\n¬†¬†¬†¬†\\(Q(s,a) \\in \\mathbb{R}\\) (arbitrarily), for all \\(s \\in \\mathcal{S}\\), \\(a \\in \\mathcal{A}(s)\\)\n¬†¬†¬†¬†\\(\\mathrm{Returns}(s,a) \\gets\\) empty list, for all \\(s \\in \\mathcal{S}\\), \\(a \\in \\mathcal{A}(s)\\)\n\nLoop forever:\n¬†¬†¬†¬†Generate an episode following \\(\\pi\\): \\(S_0,A_0, R_1, S_1, A_1, \\dots, R_T, S_T\\)\n¬†¬†¬†¬†\\(G \\gets 0\\)\n¬†¬†¬†¬†Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(G \\gets \\gamma G + R_{t+1}\\)\n¬†¬†¬†¬†¬†¬†¬†¬†If \\((S_t,A_t)\\) is not in \\(\\{(S_0,A_0), \\dots, (S_{t-1},A_{t-1})\\}\\):\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†Append \\(G\\) to \\(\\mathrm{Returns}(S_t)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(Q(S_t, A_t) \\gets \\mathrm{average}(\\mathrm{Returns}(S_t,A_t))\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(A^* \\gets \\arg\\max_{a} Q(S_t,a)\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†For all \\(a \\in \\mathcal{A}(S_t)\\):\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(\\pi(a|S_t) \\gets \\begin{cases} 1 - \\varepsilon + \\varepsilon/|\\mathcal{A}(S_t)| \\\\ \\varepsilon/|\\mathcal{A}(S_t)| \\end{cases}\\)\n\n\n\nWe will use this algorithm for Exercise¬†5.12 and also discuss some properties of optimal \\(\\varepsilon\\)-soft policies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#sec-off-policy-prediction-via-importance-sampling",
    "href": "chapters/05-monte-carlo-methods.html#sec-off-policy-prediction-via-importance-sampling",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.5 Off-policy Prediction via Importance Sampling",
    "text": "5.5 Off-policy Prediction via Importance Sampling\nLet‚Äôs try to motivate importance sampling and also see where the word ‚Äòimportance‚Äô comes from through the rubber sheet problem.\n\n5.5.1 üóê Importance Sampling for the Rubber Sheet Problem\nLet‚Äôs return to Example¬†5.2, where we first saw the rubber sheet problem. This time we solve it on the following \\(3\\times 15\\) grid. The goal is to estimate the height at the centre point (marked with an \\(x\\)) using Monte Carlo (MC) simulation.\n\n# === the long rubber sheet problem ===\n# 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n# 1                 x                 1\n# 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nradius_lanky = 7\nboundary_height = 1\nboundary_lanky = np.zeros((3, 2 * radius_lanky + 1))\nboundary_lanky[1, 0] = boundary_height\nboundary_lanky[1, 2 * radius_lanky] = boundary_height\ncenter_lanky = (1, radius_lanky)\n\nBefore running MC, we compute the exact value at the centre so we can judge the estimator‚Äôs accuracy. For the analytical derivation, see Section 5.8.3:\n\n\nCode\n# code-fold: true\ndef g(i):\n    if i == 0:\n        return 1\n    elif i == 1:\n        return 2\n    else:\n        return 4 * g(i - 1) - g(i - 2)\n\n\nprint(f\"The exact value at the center is {boundary_height/g(radius_lanky):.6f}\")\n\n\nThe exact value at the center is 0.000198\n\n\nMC performs poorly on this long, narrow strip. Almost every random walk hits the top or bottom boundary, and only very rarely does one reach the left or right boundary. With \\(1000\\) simulation, the best sample means we could see are:\n\n\\(0\\), if all walks hit the top or bottom;\n\\(1/1000=0.001\\), if exactly one walk hits the left or right.\n\nThe true value is about \\(0.0002\\). So even in the luckiest case, the estimator‚Äôs error is \\(0.0002\\).\nSo here is such a typical run:\n\n\nCode\nrng = np.random.default_rng(0)\nn_samples = 1000\nsamples = monte_carlo_rubber_sheet(boundary_lanky, center_lanky, n_samples, rng)\nf\"MC estimate after {n_samples} samples: {sum(samples)/n_samples}\"\n\n\n'MC estimate after 1000 samples: 0.0'\n\n\nInformally, the method is‚Äîif you‚Äôll excuse my anthropomorphising‚Äîbehaving as if it thought: ‚ÄúIt‚Äôs so unlikely I‚Äôll ever reach the side boundaries that I‚Äôll ignore them and just bounce between the top and bottom.‚Äù\nBut the left and right sides hold important information, while repeatedly sampling the top and bottom gives little new information. We can say that the left and right boundaries are important, and therefore it‚Äôs important to sample them more often.\nThis is the basic idea behind importance sampling: we know that some outcomes (or behaviours) should be sampled more frequently because they contribute more to the expectation we care about. However, if we do sample more frequently from these outcomes, we must correct for this bias in our final calculation.\nLet‚Äôs formalise this for the rubber sheet problem above and bring it into the language of machine learning. We can see random walks on a grid as episodes of an MDP with a deterministic environment. An random walk is an episode \\(\\tau = S_0,A_0,\\dots,S_{T-1},A_{T-1},S_T\\) (we are ignoring rewards for now), where the \\(S_i\\) are the positions in the grid and the directions \\(A_i\\) the chosen actions \\(U,D,L,R\\). \\(S_0\\) is the center position and \\(T\\) is the time of first exist and thus \\(S_T\\) is the first boundary state.\nWe get equidirectional random walks from the uniform random policy \\(\\pi\\) that assigns equal probability to all directions \\(\\pi(U) = \\pi(D) = \\pi(L) = \\pi(R) = 0.25\\) independent of the position. The probability of a particular trajectory \\(\\tau\\) under \\(\\pi\\) is independent of the \\(S_i\\) as the environment is deterministic and the policy is also independent of the state \\[\n\\mathrm{Pr}_{\\pi}(\\tau) = \\pi(A_0) \\cdot \\dots \\cdot \\pi(A_{T-1}).\n\\]\nThe expected exit height is therefore \\[\n\\mathbb{E}_{\\tau \\sim \\pi}[h(\\tau)] = \\sum_{\\tau} h(\\tau) \\cdot \\mathrm{Pr}_{\\pi}(\\tau)\n\\] where \\(h(\\tau) = h(S_T)\\).\nNow, instead of the target policy \\(\\pi\\), suppose we sample using a behaviour policy \\(b\\) which prefers left and right (and is also independent of the states): \\[\nb(R) = b(L) = 0.45, \\quad b(U) = b(D) = 0.05.\n\\]\nHow can we use episodes sampled according to \\(b\\) to estimate \\(\\mathbb{E}_{\\tau \\sim \\pi_t}[h(\\tau)]\\)? We can use a clever trick here: \\[\n\\begin{split}\n\\mathbb{E}_{\\tau \\sim \\pi}[h(\\tau)] &= \\sum_{\\tau} h(\\tau) \\mathrm{Pr}_{\\pi}(\\tau) \\\\\n&= \\sum_{\\tau} h(\\tau) \\frac{\\mathrm{Pr}_\\pi(\\tau)}{\\mathrm{Pr}_b(\\tau)} \\cdot \\mathrm{Pr}_b(\\tau) \\\\\n&= \\mathbb{E}_{\\tau \\sim b}\\left[h(\\tau) \\frac{\\mathrm{Pr}_\\pi(\\tau)}{\\mathrm{Pr}_b(\\tau)}\\right]\n\\end{split}\n\\]\nA simple unbiased estimator given \\(n\\) samples \\(\\tau_i \\sim b\\) is \\[\n\\frac{1}{n}\\sum h(\\tau_i)w(\\tau_i) \\quad \\text{where } w(\\tau_i) = \\frac{\\pi_t(\\tau_i)}{\\pi_b(\\tau_i)}\n\\] where \\(w(\\tau)\\) is called the importance weight.\nNow, let‚Äôs implement importance sampling. First we define a function that returns \\(h(\\tau)w(\\tau)\\) for one episode following \\(b\\):\n\n# === importance sampling single episode ===\ndef single_walk_importance_sampling(\n    grid,\n    start_point,\n    b: dict[Direction, float],\n    rng,\n):\n    \"\"\"Sample œÑ under `b` and return h(œÑ)¬∑w(œÑ)\n    for uniform target œÄ(a)=0.25.\n    \"\"\"\n    directions = list(b.keys())\n    probs = list(b.values())\n\n    # per-action ratios œÄ(a) / b(a); here œÄ(a) = 0.25\n    direction_ratio = {directions[i]: 0.25 / probs[i] for i in range(len(b))}\n\n    i, j = start_point\n    n_rows, n_cols = grid.shape\n    importance_weight = 1\n\n    # walk until a boundary cell is reached\n    while not_on_boundary(i, j, n_rows, n_cols):\n        # sample an action according to b\n        direction = rng.choice(directions, p=probs)\n\n        # move on grid\n        di, dj = MOVE_VECTORS[direction]\n        i += di\n        j += dj\n\n        importance_weight *= direction_ratio[direction]\n\n    # on exit, grid[i, j] = h(s_T)\n    return importance_weight * grid[i, j]\n\nNow we can sample some values and take the average to get an estimate:\n\n# === importance sampling for the long rubber sheet ===\nn_samples = 1000\nrng = np.random.default_rng(0)\n\nb = {\n    Direction.RIGHT: 0.45,\n    Direction.LEFT: 0.45,\n    Direction.UP: 0.05,\n    Direction.DOWN: 0.05,\n}\nsamples = [\n    single_walk_importance_sampling(boundary_lanky, center_lanky, b, rng)\n    for _ in range(n_samples)\n]\nprint(f\"importance sampling after {n_samples} iterations:\")\nprint(np.average(samples))\n\nimportance sampling after 1000 iterations:\n0.0001407157859082321\n\n\nWe can see that importance sampling MC performs better than ‚Äòcrude‚Äô MC‚Äîas a simple MC is called in contrast to importance sampling MC. Importance sampling MC has an error of about \\(0.00006\\) compared to the \\(0.0002\\) from the crude MC before. And generally, we can expect better results from the importance sampling MC due to its lower variance.\nThe error of MC techniques depends only on two factors‚Äîthe standard deviation \\(\\sigma\\) and \\(1/\\sqrt{n}\\) (under the normal approximation that holds for large sample numbers). Importance sampling is a variance reduction technique that gives us an improvement in our estimates without driving up the simulation costs. We can compare the two variances of crude MC and importance sampling for our example:\n\n\nCode\nseed = 0\nn_samples = 1  # todo: 100_000\n\n# crude\nrng = np.random.default_rng(seed)\nsamples_crude = monte_carlo_rubber_sheet(boundary_lanky, center_lanky, n_samples, rng)\nœÉ_crude = np.std(samples_crude)\n\n# importance sampling\nrng = np.random.default_rng(seed)\nsamples_is = [\n    single_walk_importance_sampling(boundary_lanky, center_lanky, b, rng)\n    for _ in range(n_samples)\n]\nœÉ_is = np.std(samples_is)\n\nprint(f\"Estimates for œÉ after {n_samples:,} iterations\")\nprint(f\"crude Monte Carlo œÉ: {œÉ_crude:.5f}\")\nprint(f\"importance sampling œÉ: {œÉ_is:.5f}\")\n\n\nEstimates for œÉ after 1 iterations\ncrude Monte Carlo œÉ: 0.00000\nimportance sampling œÉ: 0.00000\n\n\nSo we get a roughly \\(10\\times\\) boost in our accuracy, just by sampling more the important directions.\n\n\n5.5.2 üóê Importance Sampling in ML\nAfter this little detour of importance sampling in statistics, let‚Äôs talk about how it shows up in ML for off-policy prediction. The goal here isn‚Äôt variance reduction at all; it‚Äôs to let us evaluate a target policy using data generated by a different behaviour policy. In fact, the variance can easily blow up to infinity.\nSuppose we have a behaviour policy \\(b\\), which we use to generate episodes, and a target policy \\(\\pi\\), for which we want the expected return \\(v_\\pi(s)\\). As before, we can pull out the importance sampling trick: \\[\n\\begin{split}\nv_\\pi(s) &= \\mathbb{E}_{\\tau \\sim \\pi}[G_0(\\tau)| S_0 = s] \\\\\n&= \\sum_{\\tau} G_0(\\tau) \\mathrm{Pr}_{\\pi}(\\tau|S_0 = s) \\\\\n&= \\sum_{\\tau} G_0(\\tau) \\frac{\\mathrm{Pr}_\\pi(\\tau |S_0 = s)}{\\mathrm{Pr}_b(\\tau|S_0 = s)} \\mathrm{Pr}_b(\\tau|S_0 = s) \\\\\n&= \\mathbb{E}_{\\tau \\sim b}\\left[G_0(\\tau) \\frac{\\mathrm{Pr}_\\pi(\\tau|S_0 = s)}{\\mathrm{Pr}_b(\\tau|S_0 = s)}\\Big| S_0 = s\\right]\n\\end{split}\n\\]\nWe call the ratio inside the expectation the importance weight: \\[\nw(\\tau) = \\frac{\\mathrm{Pr}_\\pi(\\tau|S_0 = s)}{\\mathrm{Pr}_b(\\tau|S_0=s)}.\n\\]\nLet‚Äôs compute \\(w(\\tau)\\) for an episode \\(\\tau = S_0, A_0, \\dots, A_{T-1}, R_T, S_T\\). First note that \\[\n\\mathrm{Pr}_\\pi(\\tau | S_0 = s) = \\prod_{k=0}^{T-1} \\pi(A_k|S_k) p(S_{k+1}| S_k, A_k)\n\\] and equally for \\(\\mathrm{Pr}_\\pi(\\tau|S_0 = s)\\). So the importance weight is \\[\n\\begin{split}\nw(\\tau) &= \\frac{\\mathrm{Pr}_\\pi(\\tau|S_0 = s)}{\\mathrm{Pr}_b(\\tau|S_0 = s)} \\\\\n&= \\prod_{k = 0}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}\n\\end{split}\n\\] All the environment dynamics \\(p(\\cdot)\\) cancel out, so the importance weight only depends on the policies which we can compute.\nIf we collect everything and generalise slightly, we can write: \\[\nv_\\pi(s) = \\mathbb{E}_{\\tau \\sim b}[w_{t:}(\\tau)G_t(\\tau) | S_t = s]\n\\tag{5.3}\\]\nwith \\[\nw_{t:}(\\tau) = \\prod_{k = t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}\n\\tag{5.4}\\]\nI‚Äôve drifted away from Sutton and Barto (2018) notation here‚Äîand I‚Äôll stick with it for now, because for me their notation is a bit dense. I know‚Ä¶ bold move.\nNow let‚Äôs meet the estimators for Equation¬†5.3. For episodes \\(\\tau^{(1)}, \\tau^{(2)}, \\dots, \\tau^{(n)} \\sim b\\) we define:\n\nOrdinary importance sampling (OIS): \\[\nV^{\\mathrm{OIS}}(s) = \\frac{\\sum_{(i,t) \\in \\mathcal{T}(s)} w_{t:}(\\tau^{(i)}) G_t(\\tau^{(i)})}{|\\mathcal{T}(s)|},\n\\tag{5.5}\\] where \\(\\mathcal{T}(s)\\) is either:\n\nfirst-visit: all \\((i,t)\\) such that \\(s\\) is first visited at time \\(t\\) in episode \\(i\\), or\nevery visit: all \\((i, t)\\) such that \\(S^{(i)}_t = s\\).\n\nWeighted importance sampling (WIS): \\[\nV^{\\mathrm{WIS}}(s) = \\frac{\\sum_{(i,t) \\in \\mathcal{T}(s)} w_{t:}(\\tau^{(i)}) G_t(\\tau^{(i)})}{\\sum_{(i,t) \\in \\mathcal{T}(s)} w_{t:}(\\tau^{(i)})}.\n\\tag{5.6}\\] This also comes in first-visit and every-visit flavours.\n\n\n\n5.5.3 üóê Bias, MSE, and All That\nTime to talk properly about estimators. An estimator is just a rule for computing an estimate of some unknown quantity from data. If you want, you can think of an estimator as a random variable built out of other random variables (your data).\nIn our case, the unknown quantity is \\(v_\\pi\\), the true value function under the target policy \\(\\pi\\); the data are the episodes \\(\\tau^{(1)}, \\dots, \\tau^{(n)}\\) generated under the behaviour policy \\(b\\); and the estimators are the various importance sampling constructions: ordinary vs weighted, and first-visit vs every-visit.\n\nFirst-Visit Ordinary importance Sampling\nThe first-visit ordinary IS estimator, \\(V_\\mathrm{fv}^\\mathrm{OIS}\\), is probably the simplest of the lot. For a given state \\(s\\), we collect the first \\(m\\) first-visit samples and treat them as i.i.d. pairs \\((w_j,G_j)\\) of importance weights and returns. Then: \\[\nV^{\\mathrm{OIS}}_\\text{fv}(s) = \\frac{1}{m}\\sum_{j=1}^m w_j G_j.\n\\]\nBecause it‚Äôs just the mean of independent samples, it‚Äôs unbiased: \\[\n\\mathrm{E}[V^{\\mathrm{OIS}}_\\text{fv}(s)] = v_\\pi(s)\n\\] basically by construction, since \\[\n\\mathbb{E}[w_jG_j] = v_\\pi(s)\n\\]\nA standard way of judging an estimator is the mean squared error (MSE): \\[\n\\mathrm{MSE}(V^{\\mathrm{OIS}}_\\text{fv}(s)) = \\mathbb{E}[(\\mathrm{V}^{OIS}_\\text{fv}(s) - v_\\pi(s))^2].\n\\]\nFor an unbiased estimator, the MSE is just the variance, so it scales like \\(1/m\\) What we usually feel in practice is the root MSE (RMSE): \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}},\n\\] which gives the familiar Monte-Carlo rate: \\(O(1/\\sqrt{m})\\).\nThe catch? Ordinary importance sampling can have infinite variance if the importance weights themselves have infinite variance‚Äîwhich can happen for both first-visit and every-visit versions (see Example¬†5.5 and the solution to Exercise¬†5.8). In those cases, you obviously don‚Äôt get the \\(O(1/\\sqrt{m})\\) behaviour. Convergence does still happen, but it becomes even slower and generally not usable.\n\n\nFirst-Visit Weighted importance Sampling\nNow let‚Äôs look at the first-visit weighted IS estimator: \\[\nV_{\\text{fv}}^{\\mathrm{WIS}} = \\frac{\\sum_{j=1}^m w_j G_j}{\\sum_{j=1} w_j},\n\\] again for i.i.d. samples \\((w_j,G_j)\\). This estimator trades a little bias for lower variance, where bias is defined as: \\[\nB(V_{\\text{fv}}^{\\mathrm{WIS}}) = \\mathbb{E}[V_{\\mathrm{fv}}^{\\mathrm{WIS}}] - v_\\pi(s),\n\\]\nTo see why the estimator is generally biased, write it as \\[\nV_{\\text{fv}}^{\\mathrm{WIS}} = \\frac{\\left(\\sum_{j=1}^m w_j G_j\\right)/m}{\\left(\\sum_{j=1} w_j\\right)/m},\n\\] The numerator is \\(V_{\\text{fv}}^{\\mathrm{OIS}}\\), whose expectation is \\(v_\\pi(s)\\); the denominator has expectation 1 (we‚Äôll show this in a moment). But because these two random variables are dependent, we can‚Äôt in general take the expectation through the quotient: \\[\n\\mathbb{E}[V_{\\text{fv}}^{\\mathrm{WIS}}] = \\mathbb{E}\\left[\\frac{V_{\\text{fv}}^{\\mathrm{OIS}}}{(\\sum_j w_j)/m}\\right]\n\\not= \\frac{\\mathbb{E}[V_{\\text{fv}}^{\\mathrm{OIS}}]}{\\mathbb{E}[(\\sum_j w_j)/m]}\n= \\frac{v_\\pi(s)}{1}\n\\] And here is why \\(\\mathbb{E}[w]=1\\): \\[\n\\begin{split}\n\\mathbb{E}_{\\tau \\sim b}[w] &= \\sum_{\\tau} \\frac{p_\\pi(\\tau|S_0=s)}{p_b(\\tau)|S_0=s} p_b(\\tau|S_0=s) \\\\\n&= \\sum_{\\tau} p_\\pi(\\tau|S_0=s) \\\\\n&= 1\n\\end{split}\n\\]\nDespite the bias, it‚Äôs really not a disaster: the bias shrinks as \\(1/m\\) (Agapiou et al. 2017, Theorem 2.1), and the MSE shrinks as \\(1/m\\) too, just as with OIS. So the bias isn‚Äôt doing any real harm.\nA crucial advantage is that weighted IS always has finite variance as long as returns are bounded by some \\(a\\) (maybe even if they are unbounded, I didn‚Äôt check that). We can write it as a convex combination: \\[\nV^\\mathrm{WIS}_\\text{fv} = \\sum_{j=1}^m \\alpha_j G_j \\quad \\text{with } \\alpha_j = \\frac{w_j}{\\sum_{k=1}^m w_k}\n\\]\nSince the \\(\\alpha_j\\) just do a weighted sum of the \\(‚à£G_j‚à£\\leq a\\), we have \\(\\mathbb{E}[(V^\\mathrm{WIS}_\\text{fv})^2] \\leq a^2\\).\n\n\nEvery-Visit Importance Sampling\nThe every-visit versions is maybe the easiest to come up conceptually. However, the pairs \\((w_j,G_j)\\) are not independent when they come from the same episode. This creates correlations that complicate the variance analysis.\nThe good news is that the every-visit estimators are still consistent: \\[\nV_\\mathrm{ev}^\\mathrm{OIS} \\to v_œÄ(s), V_\\mathrm{ev}^\\mathrm{WIS} \\to v_\\pi(s) \\quad \\text{for } n \\to \\infty\n\\]\nbut they are no longer unbiased for finite data‚Äîeven for ordinary importance sampling. Practically, the every-visit estimators make fuller use of data: they reuse later parts of each episode instead of discarding everything after the first occurrence of a state. But I don‚Äôt know when this is actually useful.\n\nExercise 5.5 Consider an MDP with a single nonterminal state and a single action that transitions back to the nonterminal state with probability \\(p\\) and transitions to the terminal state with probability \\(1-p\\). Let the reward be \\(+1\\) on all transitions, and let \\(\\gamma = 1\\). Suppose you observe one episode that lasts 10 steps, with a return of 10. What are the first-visit and every-visit estimators of the value of the nonterminal state?\n\n\nSolution 5.5. The exercise itself is straightforward:\n\nThe first-visit estimate is the first return: 10.\nThe every-visit estimate averages all returns after each visit to the state: \\[\n\\frac{10 + 9 + \\dots + 1}{10} = \\frac{10\\cdot 11}{2 \\cdot 10} = 5.5\n\\]\n\nThat‚Äôs the basic part. Now let‚Äôs go a bit further and use this setup to show explicitly that the every-visit estimator \\(V_{\\text{ev}}(s)\\) is biased.\nThe episode length \\(L\\) is geometrically distributed: \\[\n\\mathrm{Pr}(L = \\ell) = p^{\\ell-1}(1-p).\n\\] The return of such an episode is just \\(\\ell\\), so the true value of the nonterminal state is \\[\n\\begin{split}\nv(s) &= \\sum_{\\ell = 1}^\\infty \\ell \\cdot p^{\\ell -1}(1-p) \\\\\n&= (1-p) \\left(\\sum_{\\ell = 1}^\\infty p^\\ell \\right)' \\\\\n&= (1-p) \\left( \\frac{1}{1-p} \\right)' \\\\\n&= \\frac{1}{1-p}\n\\end{split}\n\\]\nThe every-visit estimator for a single episode of length \\(L\\) is \\[\nV_\\text{ev} = \\frac{1+\\dots+L}{L} = \\frac{L+1}{2}\n\\] So \\[\n\\mathbb{E}(V_\\text{ev}) = \\sum_{\\ell = 1}^\\infty \\frac{\\ell + 1}{2} p^{\\ell-1}(1-p) = \\frac{2-p}{2(1-p)}.\n\\] Thus the bias is \\[\nB(V_\\text{ev}) = \\mathbb{E}(V_\\text{ev}) - v = - \\frac{1}{2} \\frac{p}{1-p}.\n\\] So we can see that the estimator is biased when sampling a single episode.\nNow let‚Äôs go above and beyond and compute the bias for the general case of \\(m\\) episodes. This is mainly to demonstrate how annoying it can be to work with data that aren‚Äôt i.i.d.\nFor \\(m\\) episodes of lengths \\(L_1,\\dots,L_m\\), the every-visit estimator is \\[\n\\begin{split}\nV_\\text{ev} &= \\frac{\\sum_{i=1}^m \\sum_{t=1}^{L_i} G_{i,t}}{\\sum_{i=1}^m L_i} \\\\\n&= \\frac{\\sum_{i=1}^m \\frac{L_i(L_i+1)}{2}}{\\sum_{i=1}^m L_i} \\\\\n&= \\frac{1}{2} \\left( 1 + \\frac{\\sum_{i=1}^m L_i^2}{\\sum_{i=1}^m L_i}\\right),\n\\end{split}\n\\]\nLet \\(S_m=\\sum_{i=1}^m L_i\\). We now we rely on two non-trivial facts:\n\n\\(\\mathbb{E}[L_1^2|S_m = s] = \\frac{s(2s-m+1)}{m(m+1)}\\)\n\\(\\mathbb{E}[S_m] = \\frac{m}{1-p}\\).\n\nThen \\[\n\\begin{split}\n\\mathbb{E}\\left[ \\frac{\\sum_{i=1}^m L_i^2}{\\sum_{i=1}^m L_i} \\right] &= \\sum_{s = m}^\\infty \\mathbb{E}\\left[ \\frac{\\sum_{i=1}^m L_i^2}{s} | S_m = s \\right] \\mathrm{Pr}(S_m = s) \\\\\n&= \\sum_{s = m}^\\infty \\frac{m}{s}\\mathbb{E}\\left[ L_1^2 | S_m = s \\right] \\mathrm{Pr}(S_m = s) \\\\\n&= m\\sum_{s = m}^\\infty \\frac{1}{s} \\frac{s(2s-m+1)}{m(m+1)} \\mathrm{Pr}(S_m = s) \\\\\n&= \\frac{1}{m+1} \\sum_{s=m}^\\infty (2s-m+1) \\mathrm{Pr}(S_m = s) \\\\\n&= \\frac{1}{m+1}\\mathbb{E}[2S_m - m + 1] \\\\\n&= \\frac{1}{m+1}\\left(\\frac{2m}{1-p} - m + 1\\right).\n\\end{split}\n\\]\nSubstituting this into the expression for \\(\\mathbb{E}[V_\\text{ev}]\\) and simplifying gives: \\[\n\\mathbb{E}[V_\\text{ev}] = \\frac{m + 1 - p}{(m+1)(1-p)}.\n\\] So the bias is \\[\nB[V_\\text{ev}] = -\\frac{1}{m+1}\\frac{p}{1-p}.\n\\]\nSurprinsingly, this is a very straightforward generalisation of the \\(m=1\\) case. Also the bias falls as \\(1/m\\).\n\n\nExample 5.4 This is example 5.4: Off-policy Estimation of a Blackjack State Value from Sutton and Barto (2018).\nLet‚Äôs use importance sampling for off-policy prediction for the first time. The goal is to estimate the value \\(v_\\pi(s)\\) of a specific Blackjack state under a target policy \\(\\pi\\), even though our data will be generated by a behaviour policy \\(b\\). Concretely:\n\nMDP = blackjack\n\\(\\pi =\\) greedy policy from Example¬†5.1\n\\(b =\\) uniform random policy\n\\(s =\\) (player‚Äôs total = 13, usable ace = yes, dealer‚Äôs showing = 2)\n\nSutton and Barto (2018) give the reference value \\(v_\\pi(s)=‚àí0.27726\\). Let‚Äôs check it ourselves with crude Monte Carlo just to be sure:\n\n# === crude MC estimate of blackjack state value ===\ndef generate_episode_from_state(env, œÄ, start_state):\n    \"\"\"Generate an episode starting from start_state following œÄ.\"\"\"\n    env.set_state(start_state)\n\n    state, terminated = start_state, False\n    while not terminated:\n        action = œÄ[state]\n        state, reward, terminated = env.step(action)\n\n    return reward\n\n\nenv = BlackJack()\nstart_state = State(13, True, 2)\nn_samples = 1  # 100_000\n\ntrials = [\n    generate_episode_from_state(env, œÄ_at_least_20, start_state)\n    for _ in range(n_samples)\n]\nprint(f\"v_œÄ(s) ~= {sum(trials) / n_samples} ({n_samples} trials)\")\n\nv_œÄ(s) ~= 1.0 (1 trials)\n\n\nThis lands pretty close to \\(-0.27726\\), so we believe them and take their number as the reference.\nNow let‚Äôs set up importance sampling. To do off-policy prediction we need to calculate the importance weight Equation¬†5.4. Because \\(b\\) is stochastic and \\(\\pi\\) is deterministic, the importance weight for a trajectory is: \\[\nw(\\tau) = \\begin{cases} 0 & \\text{if } \\exists t \\text{ s.t. } A_{t} \\neq \\pi(S_{t}) \\\\ \\prod_{t=0}^{T-1}\\frac{1}{b(A_t)} & \\text{otherwise} \\end{cases}\n\\]\nHere is the implementation of the function bj_importance_weight_compliance that generates an episode and returns the importance weight and the return. The behaviour policy \\(b\\) is hardcoded as uniformly random and the target policy \\(\\pi\\) is a parameter and expects a deterministic policy. Since \\(b\\) is stochastic and \\(\\pi\\) is deterministic, the importance weight is \\(0\\) if \\(b\\) does something \\(\\pi\\) wouldn‚Äôt. This results in a compliance version of importance sampling: as soon as the behaviour policy deviates from the target policy the resulting importance-weight is \\(0\\). That means in our case‚Äîwhere we have two actions and the behaviour policy is uniformly random‚Äîthe importance-weight is either \\(0\\) if \\(b\\) does something \\(\\pi\\) wouldn‚Äôt or \\(2^\\ell\\) where \\(\\ell\\) is the number of compliant actions.\n\n# === single importance-sampling episode for Black Jack ===\n\nEpisodeWG = tuple[float, float]  # (weight, return)\n\n\ndef bj_importance_weight_compliance(\n    env: BlackJack, œÄ: dict[State, Action], start_state: State, rng\n) -&gt; EpisodeWG:\n    \"\"\"Generate one episode under the *behaviour* policy b (uniform random)\n    and return (importance-weight, return)\n    for target deterministic policy œÄ.\n\n    Note:\n    If the behaviour policy deviates even once (non-compliant),\n    the correct IS weight is 0 and this returns (0, 0).\"\"\"\n    env.rng = rng\n    env.set_state(start_state)\n\n    episode_length = 0\n    state, terminated = start_state, False\n    while not terminated:\n        episode_length += 1\n\n        # behaviour: uniformly random\n        action = rng.choice(env.action_space)\n\n        # if non-compliant, return (0, 0)\n        if action != œÄ[state]:\n1            return 0, 0\n\n        # otherwise, continue as usual\n        state, reward, terminated = env.step(action)\n\n    # compliant for the entire episode:\n    return 2**episode_length, reward\n\n\n1\n\nIt‚Äôs ok to return \\((w,G) = (0,0)\\) as soon as \\(b\\) is non-compliant. We don‚Äôt have to know the real \\(G\\). It only appears in products \\(w \\cdot G\\) so its value doesn‚Äôt matter if \\(w = 0\\).\n\n\n\n\nJust to get a feel for it we can see the returns for 10 episodes\n\n\nCode\nrng = random.Random(0)\nstart_state = State(13, True, 2)\nfor i in range(10):\n    w, r = bj_importance_weight_compliance(env, œÄ_at_least_20, start_state, rng)\n\n    print(f\"episode {i+1:2d} ‚Äì Weight {w}, return {r}\")\n\n\nepisode  1 ‚Äì Weight 0, return 0\nepisode  2 ‚Äì Weight 0, return 0\nepisode  3 ‚Äì Weight 0, return 0\nepisode  4 ‚Äì Weight 0, return 0\nepisode  5 ‚Äì Weight 0, return 0\nepisode  6 ‚Äì Weight 8, return -1\nepisode  7 ‚Äì Weight 0, return 0\nepisode  8 ‚Äì Weight 8, return -1\nepisode  9 ‚Äì Weight 0, return 0\nepisode 10 ‚Äì Weight 0, return 0\n\n\nSo most of the time the weight is 0 and if not it can get quite big. This shows why ordinary importance sampling for off-policy prediction can have such high variance.\nNow let‚Äôs plot the performance of ordinary and weighted importance sampling.\n\nCode\nimport random\nimport seaborn as sns\n\n\ndef mse_vs_m(\n    env, policy, start_state, max_m: int, iterations_per_m: int, true_value: float, rng\n):\n    \"\"\"Estimate MSE of ordinary and weighted importance sampling\n    as a function of the number of episodes m.\n    \"\"\"\n\n    def compute_estimators_from_array(\n        ep_arr: np.ndarray,\n    ) -&gt; tuple[np.float64, np.float64]:\n        \"\"\"\n        Given an array of shape (m, 2) where:\n            col 0 = importance weights (w)\n            col 1 = returns (G)\n        compute:\n            ordinary IS = mean(w * G)\n            weighted IS = sum(w * G) / sum(w)\n        \"\"\"\n        w = ep_arr[:, 0]\n        g = ep_arr[:, 1]\n\n        ordinary_IS = np.mean(w * g)\n\n        denom = w.sum()\n        weighted_IS = (w * g).sum() / denom if denom != 0.0 else np.nan\n\n        return ordinary_IS, weighted_IS\n\n    # --- Prepare arrays to record MSEs for different m values ---\n    m_values = np.arange(1, max_m + 1)\n    ordinary_mses = np.empty_like(m_values, dtype=float)\n    weighted_mses = np.empty_like(m_values, dtype=float)\n\n    # --- For each m (sample size), estimate MSE over many independent runs ---\n    for idx, m in enumerate(m_values):\n        ord_estimates = np.empty(iterations_per_m, dtype=float)\n        wgt_estimates = np.empty(iterations_per_m, dtype=float)\n\n        for it in range(iterations_per_m):\n            # --- Collect m independent (weight, return) samples ---\n            episodes = [\n                bj_importance_weight_compliance(env, policy, start_state, rng)\n                for _ in range(m)\n            ]\n            ep_arr = np.array(episodes, dtype=float)  # shape (m, 2)\n            ord_estimates[it], wgt_estimates[it] = compute_estimators_from_array(ep_arr)\n\n        # --- Compute mean squared error relative to the known true value ---\n        ordinary_mses[idx] = np.nanmean((ord_estimates - true_value) ** 2)\n        weighted_mses[idx] = np.nanmean((wgt_estimates - true_value) ** 2)\n\n    return m_values, ordinary_mses, weighted_mses\n\n\ndef plot_ordinary_vs_weighted_IS(max_episodes, iterations_per_episode, rng):\n    bj_env = BlackJack()\n    start_state = State(13, True, 2)\n    true_value = -0.27726  # precomputed\n\n    m_vals, o_mses, w_mses = mse_vs_m(\n        bj_env,\n        œÄ_at_least_20,\n        start_state,\n        max_episodes,\n        iterations_per_episode,\n        true_value,\n        rng,\n    )\n\n    # --- Plot ---\n    sns.lineplot(x=m_vals, y=o_mses, label=\"Ordinary IS MSE\", alpha=0.9)\n    sns.lineplot(x=m_vals, y=w_mses, label=\"Weighted IS MSE\", alpha=0.9)\n    plt.xscale(\"log\")\n    plt.yscale(\"log\")\n    plt.ylim(top=10)\n    plt.xlabel(\"Saple size (episodes)\")\n    plt.ylabel(f\"Mean squarred error (over {iterations_per_episode} runs)\")\n    plt.title(\"Ordinary vs Weighted Importance Sampling\\non single Blackjack state\")\n    plt.show()\n\n\n# Example seed that gives nice illustrative curves # 21, 23, 38, 9112\nrng = random.Random(0)\nmax_episodes = 100\niterations_per_episode = 100\n# max_episodes = 1\n# iterations_per_episode = 1\n\nplot_ordinary_vs_weighted_IS(max_episodes, iterations_per_episode, rng)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.4: This is like figure 5.3 from Sutton and Barto (2018). Weighted importance sapling produces lower error estimates for small sample sizes. Both curves are based on the same data.\n\n\n\nWe can see in Figure¬†5.4 that the variance of ordinary importance sampling is higher (it produces higher spikes). For samples of size around 100 they seem to produce comparatively similar results.\n\n\nExample 5.5 Infinite Variance (Sutton and Barto 2018, Example 5.5).\nI never thought about what infinite variance means in terms of statistics until this example.\nConsider the tiny MDP in Figure¬†5.5 (a).\nThe optimal policy is \\(\\pi(s) = \\mathrm{left}\\) and \\(v_\\pi(s) = +1\\). Let‚Äôs see what importance sampling does with uniform random behaviour policy \\(b\\).\nWeighted importance sampling does a pretty good job here. As soon as it‚Äôs defined, i.e., there is one compliant episode in the samples, it estimates exactly \\(+1\\).\nOrdinary importance sampling, does a much worse job.\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gc\n\n# --- Sampling function ---\n_rng = np.random.default_rng()\n\n\n# This simulates one run on the MDP given above (but much faster)\ndef sample_powers_of_two(size, rng=_rng):\n    nonzero_mask = rng.random(size) &gt; 10 / 11\n    samples = np.zeros(size, dtype=np.int64)\n    k = nonzero_mask.sum()\n    if k:\n        n = rng.geometric(0.55, size=k).astype(np.int64)\n        samples[nonzero_mask] = 1 &lt;&lt; n\n    return samples\n\n\n# --- Parameters ---\n# N = 100_000_000\nN = 10_000_000\nnum_runs = 3\n\n# --- Generate and plot ---\nplt.figure(figsize=(10, 5))\nplot_points = 2000\n\nfor i in range(num_runs):\n    samples = sample_powers_of_two(N)\n    running_avg = np.cumsum(samples) / np.arange(1, N + 1)\n\n    raw = np.logspace(0, np.log10(N), num=plot_points)\n    # indices as 1-based positions, then convert to zero-based for indexing\n    idxs = np.unique(np.minimum(N, np.maximum(1, np.round(raw).astype(np.int64)))) - 1\n    xs = idxs + 1\n    ys = running_avg[idxs]\n\n    plt.plot(xs, ys, lw=1.0, alpha=0.7)\n\n    del running_avg\n    del samples\n    gc.collect()\n\nplt.title(\n    rf\"MC estimate of $v_\\pi(s)$ with ordinary importance sampling ({num_runs} runs)\"\n)\nplt.xlabel(\"Sample size (Episodes)\")\nplt.xscale(\"log\")\nplt.ylabel(\"V(s)\")\nplt.ylim(top=2)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A MDP with only one non-terminal state \\(s\\) and two actions \\(\\mathrm{left}\\) and \\(\\mathrm{right}\\). The action \\(\\mathrm{right}\\) deterministically terminates the MDP with \\(0\\) reward. The action \\(\\mathrm{left}\\) terminates with probability \\(0.1\\) with a reward of \\(+1\\), in the other case it loops back to \\(s\\) with reward \\(0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) MC estimates for \\(v_\\pi(s)\\) using ordinary importance sampling for the MDP shown above. The target policy \\(\\pi\\) is the optimal policy and the behaviour policy is the uniform random policy.\n\n\n\n\n\nFigure¬†5.5: (Sutton and Barto 2018, fig. 5.4). Ordinary importance sampling produces surprisingly unstable estimates on the one-state MDP shown above. The correct estimate here is 1 (\\(\\gamma = 1\\)), and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite. These results are for off-policy first-visit MC.\n\n\n\nThe problem for ordinary importance sampling is that the importance-weights have infinite variance. For the variance it‚Äôs a battle between very fast growing importance-weights and very fast falling probabilities, which in this case is won by the importance-weights. We look at this in a bit more detail.\nA bit of calculating shows that the result of an episode can be described by two independent random variables:\n\n\\(C\\): weather the episode is compliant \\[\n\\mathrm{Pr}(C = F) = 11/10, \\quad \\mathrm{Pr}(C = T) = 1/11\n\\]\n\\(L\\): the length of the episode \\[\n\\mathrm{Pr}(L = \\ell) = 0.55 \\cdot 0.45^{\\ell - 1}.\n\\]\n\nSo for one episode with reward \\(G\\) and importance-weight \\(w\\): \\[\n\\begin{split}\n\\mathbb{E}[(wG)^2] &= \\mathbb{E}[(2^L \\cdot [C = T])^2] \\\\\n&= \\mathbb{E}[C=T] \\cdot \\mathbb{E}[4^L] \\\\\n&= \\frac{1}{11} \\sum_{\\ell = 1}^\\infty 4^\\ell \\cdot 0.55 \\cdot 0.45^{\\ell-1} \\\\\n&= \\frac{2.2}{11} \\sum_{\\ell = 1}^\\infty 1.8^{\\ell-1} \\\\\n&= \\infty\n\\end{split}\n\\]\nThus \\(\\mathrm{var}(wG) = \\infty\\) and no amount of averaging can make this finite for the sample mean used by ordinary importance sampling. Thus ordinary importance sampling for this problem has infinite variance.\nThat doesn‚Äôt mean we couldn‚Äôt use it. The law of large numbers still applies and says that \\(V^\\mathrm{OIS}_\\text{fv}(s) \\to v_\\pi(s)\\) for sample size going to infinity. But we can‚Äôt use the central limit theorem and thus the standard error does not decreases with \\(1/\\sqrt{n}\\). However, there are other error metrics that can be used. For example the mean absolute deviation which is finite for \\(V^\\mathrm{OIS}\\). But the problem is that it scales even slower then \\(1/\\sqrt{n}\\), so, it‚Äôs eaven less usefull.\n\n\nExercise 5.6 What is the equation analogous to Equation¬†5.6 for action values \\(Q(s, a)\\) instead of state values \\(V(s)\\), again given returns generated using \\(b\\)?\n\n\nSolution 5.6. Let‚Äôs go through all the important steps. Importance sampling trick: \\[\n\\begin{split}\nq_\\pi(s,a) &= \\mathbb{E}_{\\tau \\sim \\pi}[G_t(\\tau)| S_t = s, A_t = a] \\\\\n&= \\sum_{\\tau} G_t(\\tau) p_\\pi(\\tau | S_t = s, A_t = a) \\\\\n&= \\sum_{\\tau} G_t(\\tau) \\frac{p_\\pi(\\tau | S_t = s, A_t = a)}{p_b(\\tau | S_t = s, A_t = a)}p_b(\\tau | S_t = s, A_t = a) \\\\\n&= \\mathbb{E}_{\\tau\\sim b} [w_{t+1}(\\tau)G_t(\\tau) | S_t = s, A_t = a]\n\\end{split}\n\\]\nThe last line follows from: \\[\n\\begin{split}\n\\frac{p_\\pi(\\tau | S_t = s, A_t = a)}{p_b(\\tau | S_t = s, A_t = a)}\n&= \\frac{\\pi(A_{t+1}|S_{t+1})\\dots \\pi(A_{T-1}|S_{T-1})}{b(A_{t+1}|S_{t+1})\\dots b(A_{T-1}|S_{T-1})} \\\\\n&= w_{t+1}(\\tau)\n\\end{split}\n\\]\nSo we get for ordinary importance sampling \\[\nQ^{\\mathrm{OIS}}(s,a) = \\frac{\\sum_{(i,t) \\in \\mathcal{T}(s,a)} w_{t+1}(\\tau_i) G_t(\\tau_i)}{|\\mathcal{T}(s,a)|}\n\\]\nand for weighted importance sampling \\[\nQ^{\\mathrm{WIS}}(s,a) = \\frac{\\sum_{(i,t) \\in \\mathcal{T}(s,a)} w_{t+1}(\\tau_i) G_t(\\tau_i)}{\\sum_{(i,t) \\in \\mathcal{T}(s,a)} w_{t+1}(\\tau_i)}\n\\]\n\n\nExercise 5.7 In learning curves such as those shown in Figure¬†5.4 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?\n\n\nSolution 5.7. The increasing/decreasing behaviour is not that visible in my plot. It looks it is visible but it‚Äôs rather an effect of the variance. When we increase the number of runs we can see that the curve curve goes down but slower than ordinary importance sampling at first:\n\n\nCode\n# Example seed that gives nice illustrative curves # 21, 23, 38, 9112\nrng = random.Random(0)\nmax_episodes = 10\niterations_per_episode = 10000\n# max_m = 1\n# iterations_per_m = 1\n\nplot_ordinary_vs_weighted_IS(max_episodes, iterations_per_episode, rng)\n\n\n\n\n\n\n\n\n\nNonetheless this is most likely duo to the bias of weighted importance sampling. Note that for an estimator we have \\[\n\\text{MSE} = \\text{Var} + \\text{Bias}^2\n\\]\nThe Bias term goes away pretty quickly but for small samples it still skews the estimator away from the true value.\n\n\nExercise 5.8 The results with Example¬†5.5 and shown in Figure¬†5.5 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?\n\n\nSolution 5.8. To show that the variance of the every-visit ordinary importance sapmling estimator \\(V^\\mathrm{OIS}_\\text{ev}\\) is infinite we will show that \\(\\mathbb{E}[(V^\\mathrm{OIS}_\\text{ev})^2]\\).\nAs said in Example¬†5.5 we can describe a single episode as a random vector \\((C,L)\\) with \\(C\\) flaging if the episode is compliant and \\(L\\) the length of the episode. The sum \\(\\sum w_tG_t\\) for this episode is then \\[\n\\sum w_tG_t = [C = T] \\sum 2^t = [C = T] 2^{L+1} - 2\n\\]\nNow we can describe the general case with \\(m\\) episodes which can be modeled by \\(m\\) i.i.d. random vectors \\((C_1,L_1),\\dots,(C_m,L_m)\\) and \\[\nV^\\mathrm{OIS}_\\text{ev} = \\frac{\\sum_{i=1}^m [C = T] (2^{L_i+1}-2)}{\\sum_{i=1}^m L_i}\n\\]\nNow we can provide an estimate \\[\n\\begin{split}\n\\mathbb{E}[(V^\\mathrm{OIS}_\\text{ev})^2] &\\geq \\sum_{\\ell = 1}^\\infty \\left(\\frac{(2^{\\ell+1}-2) + \\sum_{i=2}^m (2^{1+1}-2)}{\\ell + \\sum_{i=1}^m 1}\\right)^2 \\\\\n&\\quad\\cdot \\mathrm{Pr}(L_1 = \\ell, L_2,\\dots,L_m = 1) \\\\\n&\\quad\\cdot \\mathrm{Pr}(C_1, \\dots, C_m = 1) \\\\\n&= \\sum_{\\ell = 1}^\\infty \\left(\\frac{2^{\\ell+1}+2(m-2)}{\\ell+m-1}\\right)^2 \\cdot  0.55^m \\cdot 0.45^{\\ell-1} \\cdot (1/11)^m \\\\\n&\\approx \\sum_{\\ell = 1}^\\infty 4^\\ell 0.45^\\ell\\\\\n&= \\infty\n\\end{split}\n\\]\nSo since the second moment is infinite the variance is too.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#incremental-implementation",
    "href": "chapters/05-monte-carlo-methods.html#incremental-implementation",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.6 Incremental Implementation",
    "text": "5.6 Incremental Implementation\nWe have returns \\(G_1,\\dots,G_{n-1}\\) and random weights \\(W_1,\\dots,W_{n-1}\\). The weighted average is \\[\nV_n = \\frac{\\sum_{k=1}^{n-1}W_kG_k}{\\sum_{k=1}^{n-1}W_k}\n\\]\n\nExercise 5.9 Modify the algorithm for first-visit MC policy evaluation (Listing¬†6.1) to use the incremental implementation for sample averages described in Section 2.4.\n\n\nSolution 5.9. \n\nInput: \\(\\pi\\), the policy to be evaluated\nInitialisation: \\(V(s) \\gets 0\\), for all \\(s \\in \\mathcal{S}\\)\n¬†¬†¬†¬†\\(N(s) \\gets 0\\), for all \\(s \\in \\mathcal{S}\\)\n\nLoop forever:\n¬†¬†¬†¬†Generate an episode following \\(\\pi\\): \\(S_0,A_0, R_1, S_1, A_1, \\dots, R_T, S_T\\)\n¬†¬†¬†¬†\\(G \\gets 0\\)\n¬†¬†¬†¬†Loop for each step of episode, \\(t = T-1, T-2, \\dots, 0\\):\n¬†¬†¬†¬†¬†¬†¬†¬†\\(G \\gets \\gamma G + R_{t+1}\\)\n¬†¬†¬†¬†¬†¬†¬†¬†If \\(S_t\\) is not in \\(\\{S_0,S_1, \\dots, S_{t-1}\\}\\):\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(N(S_t) \\gets N(S_t) + 1\\)\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†\\(V(S_t) \\gets V(S_t) + \\frac{1}{N(S_t)}[G - V(S_t)]\\)\n\n\n\nExercise 5.10 Derive the weighted-average update rule (5.8) from (5.7). Follow the pattern of the derivation of the unweighted rule (2.3).\n\n\nSolution 5.10. TBD",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#off-policy-monte-carlo-control",
    "href": "chapters/05-monte-carlo-methods.html#off-policy-monte-carlo-control",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.7 Off-policy Monte Carlo Control",
    "text": "5.7 Off-policy Monte Carlo Control\n\nExercise 5.11 In the boxed algorithm for off-policy MC control, you may have been expecting the \\(W\\) update to have involved the importance-sampling ratio \\(\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}\\), but instead it involves \\(\\frac{1}{b(A_t|S_t)}\\). Why is this nevertheless correct?\n\n\nSolution 5.11. The whole thing about proceeding to the next episode and updating \\(W\\) is just complinace. If the behavioral policy does something the target policy would not do, the weight is \\(0\\). In the other case the importance sampling ratio is just \\[\n\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} = \\frac{1}{b(A_t|S_t)}\n\\]\n\n\nExercise 5.12 Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by \\(+1\\), \\(-1\\), or \\(0\\) in each step, for a total of nine (\\(3 \\times 3\\)) actions. Both velocity components are restricted to be nonnegative and less than \\(5\\), and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are 1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car‚Äôs location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability \\(0.1\\) at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).\n\n\nSolution 5.12. This is a substantial exercise. Setting up the environment, the training code, debugging everyting. That‚Äôs quite a lot of work. Also training on the full-size tracks is too much to do inside these notes. So I use a smaller track for exposition (Figure¬†5.6) and for the bigger problems I use pretrained models.\nOn all problem I use on-policy \\(\\varepsilon\\)-soft first-visit sample-average Monte-Carlo control for training. That‚Äôs quite a mouthfull but I will discuss each of these modifiers in this solution. At the very end I will also say some thoughts about Monte-Carlo exploring-starts and off-policy Monte-Carlo.\n\nThe track environment\nI externalised the racetrack environment into scripts.environment.race_track. Its only required argument is the layout represented as an ASCII string. There are some other knobs, among them the size of the maximal velocity components max_velocity or if the velocity components can be negative only_positive_velocity3. How I read the exercise, they want only_positive_velocity=True but I think that‚Äôs a bit strange, especially since then some parts of the provided exmaple tracks would be unreachable. So I will use only_posititve_velocity=False in all of the environments. Also prob_steering_failure=0.1 by default, as required in the exercise.\nIn the next code I define the small example env_tight. This environment only allows has max_velocity=2, this makes optimal episodes taking the left route shorter than taking the right route.\n\nimport scripts.environment.race_track as rt\n\ntrack_tight = \"\"\" \n##FFF\n##---\nF#---\n-#---\n-#---\n-#---\n-#---\n-#---\n-#---\n-S---\n\"\"\"\n\nenv_tight = rt.RaceTrack(\n    track_tight,\n    only_positive_velocity=False,\n    max_velocity=2,\n)\n\nThe figure explains the track a bit more in detail\n\nCode\nimport scripts.environment.race_track as rt\nimport random\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\ndef plot_episode(env, states, title=None, radius=0.3, figsize=(8,8)):\n    \"\"\"\n    Annotate every visit (timestep) but jitter repeated visits around the cell.\n    radius: radial offset (in cell units) for jitter ring; increase if labels collide.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n\n    img = env.grid._track.T\n    cmap = ListedColormap([\"#444444\", \"#e68a8a\", \"#f2f2f2\", \"#8fd18f\"])\n    ax.imshow(img, cmap=cmap, origin=\"lower\", interpolation=\"nearest\")\n    ax.set_xticks(np.arange(env.grid.width))\n    ax.set_yticks(np.arange(env.grid.height))\n    ax.set_xlim(-0.5, env.grid.width - 0.5)\n    ax.set_ylim(-0.5, env.grid.height - 0.5)\n    ax.set_aspect(\"equal\")\n\n    # Rotate x-tick labels\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n    # grid lines\n    for x in range(env.grid.width):\n        for y in range(env.grid.height):\n            rect = plt.Rectangle(\n                (x - 0.5, y - 0.5),\n                1,\n                1,\n                fill=False,\n                edgecolor=\"#444444\",\n                linewidth=0.1,\n            )\n            ax.add_patch(rect)\n\n    if len(states) &gt; 0:\n        positions = [s.position for s in states]\n        xs = [p[0] for p in positions]\n        ys = [p[1] for p in positions]\n\n        ax.plot(xs, ys, marker=\"o\")\n\n        # Build map from position -&gt; list of visit indices\n        visits = {}\n        for t, pos in enumerate(positions):\n            visits.setdefault(pos, []).append(t)\n\n        # Annotate each visit with radial offsets to avoid overlap\n        for pos, idx_list in visits.items():\n            x0, y0 = pos\n            n = len(idx_list)\n            for k, t in enumerate(idx_list):\n                if n == 1:\n                    dx, dy = (0.08, 0.18)\n                else:\n                    angle = 2 * math.pi * k / n\n                    dx = math.cos(angle) * radius\n                    dy = math.sin(angle) * radius\n                ax.text(\n                    x0 + dx,\n                    y0 + dy,\n                    str(t),\n                    fontsize=9,\n                    va=\"center\",\n                    ha=\"center\",\n                    bbox=dict(boxstyle=\"round,pad=0.1\", alpha=0.9),\n                )\n\n    if title:\n        ax.set_title(title)\n\n    return fig, ax\n\n\nfig, ax = plot_episode(\n    env_tight,\n    [],\n)\nfig.set_size_inches(2.5, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.6: A plot of the layout of env_tight. In the plots the starting cells are red, green squares the finish cells, the squares are the normal track pieces, and the dark squares the walls. The best route is going left and has a length of 5. Going the right path requires one more step.\n\n\n\n\n\nOn-policy first-visit MC control (for Œµ-soft policies)\nWe need to implement Listing¬†5.3. The algorithm lends itself to the following implementation, where the policies are deterministic, but in the episode generation we soften these to \\(\\varepsilon\\)-soft policies, this is basically an implementation of the idea that \\(\\varepsilon\\)-soft policies can be thought of hard policies on the a changed environment that randomices the choosen action with probablity \\(\\varepsilon\\) (Sutton and Barto 2018, chap. 5.4).\nThis is what generate_episode_soft does, it takes an envirnment, a deterministic policy, an \\(\\varepsilon\\) and produces an episode following \\(\\pi\\) where each step the action gets randomply choosen with probablity \\(\\varepsilon\\). With that the implementation of Listing¬†5.3 in on_policy_mc_control_soft is actually quite similar to the code used Example¬†5.3 for MC exploring starts. In a sense on-policy \\(\\varepsilon\\)-soft MC control could be thought of as Monte Carlo with exploring episodes.\n\nfrom collections import Counter\n\n\ndef generate_episode_soft(\n    env: rt.RaceTrack,\n    œÄ: dict[rt.State, rt.Action],\n    Œµ: float,\n    rng: random.Random,\n    start_pos=None,\n    max_lenght=None,\n):\n    states = []  # S_0, ..., S_T\n    rewards = []  # R_0, ..., R_T\n    actions = []  # A_0, ..., A_{T-1}\n\n    if start_pos:\n        state, reward, terminated = env.set_state(rt.State(start_pos, (0, 0)))\n    else:\n        state, reward, terminated = env.reset()\n\n    states.append(state)\n    rewards.append(reward)\n\n    ep_length = 0\n    while not terminated:\n        if (not max_lenght is None) and ep_length &gt;= max_lenght:\n            raise RuntimeError(\"episode seems to be too long\")\n\n        ep_length += 1\n\n        greedy_action = œÄ[state]\n        # Œµ-greedy behaviour policy\n        if rng.random() &lt; Œµ:\n            action = rng.choice(env.action_space)\n        else:\n            action = greedy_action\n\n        actions.append(action)\n\n        state, reward, terminated = env.step(action)\n\n        states.append(state)\n        rewards.append(reward)\n\n    return states, rewards, actions\n\n\ndef on_policy_mc_control_soft(env: rt.RaceTrack, n_episodes, Œµ, rng: random.Random):\n    q = {\n        (state, action): rng.random()\n        for state in env.state_space\n        for action in env.action_space\n    }\n    œÄ = {\n        state: max(env.action_space, key=lambda action: q[(state, action)])\n        for state in env.state_space\n    }\n    c = {(state, action): 0 for state in env.state_space for action in env.action_space}\n\n    # record loss for diagnostic\n    loss = []\n\n    for _ in range(n_episodes):\n        # make an episode\n        states, rewards, actions = generate_episode_soft(env, œÄ, Œµ, rng)\n\n        # first visit\n        G = 0\n        state_actions = [(states[i], actions[i]) for i in range(len(actions))]\n        visit_counter = Counter(state_actions)\n        for i in reversed(range(len(actions))):\n            G += rewards[i + 1]\n            sa = state_actions[i]\n\n            if visit_counter[sa] &gt; 1:\n                visit_counter[sa] += -1\n            else:\n                # update q\n                c[sa] += 1\n                q[sa] += (G - q[sa]) / c[sa]\n\n                # update œÄ\n                state = sa[0]\n                œÄ[state] = max(env.action_space, key=lambda a: q[(state, a)])\n\n        loss.append(-G)\n\n    return œÄ, q, loss\n\n\n\nTraining the small example\nSo let‚Äôs train an \\(\\varepsilon\\)-soft agent for env_tight with \\(\\varepsilon = 0.1\\) and plot the loss that occured during the training. The term loss is generally used in gradient descent methods and is the thing which is tried to be minimized. I thought it would be a good fit here. In our case the loss is just the length of the episode. The shorter the episode, the better.\n\n\nCode\n# preparation\nrng = random.Random(10)\nenv_tight.rng = rng\nenv_tight.prob_steering_failure = 0.1\n\n# training\nŒµ = 0.1\nn_episodes = 500\nœÄ_tight, q_tight, loss_tight = on_policy_mc_control_soft(\n    env_tight,\n    n_episodes,\n    Œµ,\n    rng,\n)\n\n\n# plotting\nwindow = 20\n\nloss_avg = np.convolve(loss_tight, np.ones(window) / window, mode=\"valid\")\nplt.plot(\n    range(1, n_episodes + 1),\n    loss_tight,\n    label=\"Loss (episode length)\",\n    alpha=0.3,\n)\nplt.plot(\n    range(1, n_episodes + 1)[window - 1 :],\n    loss_avg,\n    label=f\"Average over {window} episodes\",\n)\nplt.plot(\n    range(1, n_episodes + 1),\n    [4] * n_episodes,\n    linestyle=\"--\",\n    label=f\"Length optimal episode\",\n)\nplt.yscale(\"log\")\nplt.title(\"Loss during training on `env_tight`\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nOverall, the loss doesn‚Äôt go up, and also has some significant drops, which is good. We can also see that in this training run there is only a single episode that reaches the optimal running length of 4 and in the later episodes it regularily gets to 5. Also note that the first couple of episodes had enourmus lengts for the of over 10,000. This is the problem balancing explotaition and exploration again. If I had choosen a higher \\(\\varepsilon\\) then the first couple of episodes would be smaller but the agent had less possibilities to improve as more often it‚Äôs actions are out of its control.\nThese very long intial episodes are also a reason why it‚Äôs a bad idea to use every-visit MC for this problem. In every-visit MC these long episodes give lots of unhelpful rewards, I‚Äôm pretty sure that some episodes have more every-visits in the first 10 episodes than first-visit in the total run of the training. Also, in this problem good policies usually produce episodes that visit every state once anyways.\nNow, let‚Äôs see what the trained policy produces without steering failures:\n\n\nCode\nenv_tight.prob_steering_failure = 0.0\nstates, _, _ = generate_episode_soft(\n    env_tight,\n    œÄ_tight,\n    0.0,\n    rng,\n    max_lenght=200,\n)\n\nplot_episode(env_tight, states)\n\n\n\n\n\n\n\n\n\nAs seen in the loss, the policy prefers taking the right path. Mostl likely, going right is optimal for \\(\\varepsilon\\)-soft policies with \\(\\varepsilon=0.1\\).\nThis is a demonstration thet soft policies can to avoid optimal behaviour if they require clutch control without error for failure.\n\nThe big tracks\nNow let‚Äôs turn our attention to the original problems (Sutton and Barto 2018, fig. 5.5). The code that we have used so far is too slow for these problems and also for the approach I have chosen learning requires hundreds of millions of episodes on these problems. So I have used a script in scripts/solving_race_track/train_left_track.py and scripts/solving_race_track/train_right_track.py to train policies for a couple of hours. Although, if you want really fast code I wouldn‚Äôt use python.\nWe start with the right track, because surprisingly it‚Äôs easier to solve. It‚Äôs an interesting observation. The right track has a bigger state space but is easier for our MC method, since the space of relevant states, states that actually matter for a typical run is smaller. This is because optimised episodes for the right track are actually shorter, as the finishing line has a lower y-coordinate, and thus can be reached in fewer steps. Also intiially‚Äîwhere randomness very high‚Äîthe average episode lengths are shorter.\nThis code loads the trained policy, the environment and also the loss during training.\n\n\nCode\nimport pickle\n\nwith open(\"results/race_track_right.pkl\", \"rb\") as file_in:\n    result = pickle.load(file_in)\n\nn_episodes_right = result[\"num_episodes\"]\nœÄ_right = result[\"policy\"]\nŒµ_max_right = result[\"epsilon_max\"]\nŒµ_min_right = result[\"epsilon_min\"]\nenv_right = result[\"env\"]\nenv_right.prob_steering_failure = 0.0\nloss_means_right = result[\"loss_means\"]\nloss_means_starts_right = result[\"loss_means_starts\"]\nloss_means_window_right = result[\"loss_means_window\"]\n\n\nprint(f\"The policy was trained for {n_episodes_right:,} episodes.\")\nprint(f\"The Œµ decreased exponentially from {Œµ_max_right} to {Œµ_min_right}\")\n\n\nThe policy was trained for 1,000,000,000 episodes.\nThe Œµ decreased exponentially from 0.25 to 0.025\n\n\nIn many parts of the training the loss decreases like \\(n^{-m}\\) for some \\(m &lt; 1\\). This is a typical behavior for Monte Carlo methods. I have highlighted one part where it scales approximately like \\(n^{-1/4}\\). I don‚Äôt know why the scaling is how it is.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSTART, END = 750, 1150\n\n# Slice data\nx = loss_means_starts_right[START:END]\ny = loss_means_right[START:END]\n\n# Log-transform for linear fitting\nlog_x, log_y = np.log(x), np.log(y)\n\n# Fit linear trend in log-log space\nslope, intercept = np.polyfit(log_x, log_y, 1)\ndecay_rate = np.exp(intercept) * x**slope\ndecay_rate_str = f\"{slope:.2f}\"\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(loss_means_starts_right, loss_means_right, label=\"Loss\")\nplt.plot(x, decay_rate, label=rf\"fit: $n^{{{decay_rate_str}}}$\")\n\n# Formatting\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.legend()\nplt.title(f\"Loss averaged over {loss_means_window_right:,} episodes\")\nplt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nNow let‚Äôs see how the trajectories look like that this policy returns.\n\n\n\n\n\n\nCode\nstates, _, _ = generate_episode_soft(\n    env_right,\n    œÄ_right,\n    0.0,\n    rng,\n    max_lenght=200,\n    start_pos=(0, 0),\n)\n\nplot_episode(env_right, states)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The policy finds a good trajectory from all the left.\n\n\n\n\n\n\nCode\nstates, _, _ = generate_episode_soft(\n    env_right,\n    œÄ_right,\n    0.0,\n    rng,\n    max_lenght=200,\n    start_pos=(20, 0),\n)\n\nplot_episode(env_right, states)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The policy also finds a good trajector from the some starting positions on the right.\n\n\n\n\n\n\nCode\nrng = random.Random(8735)\nenv_right.rng = rng\n\nstates, _, _ = generate_episode_soft(\n    env_right,\n    œÄ_right,\n    0.0,\n    rng,\n    max_lenght=200,\n    start_pos=(1, 0),\n)\n\nplot_episode(env_right, states)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) For some other starting positions the policy just prefers to crash the car and gamble for anoter staring spot\n\n\n\n\n\n\n\nFigure¬†5.7: The policy has found pretty good strategies for some positions, as shown in (a) and (b). Other positions like the one shown in the last picture have a policy of crashing. , which rerols the starting position. In this example here, there are a couple of crashes until the car gets set to a position for which the policy knows how to finish the race.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#appendix",
    "href": "chapters/05-monte-carlo-methods.html#appendix",
    "title": "5¬† Monte Carlo Methods",
    "section": "5.8 üóê Appendix",
    "text": "5.8 üóê Appendix\n\n5.8.1 Monte Carlo is Slow\nThe biggest problem is that MC simulation improves very slowly on average. The rate of convergence can be estimate using the central limit theorem that says that the distribution of the sample mean \\(\\bar{X}_n\\) of \\(n\\) samples \\(X_1,\\dots,X_n\\) approximates a normal distribution center around the true mean \\(\\mu\\): \\[\n\\bar{X}_n \\overset{d}{\\to} \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}),\n\\]\nwhere \\(\\sigma^2\\) is the true variance of the trials. We don‚Äôt know \\(\\mu\\) or \\(\\sigma\\) but we can use this to understand the rate of convergence of MC simulation.\nAs we are working with quite big \\(n\\) in MC simulations we‚Äôll just do a leap of faith and work just say use \\(\\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\\) for the distribution of \\(\\bar{X}_n\\) directly (in general this convergence is of the order \\(\\frac{1}{\\sqrt{n}})\\), see the Berry‚ÄìEsseen theorem, but we will ignore it here). Using the normal approximation this gives us that with a bit more than 95% probability \\(\\bar{X}_n\\) falls within the two sigma interval \\([\\mu - \\frac{2\\sigma}{\\sqrt{n}}, \\mu + \\frac{2\\sigma}{\\sqrt{n}}]\\). If we want to cut this interval in half, i.e., make \\(\\frac{2\\sigma}{\\sqrt{n}}\\) half as big, we have to quadruple the number of trials \\(n\\), and if we want to get another decimal point in precision we need 100 times more samples. That is really slow.\nA saving grace for MC could be that it is very easy to scale in parallel computations but I don‚Äôt know how much that helps in real live and here in the real of these notes we are not dealing with any parallelisation.\n\n\n5.8.2 Confidence intervals\nWe also should provide some error bars or something for the point estimate. The standard MC approach here is to basically use our estimates for the mean and standard deviations and drop them in \\[\n\\begin{split}\n\\hat{\\mu} &= \\bar{X}_n = \\frac{X_1 + \\dots X_n}{n} \\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2\n\\end{split}\n\\]\nand say that \\(\\mu \\in [\\hat{\\mu} - \\frac{2\\hat{\\sigma}}{\\sqrt{n}}, \\mu + \\frac{2\\hat{\\sigma}}{\\sqrt{n}}]\\) with probability 95%. Well‚Ä¶ not quite we can only say that before we sampled our data that the true value will be in this interval with probability around 95%. Maybe there is a Bayesian reasoning that actually allows the former formulation, but let‚Äôs not get bogged down in to much scientific philosophy here and just follow good practise.\n\n\n5.8.3 The strip problem\nConsider the following discrete harmonic problem. The domain is a \\(3\\times(2r+1)\\) grid (\\(3\\) rows and \\(2r+1\\) columns). All boundary cells have value 0 except for the two boundary cells on the middle row‚Äîthe cells at \\((1,0)\\) and \\((1,2r)\\)‚Äîwhich have value 1.\nBecause the top and bottom rows are entirely boundary, the only non-trivial values lie along the middle row. Let \\[\nf(i)=\\text{value at } (1,i)\\quad ,i=0,1,\\dots,2r.\n\\]\nBy the boundary conditions and the four-neighbour averaging rule, the interior values satisfy \\[\n\\begin{split}\nf(0) &= f(2r) = 1, \\text{and} \\\\\nf(i) &= \\frac{1}{4}(f(i-1) + f(i+1)) \\text{ for } 0 &lt; i &lt; 2r.\n\\end{split}\n\\tag{5.7}\\]\nThe theorem below gives the unique solution. In particular, the value at the centre, \\(f(r)\\), is given by \\(1/g(r)\\).\n\nTheorem 5.1 The solution \\(f(i)\\) for Equation¬†5.7 is given by \\[\nf(i) = \\frac{g(|i-r|)}{g(r)}, \\quad i = 0,1,\\dots,2r\n\\tag{5.8}\\] where \\(g \\colon \\mathbb{N} \\to \\mathbb{N}\\) is defined by \\[\ng(n) = \\begin{cases} 1 &\\text{if }n=0\\\\\n2 &\\text{if }n=1\\\\\n4g(n-1) - g(n-2) &\\text{if }n&gt;1\n\\end{cases}\n\\]\n\n\nProof. Consider first the infinite version of the recurrence \\[\nf(i) = \\frac{1}{4}(f(i-1)+ f(i+1)) \\quad i \\in \\mathbb{Z}.\n\\tag{5.9}\\] Place a reference cell at \\(i=0\\) with value \\(1\\), and look for a symmetric solution about this cell. Let \\(g(n)\\) denote the value at distance \\(n\\) from the centre. Then we have clearly \\(g(0) = 1\\). Also symmetry and Equation¬†5.7 force the centre to be the average of its two neighbours, so \\(g(1) = 2\\) because the center cell is the average of the two adjacent cells. Applying Equation¬†5.9 at distance \\(n&gt;1\\) yields the recurrenc \\(g(n) = 4g(n-1) - g(n-2)\\).\nSince Equation¬†5.9 is linear, any scalar multiple of \\(g\\) is also a solution. For the finite strip, we rescale by \\(g(r)\\) so that the values at distance \\(r\\)‚Äîthe two boundary cells \\((1,0)\\) and \\((1,2r)\\)‚Äîare equal to 1. This gives the formula in Equation¬†5.8.\n\n\n\n\n\nAgapiou, Sergios, Omiros Papaspiliopoulos, Daniel Sanz-Alonso, and Andrew M. Stuart. 2017. ‚ÄúImportance Sampling: Intrinsic Dimension and Computational Cost.‚Äù Statistical Science 32 (3): 405‚Äì31. https://doi.org/10.1214/17-STS611.\n\n\nEven-Dar, Eyal, Shie Mannor, and Yishay Mansour. 2006. ‚ÄúAction Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems.‚Äù J. Mach. Learn. Res. 7 (December): 1079‚Äì1105.\n\n\nOprea, John. 2000. The Mathematics of Soap Films: Explorations with Maple: Explorations with Maple. Fields Institute Communications. American Mathematical Society.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/05-monte-carlo-methods.html#footnotes",
    "href": "chapters/05-monte-carlo-methods.html#footnotes",
    "title": "5¬† Monte Carlo Methods",
    "section": "",
    "text": "originally it says ‚Äòrear‚Äô but it my diagram it‚Äôs the top‚Ü©Ô∏é\noriginally it says whole last row on the left, since the ace was next to 2. I have arranged ace next to 10 though.‚Ü©Ô∏é\nMaybe negative_velocity_components would have been a better name.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Monte Carlo Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06-temporal-difference-learning.html",
    "href": "chapters/06-temporal-difference-learning.html",
    "title": "6¬† Temporal-Difference Learning (Still in Progress üî®)",
    "section": "",
    "text": "6.1 TD Prediction\nTemporal-difference (TD) learning is a blend of dynamic programming (DP) and Monte Carlo (MC).\nLike DP it uses learned estimates to bootstrap new estimates, but also includes experience like MC does.\nThe update formula for every-visit Monte Carlo with a constant learning rate is \\[\nV(S_t) \\gets V(S_t) + \\alpha [\\underbrace{G_t - V(S_t)}_\\text{MC error}]\n\\tag{6.1}\\] where \\(G_t\\) is the actual return following time \\(t\\).\nTD(0) replaces \\(G_t\\) by the immediate return and the currant estimate \\[\nV(S_t) \\gets V(S_t) + \\alpha [\\underbrace{R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)}_\\text{TD error}]\n\\tag{6.2}\\] the TD error for time \\(t\\) is denoted by \\(\\delta_t\\) \\[\n\\delta_t := R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n\\tag{6.3}\\]\nHere is the complete algorithm\nThe algorithm is an on-line version of TD(0). It updates the value function after every step in the episode.\nThe off-line version of TD(0), updates the value function \\(V\\) at the end of an episode and this \\(V\\) is constant for the TD errors over an episode. This is more similar to the Monte Carlo update regime which updates \\(V\\) at the end of the episode and we can write the MC error in terms of TD errors in the offline case: \\[\n\\begin{split}\nG_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\\\\n&= \\delta_t + \\gamma (G_{t+1}-V(S_{t+1})) \\\\\n&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + \\dots \\gamma^{T-t}(G_T - V(S_T)) \\\\\n&= \\sum_{k=t}^{T-1} \\gamma^{k-t}\\delta_k\n\\end{split}\n\\tag{6.4}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Temporal-Difference Learning (Still in Progress üî®)</span>"
    ]
  },
  {
    "objectID": "chapters/06-temporal-difference-learning.html#td-prediction",
    "href": "chapters/06-temporal-difference-learning.html#td-prediction",
    "title": "6¬† Temporal-Difference Learning (Still in Progress üî®)",
    "section": "",
    "text": "Listing¬†6.1: Tabular TD(0) for estimating \\(v_{\\pi}\\)\n\n\nInput: \\(\\pi\\), the policy to be evaluated\nAlgorithm parameter: step-size \\(\\alpha \\in (0,1]\\)\nInitialisation: \\(V(s)\\), for all \\(s \\in \\mathcal{S}^+\\) arbitrarily and \\(V(\\mathrm{terminal}) = 0\\)\nLoop forever:\n¬†¬†¬†¬†\\(S \\gets\\) starting state from new episode\n¬†¬†¬†¬†Loop until \\(S\\) is terminal:\n¬†¬†¬†¬†¬†¬†¬†¬†\\(A \\gets\\) action given by \\(\\pi\\) for \\(S\\)\n¬†¬†¬†¬†¬†¬†¬†¬†Take action \\(A\\), observe \\(R,S'\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(V(S) \\gets V(S) + \\alpha [R + \\gamma V(S') - V(S)]\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(S \\gets S'\\)\n\n\n\n\n\n\nExercise 6.1 If \\(V\\) changes during the episode, then Equation¬†6.4 only holds approximately; what would the difference be between the two sides? Let \\(V_t\\) denote the array of state values used at time \\(t\\) in the TD error Equation¬†6.3 and in the TD update Equation¬†6.2. Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n\n\nSolution 6.1. Let \\[\nV_{t+1}(S_t) \\gets V_t(S_t) + \\alpha[R_{t+1} + \\gamma V_{t}(S_{t+1}) - V_t(S_t)]\n\\] be the per step update rule with TD error \\[\n\\delta_t = R_{t+1} + \\gamma V_{t}(S_{t+1}) - V_t(S_t)\n\\] for the on-line TD(0) algorithm.\nWe can expand the Monte Carlo error as follows: \\[\n\\begin{split}\nG_t - V_t(S_t) &= R_{t+1} + \\gamma G_{t+1} - V_t(S_t) + \\gamma V_t(S_{t+1}) - \\gamma V_t(S_{t+1}) \\\\\n&= \\delta_t + \\gamma(G_{t+1} - V_t(S_{t+1})) \\\\\n&= \\delta_t +  \\gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1})) + \\gamma(G_{t+1} - V_{t+1}(S_{t+1}))\n\\end{split}\n\\] Iterating this as above for the static case gives \\[\nG_t - V_t(S_t) = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k + \\sum_{k=t}^{T-1}\\gamma^{k-t+1} (V_{t+1}(S_{t+1}) - V_t(S_{t+1})).\n\\] A quick pedantic note, the last summand in the correction term is always zero, as \\(V_k(S_T) = 0\\) for all \\(k\\) by definition.\nSo far we have only used the definition of the TD error as above. We can simplify the correction term by using if we use the update rule. It states that from \\(V_k\\) to \\(V_{k+1}\\) only \\(S_k\\) changes, i.e., \\(V_k(s) = V_{k+1}(s)\\) if \\(s \\neq S_k\\). In particular \\(V_k(S_{k+1}) - V_{k+1}(S_{k+1}) = 0\\) if \\(S_k \\neq S_{k+1}\\) and also \\[\n\\begin{split}\nV_{k+1}(S_{k}) - V_k(S_{k}) = \\alpha \\delta_k\n\\end{split}\n\\]\nWith this we get \\[\nG_t - V_t(S_t) = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k + \\alpha \\sum_{k=t}^{T-1}\\gamma^{k-t+1} \\mathbf{1}[S_{k+1} = S_k]\\delta_k.\n\\]\n\n\nExample 6.1 Driving Home (Sutton and Barto 2018, Example 6.1)\nThis is a placeholder for the driving home example.\n\n\nExercise 6.2 This is an exercise to help develop your intuition about why TD methods are often more efficient than Monte Carlo methods. Consider the driving home example and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario in which a TD update would be better on average than a Monte Carlo update? Give an example scenario‚Äîa description of past experience and a current state‚Äîin which you would expect the TD update to be better. Here‚Äôs a hint: Suppose you have lots of experience driving home from work. Then you move to a new building and a new parking lot (but you still enter the highway at the same place). Now you are starting to learn predictions for the new building. Can you see why TD updates are likely to be much better, at least initially, in this case? Might the same sort of thing happen in the original scenario?\n\n\nSolution 6.2. Sheesh, another ‚Äòno clear answers‚Äô exercise. I fear those the most.\nLet‚Äôs go and take their hint.\nLet‚Äôs say everything until ‚Äòexiting highway‚Äô stays the same. Now, while the values for the rest of the journey might fluctuate a lot (trying out new routes, getting to know the streets) in TD(O) these changes don‚Äôt affect everything before exciting highway.\nBut this has to become a better example still",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Temporal-Difference Learning (Still in Progress üî®)</span>"
    ]
  },
  {
    "objectID": "chapters/06-temporal-difference-learning.html#advantages-of-td-prediction-models",
    "href": "chapters/06-temporal-difference-learning.html#advantages-of-td-prediction-models",
    "title": "6¬† Temporal-Difference Learning (Still in Progress üî®)",
    "section": "6.2 Advantages of TD Prediction Models",
    "text": "6.2 Advantages of TD Prediction Models\n\nExample 6.2 Random Walk (Sutton and Barto 2018, Example 6.2)\nLet‚Äôs compare Temporal Difference (TD) learning and Monte Carlo (MC) methods using a simple Markov Reward Process (MRP). The MRP consists of five non-terminal states, \\(1, \\dots, 5\\) and two the terminal state \\(0\\) and \\(6\\)‚Äã.1 In this MRP, each state \\(i\\)‚Äã transitions to the adjacent states \\(i‚àí1\\)‚Äã and \\(i+1\\)‚Äã with a probability of \\(0.5\\) each. All rewards are \\(0\\), except the transition from \\(5\\) to \\(6\\)‚Äã, which yields a reward of 1. This setup is illustrated in the following diagram:\n\n\n\n\nWith this setup the true value \\(v^*(i)\\) of a state is equal to the probability that a random walk starting at this state exits via the right side, and this turns out to be exactly \\(v^*(i) = i/6\\).\n\nSimplified Update Rule\nWe can simplify the update rules for every-visit MC and TD(0) and get rid of the rewards by incorporating them into the values of the terminal states. We require for any value function to have \\(V(0) = 0\\) and \\(V(6) = 1\\). Then the MC update (Equation¬†6.1) becomes \\[\nV(S_t) \\gets V(S_t) + \\alpha (V(S_T) - V(S_t))\n\\] and the TD(0) (Equation¬†6.2) update becomes \\[\nV(S_t) \\gets V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)]\n\\] all for \\(t &lt; T\\).\nThis is implemented in the code below\n\n# === update rules for mc and td ===\n\n\ndef mc_every_visit_update(episode: list[int], V: list[float], Œ±):\n    for s in episode:\n        V[s] += Œ± * (V[episode[-1]] - V[s])\n\n\ndef one_step_td_update(episode: list[int], V: list[float], Œ±):\n    for s, s_next in zip(episode[:-1], episode[1:]):\n        V[s] += Œ± * (V[s_next] - V[s])\n\nLet‚Äôs see what kind of learning behaviour this results for DT in a concrete example. Let us generate a couple of episodes‚Ä¶\n\n\nCode\n# === generate bunch of random walk episodes ===\nimport random\n\n\n# data generation\ndef generate_random_walk_episode(env_size, rng):\n    midpoint = (env_size + 1) // 2\n\n    state = midpoint\n    states = [state]\n    while 0 &lt; state &lt; (env_size + 1):\n        direction = 1 if rng.random() &lt; 0.5 else -1\n        state = state + direction\n        states.append(state)\n\n    return states\n\n\ndef generate_random_walk_dataset(n_episodes, batch_size, rng=None, env_size=5):\n    if rng is None:\n        rng = random.Random()\n\n    dataset = []\n    for _ in range(n_episodes):\n        episode_batch = [\n            generate_random_walk_episode(env_size, rng) for _ in range(batch_size)\n        ]\n        dataset.append(episode_batch)\n    return dataset\n\n\nrng = random.Random(5)\nrandom_walk_batches = generate_random_walk_dataset(\n    env_size=5, n_episodes=100, batch_size=1, rng=rng\n)\n\nprint(f\"Generated {len(random_walk_batches)} random walk episodes!\")\n\n\nGenerated 100 random walk episodes!\n\n\n‚Ä¶ and see how the initial estimate of \\(V(i) = 0.5\\) evolves over time using this update rule.\n\nCode\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n\n# helpers\ndef create_estimates_history(episode_batches, init_V, update_fun, Œ±):\n    batch_size = len(episode_batches[0])\n    batch_V = np.array([init_V.copy() for _ in range(batch_size)])\n\n    def mean():\n        return np.mean(batch_V, axis=0)[1:-1]\n\n    def std_err():\n        return np.std(batch_V, axis=0)[1:-1]\n\n    estimates_history = [(mean(), std_err())]\n\n    for episode_batch in episode_batches:\n        for i, episode in enumerate(episode_batch):\n            update_fun(episode, batch_V[i], Œ±=Œ±)\n        estimates_history.append((mean(), std_err()))\n\n    return estimates_history\n\n\ndef compute_true_values(env_size):\n    return [i / (env_size + 1) for i in range(0, env_size + 2)]\n\n\ndef plot_estimates_history(\n    estimates_history,\n    runs_to_plot,\n    confidence_bands=[],\n    title=None,\n    err_factor=1,\n):\n    env_size = len(estimates_history[0][0])\n    x = range(1, env_size + 1)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot each run\n\n    for run in runs_to_plot:\n        if run &lt; len(estimates_history):\n            mean_estimates, std_errs = estimates_history[run]\n            label = f\"Episode {run}\"\n            (line,) = ax.plot(\n                x,\n                mean_estimates,\n                marker=\"o\",\n                label=label,\n                zorder=3,\n            )\n            if run in confidence_bands:\n                std_errs_ = [err_factor * std_err for std_err in std_errs]\n                ax.fill_between(\n                    x,\n                    [m - s for m, s in zip(mean_estimates, std_errs_)],\n                    [m + s for m, s in zip(mean_estimates, std_errs_)],\n                    color=line.get_color(),\n                    alpha=0.2,\n                )\n\n    ax.set_ylim(0, 1)\n    ax.set_xticks(range(1, env_size + 1))\n    ax.set_xlabel(\"State\", fontsize=14)\n    ax.set_ylabel(\"Estimated Value\", fontsize=14)\n    if title is not None:\n        ax.set_title(title)\n    ax.legend()\n\n    true_values = compute_true_values(env_size)\n    ax.plot(\n        x, true_values[1:-1], label=\"True values\", alpha=0.5, color=\"black\", linewidth=1\n    )\n    return fig, ax\n\n\n# create estimates and plot\nenv_size = 5\nŒ±_mt = 0.1\n\ninit_estimates = [0.5] * (env_size + 2)\ninit_estimates[0] = 0.0\ninit_estimates[-1] = 1.0\n\ntrue_values = compute_true_values(env_size)\n\nestimates_history_td = create_estimates_history(\n    random_walk_batches,\n    init_estimates,\n    one_step_td_update,\n    Œ±_mt,\n)\n\n\nfig, ax = plot_estimates_history(\n    estimates_history_td,\n    runs_to_plot=[0, 1, 10, 100],\n    title=f\"Estimates development under TD(0)-updates for Œ±={Œ±_mt}\",\n)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.1: Estimates development under TD(0)-updates. The true values lie on the gray line.\n\n\n\nWe will discuss Figure¬†6.1 a bit more in an exercise.\nNext we compare the performance of every-visit MC and TD(0). We can use the root mean-squared error across states as performance measure:\n\\[\n\\mathrm{RSME}(V) = \\sqrt{\\frac{1}{5}\\sum_{i=1}^5 (V(i)- v^*(i))^2}.\n\\]\n(This is proportional to the Euclidean distance of the estimate and the true values, and I think RSME is a bit of a misleading name, but here I stick to the source)\nThe next plot shows the development of the averaged RMSE over time.\n\nCode\n# === RMSE comparison for MC and TD(0) ===\n\n\n# helper\ndef euclid_distance(true_values, estimates) -&gt; float:\n    # exclude terminal state in distance calculation\n    diffs = (np.array(estimates) - np.array(true_values))[1:]\n    return float(np.sqrt(np.mean(diffs**2)))\n\n\ndef create_error_history(\n    dataset,\n    init_estimates,\n    update_fun,\n    true_values,\n    Œ±=0.1,\n) -&gt; list[float]:\n    n_runs = len(dataset[0])\n\n    estimates_batch = [init_estimates.copy() for _ in range(n_runs)]\n\n    def mean_distance() -&gt; float:\n        return float(\n            np.mean([euclid_distance(true_values, v) for v in estimates_batch])\n        )\n\n    rms_history: List[float] = [mean_distance()]\n\n    for episode_batch in dataset:\n        for run_idx, episode in enumerate(episode_batch):\n            update_fun(episode, estimates_batch[run_idx], Œ±=Œ±)\n        rms_history.append(mean_distance())\n\n    return rms_history\n\n\n# data generation and plot\nbatch_size = 100\nn_episodes = 100\nrng = random.Random(0)  # reproducible data\n\ndataset = generate_random_walk_dataset(\n    n_episodes=n_episodes, batch_size=batch_size, rng=rng\n)\n\nalphas_td = [0.15, 0.1, 0.04]\nalphas_mc = [0.01, 0.03]\n\nlines_td = []\nlines_mc = []\nfor Œ± in alphas_td:\n    rms = create_error_history(\n        update_fun=one_step_td_update,\n        true_values=true_values,\n        dataset=dataset,\n        init_estimates=init_estimates,\n        Œ±=Œ±,\n    )\n    (line,) = plt.plot(rms, label=f\"Œ±={Œ±}\")\n    lines_td.append(line)\nfor Œ± in alphas_mc:\n    rms = create_error_history(\n        update_fun=mc_every_visit_update,\n        true_values=true_values,\n        dataset=dataset,\n        init_estimates=init_estimates,\n        Œ±=Œ±,\n    )\n    plt.plot(rms, label=f\"Œ±={Œ±}, mc\", linestyle=\"--\")\n    (line,) = plt.plot(rms, label=f\"Œ±={Œ±}\", linestyle=\"--\")\n    lines_mc.append(line)\n\n# Create legends for TD and MC separately\ntd_legend = plt.legend(\n    handles=lines_td,\n    labels=[f\"Œ±={Œ±}\" for Œ± in alphas_td],\n    title=\"TD(0)\",\n)\nplt.gca().add_artist(td_legend)\n\nmc_legend = plt.legend(\n    handles=lines_mc,\n    labels=[f\"Œ±={Œ±}\" for Œ± in alphas_mc],\n    title=\"ev. MC\",\n    bbox_to_anchor=(1.0, 0.75),\n)\nplt.gca().add_artist(mc_legend)\n\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Mean RMSE\")\nplt.title(f\"RMSE Comparison (averaged over {batch_size} runs)\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.2: Comparison between every visit MC and TD(0) for different values of \\(\\alpha\\). Performance is measured by RMSE across the states.\n\n\n\nSince TD(0) and every-visit MC have different speeds in how they update their estimates, we need to compare them using different ranges of the learning rate in Figure¬†6.2. Still, we can see that TD performs better: it has a better mix between fast convergence and asymptotic‚Äîif we choose the best step-size parameters such that TD and MC approach the same asymptotic performance, TD gets there faster.\nLet‚Äôs analyse the two approaches in another way, to see how their bias and variance behaves. We show the average estimates and their standard deviations per state for both TD and MC for 100 with update-rates such that they have basically stopped improving around the 100 episode mark (i.e.¬†their at their asymptotic performance).\n\n\n\n\n\n\nCode\n# === td history ===\n\nrng = random.Random(7)\nenv_size = 5\nbatch_size = 200\n\nrandom_walk_batches = generate_random_walk_dataset(\n    env_size=env_size, n_episodes=100, batch_size=batch_size, rng=rng\n)\n\nŒ±_td = 0.07\nestimates_history_td = create_estimates_history(\n    random_walk_batches,\n    init_estimates,\n    one_step_td_update,\n    Œ±_td,\n)\n\nerr_factor = 1\nfig, ax = plot_estimates_history(\n    estimates_history_td,\n    runs_to_plot=[0, 15, 30, 50, 100],\n    confidence_bands=[15, 100],\n    title=f\"TD Œ±={Œ±_td} (averaged over {batch_size} runs)\",\n    err_factor=err_factor,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) One-step TD.\n\n\n\n\n\n\nCode\nŒ±_mc = 0.04\n# === mc history ===\n\nestimates_history_mc = create_estimates_history(\n    random_walk_batches,\n    init_estimates,\n    mc_every_visit_update,\n    Œ±_mc,\n)\n\n\nfig, ax = plot_estimates_history(\n    estimates_history_mc,\n    runs_to_plot=[0, 15, 30, 50, 100],\n    confidence_bands=[15, 100],\n    title=f\"MC Œ±={Œ±_mc} (averaged over {batch_size} runs)\",\n    err_factor=err_factor,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) MC every visit\n\n\n\n\n\n\n\nFigure¬†6.3: Comparison of every-visit MC and TD(0) averaged. The learning rates are chosen such that for both there are no changes in expectation after more than 100 episodes, i.e., the means are stable for episodes 100. For the Episode counts 15 and 100 we show the confidence bands for the estimates.\n\n\n\nThe three biggest insights from Figure¬†6.3 are:\n\nevery-visit MC converges faster to the true values in expectation\nTD(0) has much lower standard error\nin fact, TD(0) doesn‚Äôt converge to the true values in expectation! It has an asymptotic bias.\n\n\n\n\nExercise 6.3 From the results shown in Figure¬†6.1 of the random walk example it appears that the first episode results in a change in only \\(V(n_1)\\)2. What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed?\n\n\nSolution 6.3. The errors in the TD updates are \\(V(S_{t+1} - V(S_t))\\) for all \\(t &lt; T\\). Since in the initial estimate all states have the same estimated value, these all vanish for \\(t &lt; T-1\\).\nThe last TD error for \\(t = T-1\\) is \\([V(S_T) - V(S_{T-1})]\\). We can see that \\(1\\) got updated so the episode excited on the left thus \\(S_T = 0\\). This makes the error \\([0 - 0.5] = -0.5\\) and the update is \\(\\alpha \\cdot (-0.5) = 0.1 \\cdot (-0.5) = -0.05\\).\n\n\nExercise 6.4 The specific results shown Figure¬†6.2 of the random walk example are dependent on the value of the step-size parameter, \\(\\alpha\\). Do you think the conclusions about which algorithm is better would be affected if a wider range of \\(\\alpha\\) values were used? Is there a different, fixed value of \\(\\alpha\\) at which either algorithm would have performed significantly better than shown? Why or why not?\n\n\nSolution 6.4. No, the conclusion is that TD gets faster to its asymptotic is independent of the step-size.\nThe answer to the second questions depends on what is meant by better? The best step-size depends on what we want, how fast or how accurate, which are in conflict with each other. So basically any reasonable \\(\\alpha\\) is optimal for some scenario.\n\n\nExercise 6.5 In Figure¬†6.2 of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high \\(\\alpha\\)‚Äôs. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized?\n\n\nSolution 6.5. Interesting question. I think the underlying answer to this question is, that the TD(0) method is asymptotically biased, which we will discuss now.\n\nMoving towards the asymptotic bias\nIn Figure¬†6.2 we can see for \\(\\alpha = 0.15\\) quite a visible dip with it‚Äôs lowest point around 20 and at 60 the performance looks stable. Let‚Äôs see how the average estimates look like over this time period.\n\n\nCode\nrng = random.Random(0)\nenv_size = 5\nŒ±_exr_6_5 = 0.15\nbs_exr_6_5 = 400\n\nrandom_walks_exr_6_5 = generate_random_walk_dataset(\n    env_size=env_size, n_episodes=200, batch_size=bs_exr_6_5, rng=rng\n)\n\n\nestimates_standard_exr_6_5 = create_estimates_history(\n    random_walks_exr_6_5,\n    init_estimates,\n    one_step_td_update,\n    Œ±_exr_6_5,\n)\n\nplot_runs = [0, 10, 20, 60]\nerr_factor = 1\nfig, ax = plot_estimates_history(\n    estimates_standard_exr_6_5,\n    runs_to_plot=plot_runs,\n    title=f\"TD(0) Estimates (Œ±={Œ±_exr_6_5}, {bs_exr_6_5} runs)\",\n)\n\n\n\n\n\n\n\n\n\nWe can see that the estimates slowly approach the gray line, the true values. At episode 20 they are very close but then they go past them and settle at the estimates given at episode 60. So this gives the dip.This asymptotic bias shrinks when \\(\\alpha\\) is smaller, so it is less visible, but the effect is still there.\n\n\nApproching the asymptotic bias from the other side\nIf we use initial estimates that start from the ‚Äòother side‚Äô so the estimates don‚Äôt have to go ‚Äòthrough‚Äô the true values to reach their asymptotic bias, the dip in rmse performance should disappear.\nIndeed, if we start with ‚Äòextreme‚Äô initial estimates\n\n\nCode\ninit_extremes = [0] * 3 + [0.5] + [1] * 3\nprint(f\"The extreme iniital estimates: {init_extremes}\")\n\n\nThe extreme iniital estimates: [0, 0, 0, 0.5, 1, 1, 1]\n\n\nand compare it with the ordinary estimates we can see that both have the same asymptotic rmse but the extreme initial estimates don‚Äôt have the dip.\n\n\n\n\n\n\nCode\nrms_extremes = create_error_history(\n    update_fun=one_step_td_update,\n    true_values=true_values,\n    dataset=random_walks_exr_6_5,\n    init_estimates=init_extremes,\n    Œ±=Œ±_exr_6_5,\n)\n\nrms_standard = create_error_history(\n    update_fun=one_step_td_update,\n    true_values=true_values,\n    dataset=random_walks_exr_6_5,\n    init_estimates=init_estimates,\n    Œ±=Œ±_exr_6_5,\n)\n\nplt.plot(rms_extremes, label=f\"Extreme Initialization\")\nplt.plot(rms_standard, label=f\"Standard Initialization\")\n\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Mean RMSE\")\nplt.title(f\"RMSE Comparison (averaged over {bs_exr_6_5} runs)\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparisson of RMSE for standard and extreme initialization. The extreme initialization doesn‚Äôt only improves in performance over time.\n\n\n\n\n\n\nCode\ninit_extremes = [0] * 3 + [0.5] + [1] * 3\n\nestimates_extreme_exr_6_5 = create_estimates_history(\n    random_walks_exr_6_5,\n    init_extremes,\n    one_step_td_update,\n    Œ±_exr_6_5,\n)\n\nplot_runs = [0, 10, 20, 60]\nerr_factor = 1\nfig, ax = plot_estimates_history(\n    estimates_extreme_exr_6_5,\n    runs_to_plot=plot_runs,\n    title=f\"TD(0) Estimates (Œ±={Œ±_exr_6_5}, {bs_exr_6_5} runs)\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The estimates starting from the extreme initialization approach the asymptotic bias from the ‚Äòother side‚Äô.\n\n\n\n\n\n\n\nFigure¬†6.4: The updates for the extreme initial estimates only improve in rmse performance.\n\n\n\n\n\nOffline TD(0) is consistent\nWe have seen that the TD(0) update rule gives an asymptotically biased estimator for fixed \\(\\alpha\\). If we switch from an on-line update to an off-line update, which uses a ‚Äòfrozen‚Äô \\(V\\) in the updates, the estimator becomes consistent, i.e., the asymptotic bias disappears.\n\n# === offline TD(0) ===\ndef offline_one_step_td_update(episode: list[int], V: list[float], Œ±):\n    V_frozen = V.copy()\n    for s, s_next in zip(episode[:-1], episode[1:]):\n        V[s] += Œ± * (V_frozen[s_next] - V_frozen[s])\n\nNow we can compare offline and online updates.\n\n\n\n\n\n\nCode\nrms_offline = create_error_history(\n    update_fun=offline_one_step_td_update,\n    true_values=true_values,\n    dataset=random_walks_exr_6_5,\n    init_estimates=init_estimates,\n    Œ±=Œ±_exr_6_5,\n)\n\nplt.plot(rms_offline, label=f\"Offline updates\")\nplt.plot(rms_standard, label=f\"Online updates\")\n\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Mean RMSE\")\nplt.title(f\"RMSE Comparison (averaged over {bs_exr_6_5} runs)\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Off-line and On-line versions of TD(0) with the same initial values and step-size \\(\\alpha\\). The offline version doesn‚Äôt show a dip. They seem to have similar performances in the long run for this example.\n\n\n\n\n\n\nCode\nestimates_offline_exr_6_5 = create_estimates_history(\n    random_walks_exr_6_5,\n    init_estimates,\n    offline_one_step_td_update,\n    Œ±_exr_6_5,\n)\n\nplot_runs = [0, 10, 20, 60, 100]\nerr_factor = 1\nfig, ax = plot_estimates_history(\n    estimates_offline_exr_6_5,\n    runs_to_plot=plot_runs,\n    title=f\"offline TD(0) Estimates (Œ±={Œ±_exr_6_5}, {bs_exr_6_5} runs)\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Development of the mean estimates of offline TD(0). The asymptotic means (around episode 100) lie on the true values. So offline TD(0) gives an unbiased estimator.\n\n\n\n\n\n\n\nFigure¬†6.5: RMSE and mean estimates for offline TD(0).\n\n\n\nSo Figure¬†6.5 shows that offline TD(0) is unbiased. However its performance is not better than than the online version. Actually, in my test online TD(0) was always equally good or even better.\n\n\nWhy is offline TD(0) consistent\nSo far all results have been purely empirical. Online TD(0) is also not easy to analyse. However, for offline TD(0) we can make a simple argument for why it‚Äôs consistent on the random walk example (still a bit shy of a proof).\nLet the true value be \\(V(i)=\\frac{i}{6}\\). Fix a state \\(i\\). For a trajectory \\(\\tau\\) let \\(n_{i\\to j}(\\tau)\\) denote the number of times the transition \\(i \\to j\\) occurs in \\(\\tau\\).\nThe offline TD(0) update to \\(V(i)\\) produced by \\(\\tau\\) is \\[\n\\Delta(\\tau) = \\alpha(n_{i \\to i+1}(\\tau)(V(i+1) - V(i)) + n_{i \\to i-1}(\\tau)(V(i-1) - V(i))).\n\\] We will show that this is \\(0\\) in expectation, so that \\(V\\) doesn‚Äôt change in expectation.\nSince transitions to the left and right are equally likely we have \\(\\mathbb{E}[n_{i \\to i+1}(\\tau)] = \\mathbb{E}[n_{i \\to i-1}(\\tau)] = C\\) for some constant \\(C\\). Thus \\[\n\\begin{split}\n\\mathbb{E}_\\tau[\\Delta(\\tau)] &= \\alpha C (V(i+1) - V(i)) + C (V(i-1) - V(i)) \\\\\n&= \\alpha C (\\frac{i+1}{6} - \\frac{i}{6} + \\frac{i-1}{6} + \\frac{i}{6}) = 0\n\\end{split}\n\\]\n\n\n\nExercise 6.6 In Example¬†6.2 we stated that the true values for the random walk example are \\(\\frac{1}{6}\\), \\(\\frac{2}{6}\\), \\(\\frac{3}{6}\\), \\(\\frac{4}{6}\\), \\(\\frac{5}{6}\\), for states 1 through 5. Describe at least two different ways that these could have been computed. Which would you guess we actually used? Why?\n\n\nSolution 6.6. Phew, no idea which method they used and why. I personally would ‚Äúcompute‚Äù the numbers via a proof and I think the authors did the same. How exactly, however, I don‚Äôt know.\nI will give two ways to prove the formula for the true values.\n\nUsing a system of linear equations\nA straightforward way is to solve a linear system of equations. Let \\(\\mathbf{v}\\) be the vector of the true values \\(\\mathbf{v} = (v_1, \\dots, v_5)\\). Then \\(v\\) is a solution to the system of linear equations \\[\nP \\mathbf{v} + \\mathbf{b}\n= \\mathbf{v}\n\\] with \\[\nP = \\begin{pmatrix}\n0 & 0.5 & 0   & 0   & 0   \\\\\n0.5 & 0 & 0.5 & 0   & 0   \\\\\n0   & 0.5 & 0 & 0.5 & 0   \\\\\n0   & 0   & 0.5 & 0 & 0.5 \\\\\n0   & 0   & 0   & 0.5 & 0\n\\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0.5\n\\end{pmatrix}\n\\] So \\(\\mathbf{v} = (I - P)^{-1} \\mathbf{b}\\).\nFor fun let‚Äôs see how we can solve this using python. We can use the sympy package to solve the system of linear equations exactly.\n\n# === exact solution for the random walk problem ===\nimport sympy as sp\n\n# transition matrix\nP2 = sp.Matrix(\n    [\n        [0, 1, 0, 0, 0],\n        [1, 0, 1, 0, 0],\n        [0, 1, 0, 1, 0],\n        [0, 0, 1, 0, 1],\n        [0, 0, 0, 1, 0],\n    ]\n)\nP = P2 / 2\n# reward\nb = sp.Matrix([0, 0, 0, 0, 1]) / 2\n\n1# compute (I - P)^{-1} b\nI = sp.eye(5)\nI_minus_P = I - P\nI_minus_P_inv = I_minus_P.inv()\nv = I_minus_P_inv * b\n\n# display results\nprint(\"(I - P)^{-1}:\")\nsp.pprint(I_minus_P_inv)\nprint(\"\\nv:\")\nsp.pprint(v)\n\n\n1\n\nThere are ways to solve the sysemt \\((I-P)\\mathbf{v} = \\mathbf{b}\\) directly I just stuck with the notationally simpler method here.\n\n\n\n\n(I - P)^{-1}:\n‚é°5/3  4/3  1  2/3  1/3‚é§\n‚é¢                     ‚é•\n‚é¢4/3  8/3  2  4/3  2/3‚é•\n‚é¢                     ‚é•\n‚é¢ 1    2   3   2    1 ‚é•\n‚é¢                     ‚é•\n‚é¢2/3  4/3  2  8/3  4/3‚é•\n‚é¢                     ‚é•\n‚é£1/3  2/3  1  4/3  5/3‚é¶\n\nv:\n‚é°1/6‚é§\n‚é¢   ‚é•\n‚é¢1/3‚é•\n‚é¢   ‚é•\n‚é¢1/2‚é•\n‚é¢   ‚é•\n‚é¢2/3‚é•\n‚é¢   ‚é•\n‚é£5/6‚é¶\n\n\n\n\nUsing a recursive relation\nAnother way is similar to a trick we have used before to turn the recurrence into a recursive system in the proof of Theorem¬†5.1.\nLet \\(f(0) = 0\\) and \\(f(1) = 1\\) and define \\(f(i) = 2f(i-1) - f(i-2)\\). The solution to this recursion is \\(f(i) = i\\). Then rescaling \\(f\\) such that \\(v(6) = 1\\) gives \\(v(i) = \\frac{f(i)}{6} = \\frac{i}{6}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Temporal-Difference Learning (Still in Progress üî®)</span>"
    ]
  },
  {
    "objectID": "chapters/06-temporal-difference-learning.html#optimality-of-td0",
    "href": "chapters/06-temporal-difference-learning.html#optimality-of-td0",
    "title": "6¬† Temporal-Difference Learning (Still in Progress üî®)",
    "section": "6.3 Optimality of TD(0)",
    "text": "6.3 Optimality of TD(0)\nBatch updating is like off-line updating only that the value function \\(V\\) is constant over a whole batch instead of a single episode. Batch every-visit MC is this \\[\nV(s) \\gets V(s) + \\alpha \\sum_{(i,t) \\in \\mathcal{T}(s)} G_t^{(i)} - V(s),\n\\tag{6.5}\\] and TD(0) batch updating \\[\nV(s) \\gets V(s) + \\alpha \\sum_{(i,t) \\in \\mathcal{T}(s)} R_{t+1}  + \\gamma V(S_{t+1}) - V(S_t)\n\\tag{6.6}\\] with \\(\\mathcal{T}(s)\\) the set of all visits to \\(s\\) in the batch.\nGeneral these update rules have the form for some fixed \\(v = V(S)\\) \\[\nv \\gets v + \\alpha [b - av]\n\\] for fixed \\(a,b \\geq 0\\)\nIterating this for small enough \\(\\alpha\\) converges. Let‚Äôs talk about this in more detail.\n\nLinear fixed point iteration\nLet‚Äôs discuss the convergence behaviour of a recursive sequence using the general update rule from above \\[\nv_{t+1} = v_t + \\alpha \\cdot ( b - a v_t)\n\\] with \\(v_0 = c\\) for some arbitrary \\(c\\). This is a fixed point iteration for an affine function.\nIf \\(\\alpha = 0\\) then this sequence is just \\(c\\). The same is true if \\(a = b = 0\\). If only \\(a = 0\\) but \\(b \\neq 0\\) then the sequence diverges.\nSo from now on let \\(\\alpha &gt; 0\\), \\(a &gt; 0\\), and \\(b \\neq 0\\). If the sequence converges it has to converge to the unique fixed point \\(v^*\\) \\[\nv^* = v^* + \\alpha \\cdot ( b - a v^*) \\Longleftrightarrow v^* = \\frac{b}{a}.\n\\]\nTowards the convergence we have \\[\n\\begin{split}\nv_{t+1} - v^* &= (1 - \\alpha a) v_t + \\alpha b - v^* \\\\\n&= (1 - \\alpha a) v_t - (1 - \\alpha a) v^* \\\\\n&= (1 - \\alpha a) (v_t - v^*)\n\\end{split}\n\\]\nRecursively we get \\(v_t - v^* = (1 - \\alpha a)^t (c - v^*)\\). So the sequence converges, if and only if, \\(|1 - \\alpha a| &lt; 1\\) which is equivalent to \\[\n0 &lt; \\alpha &lt; \\frac{2}{a}\n\\] So this is the condition for the sequence to converge.\nIn ML we also usually have \\(0 &lt; 1- \\alpha \\cdot a &lt; 1\\), which gives a smooth convergence, in the other convergence case \\(-1 &lt; 1 - \\alpha \\cdot a &lt; 0\\) the sign of the sequence flips. All three cases are here in a sample plot\n\n\nCode\ndef iterate(v, a, b, alpha):\n    return v + alpha * (b - a * v)\n\n\ndef iterate_n(c, a, b, alpha, n):\n    v = c\n    vs = [v]\n    for _ in range(n):\n        v = iterate(v, a, b, alpha)\n        vs.append(v)\n    return vs\n\n\na = 2\nb = 1\n\npositive_contraction = iterate_n(-0.25, a, b, 0.05, 50)\nnegative_contraction = iterate_n(1.2, a, b, 0.9, 50)\nexpansion = iterate_n(0.505, a, b, 1.04, 50)\n\nplt.plot(positive_contraction, label=r\"smooth convergence: $0 &lt; 1 - \\alpha a &lt; 1$\")\nplt.plot(negative_contraction, label=r\"flip flop convergence: $-1 &lt; 1 - \\alpha a &lt; 0$\")\nplt.plot(expansion, label=r\"divergence: $1 - \\alpha a &lt; -1$\")\nplt.ylim(-0.5, 1.5)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAnother way prove convergence is to use the Banach fixed point theorem for contractions.\n\nDefinition 6.1 A contraction, on a metric space \\((M,‚Äâd)\\) is a function \\(f\\) from \\(M\\) to itself, with the property that there is some real number \\(0 \\leq k &lt; 1\\) such that for all \\(x\\) and \\(y\\) in \\(M\\),\n\\[\nd(f(x),f(y))\\leq k\\,d(x,y)\n\\]\n\nThe map \\(f(v) =  v + \\alpha (b - a v)\\) is a contraction mapping if \\(1 - \\alpha a| &lt; 1\\) since \\[\n\\begin{split}\n|f(v) - f(v')| &= |v - v' + \\alpha (a v' - a v)| \\\\\n&= |(1 - \\alpha a)(v - v')| \\\\\n&= |1 - \\alpha a||v - v'|\n\\end{split}\n\\]\nWe will use a similar argument for the vector case\n\nThe vector case\nWhen dealing with a value function, we can represent it as a vector. In this cas ethe linear fixed point iteration look like this:\n\\[\n\\mathbf{v}_{t+1} = \\mathbf{v}_t + \\alpha (\\mathbf{b} - A \\mathbf{v}_t)\n\\tag{6.7}\\]\nConsider any norm \\(||\\cdot||\\) if in this norm \\(||I - \\alpha A|| = k &lt; 1\\), then \\(\\mathbf{v} \\to \\mathbf{v} + \\alpha (\\mathbf{b} - A \\mathbf{v})\\) is a contraction since \\[\n\\begin{split}\n|| \\mathbf{v} - \\mathbf{v}' + \\alpha A (\\mathbf{v}' - \\mathbf{v}) || &= ||(I - \\alpha A)(\\mathbf{v}' - \\mathbf{v})|| \\\\\n&\\leq k || \\mathbf{v}' - \\mathbf{v} ||\n\\end{split}\n\\]\nHowever, this depends on the operator norm of \\(I - \\alpha A\\). To get rid of this dependency, we have that the spectral radius \\(\\rho(I - \\alpha A)\\) is the right condition to ensure convergence. The clue is spectra norm \\(&lt; 1\\) \\(\\Longleftrightarrow\\) \\(|| 1 - \\alpha A|| &lt; 1\\) for some norm.\n\n\n\n6.3.1 Random walk under batch updating\nLet‚Äôs apply batch updating to the random walk example Example¬†6.2. We can adapt batch updating mc Equation¬†6.5 and batch-updating TD(0) Equation¬†6.6 like so \\[\n\\begin{split}\nV(i) \\gets V(i) + \\alpha [&R(i) \\cdot (1 -  V(i)) \\\\\n+ &L(i) \\cdot (0 - V(i))]\n\\end{split}\n\\] where \\(N(s)\\) and \\(R(s)\\) are the number of all the visits to state \\(s\\) that ended on the left and right, respectively. \\[\n\\begin{split}\nV(i) \\gets V(i) + \\alpha [ &T(i,i-1) (V(i-1)-V(i)) \\\\\n+ &T(i,i+1) (V(i+1)-V(i))] ,\n\\end{split}\n\\] where \\(T(i,j)\\) is the number of transitions from \\(i\\) to \\(j\\). Also keep in mind that \\(V(0) = 0\\) and \\(V(6) = 1\\) are fixed in this update rule.\nIt‚Äôs easy to find the limit of the batch updating every-visit mc rule. Each \\(V(i)\\) can be treated as a single scalar updating. Thus the limit is \\[\n\\lim_{t \\to \\infty}V_t(i) = \\begin{cases} \\frac{R(i)}{L(i) + R(i)}, &\\text{if } L(i) + R(i) &gt; 0 \\\\ V_0(i), &\\text{ else} \\end{cases}\n\\]\nFor the TD(0) case we can vectorise the individual updates to get an update formula of the form \\[\n\\mathbf{v} \\gets \\mathbf{v} + \\alpha (\\mathbf{b} - \\mathbf{A} \\mathbf{v})\n\\] where \\(\\mathbf{v}\\) also includes the two terminal states. Then \\(\\mathbf{b}_i = [i = 6]\\) and \\[\nA_{i,j} = \\begin{cases}\n1, &i = j \\in \\{0,6\\} \\\\\nT(i,i-1), &j = i-1 \\\\\nT(i,i+1), &j = i+1 \\\\\nT(i,i-1) + T(i,i+1), &j = i \\in \\{1, \\dots, 5\\}\n\\end{cases}\n\\]\nSo if we start this with \\(\\mathbf{v}^{(0)}_0 = 0\\) and \\(\\mathbf{v}^{(0)}_6 = 1\\) and follow the upate rule given above, this gives just the update given before. Actually, since the limit is unique the conditions on \\(\\mathbf{v}^{(0)}_0\\) and \\(\\mathbf{v}^{(0)}_6\\) can be relaxed and it would still converge to the same solution, which is: \\[\n\\lim_{t \\to \\infty} \\mathbf{v}^{(t)} = A^{-1}\\mathbf{b}\n\\]\nWell, actually this is not quite true, in case both \\(T(i,i-1)\\) and \\(T(i,i+1)\\) are zero, so there are no visits to \\(i\\), the initial value \\(V_0(i)\\) doesn‚Äôt change. In these cases we have to define \\(\\mathbf{b}_i = 1\\) and \\(A_{i,i} = 1, A_{i,j} = 0\\) for \\(j \\neq i\\).\nNow we have enough preparations to try these out.\n\nExample 6.3 Random walk under batch updating (Sutton and Barto 2018, Example 6.3)\nWe want to compare the limits of batch-updating for mc and td.\nWe start with the MC case:\n\n# === batch-update every-visit mc ===\n\n\ndef count_exits(episode, env_size=5):\n    \"\"\"\n    Count every-visit state occurrences for a single random-walk episode.\n    Visits are attributed to the side the episode ends on:\n    left exit ‚Üí row 0, right exit ‚Üí row 1.\n    \"\"\"\n    visits = np.bincount(episode, minlength=env_size + 2)\n    zeros = np.zeros_like(visits)\n    if episode[-1] == 0:\n        return np.vstack((visits, zeros))\n    else:\n        return np.vstack((zeros, visits))\n\n\ndef batch_mc_update_limit(exits, initial=0.5):\n    \"\"\"\n    Compute the fixed point of iterated every-visit MC batch updates\n    for the given exit counts.\n    \"\"\"\n    total = exits.sum(axis=0)\n\n    # avoid divide-by-zero warnings for unvisited states\n    with np.errstate(invalid=\"ignore\"):\n        limit = np.where(total &gt; 0, exits[1] / total, initial)\n\n    # enforce terminal values\n    limit[0] = 0.0\n    limit[-1] = 1.0\n    return limit\n\nNow the TD(0) case\n\n# === batch-update TD(0) ===\n\n\ndef count_transitions(episode, env_size=5):\n    \"\"\"\n    return matrix of transition counts for a single episode\n    \"\"\"\n    n_states = env_size + 2\n    s = np.asarray(episode[:-1], dtype=np.int64)\n    s_p = np.asarray(episode[1:], dtype=np.int64)\n    transition_idx = s * n_states + s_p\n    flat = np.bincount(transition_idx, minlength=n_states * n_states)\n    return flat.reshape((n_states, n_states)).astype(float)\n\n\ndef batch_td_update_limit(transitions, initial=0.5):\n    \"\"\"\n    Compute the fixed point of iterated batch TD(0) updates\n    for the given transition counts\n    \"\"\"\n    n = transitions.shape[0]\n\n    N = transitions.copy()\n    for i in range(1, n - 1):\n        N[i, i] -= N[i, i - 1] + N[i, i + 1]\n\n    N[0, 0] = 1.0\n    N[-1, -1] = 1.0\n\n    b = np.zeros(n, dtype=float)\n    b[-1] = 1\n\n    # special case for unvisited states\n    row_sums = transitions.sum(axis=1)\n    unvisited_mask = np.zeros(n, dtype=bool)\n    unvisited_mask[1:-1] = row_sums[1:-1] == 0\n    if unvisited_mask.any():\n        rows = np.nonzero(unvisited_mask)[0]\n        N[rows, :] = 0.0\n        N[rows, rows] = 1.0\n        b[rows] = initial\n\n    return np.linalg.solve(N, b)\n\nNow we can compare them\n\nCode\ndef experimentititonification(random_walks, env_size=5):\n    n_states = env_size + 2\n    true_values = np.array([i / (env_size + 1) for i in range(n_states)])\n\n    exits_by_replica = np.zeros((batch_size, 2, n_states), dtype=np.int64)\n    transitions_by_replica = np.zeros((batch_size, n_states, n_states), dtype=float)\n\n    rmse_per_replica_td = np.zeros(batch_size, dtype=float)\n    rmse_per_replica_mc = np.zeros(batch_size, dtype=float)\n\n    mean_rmse_td = np.zeros(n_episodes, dtype=float)\n    mean_rmse_mc = np.zeros(n_episodes, dtype=float)\n\n    for episode_idx, episode_batch in enumerate(generated_walks):\n        for replica_idx, episode in enumerate(episode_batch):\n            transitions_by_replica[replica_idx] += count_transitions(episode, env_size)\n            exits_by_replica[replica_idx] += count_exits(episode, env_size)\n\n            value_est_td = batch_td_update_limit(transitions_by_replica[replica_idx])\n            value_est_mc = batch_mc_update_limit(exits_by_replica[replica_idx])\n\n            diffs_td = value_est_td[1:-1] - true_values[1:-1]\n            diffs_mc = value_est_mc[1:-1] - true_values[1:-1]\n            rmse_per_replica_td[replica_idx] = np.sqrt(np.mean(diffs_td**2))\n            rmse_per_replica_mc[replica_idx] = np.sqrt(np.mean(diffs_mc**2))\n\n        mean_rmse_td[episode_idx] = rmse_per_replica_td.mean()\n        mean_rmse_mc[episode_idx] = rmse_per_replica_mc.mean()\n\n    return mean_rmse_td, mean_rmse_mc\n\n\nrng = random.Random()\nn_episodes = 100\nbatch_size = 24\ngenerated_walks = generate_random_walk_dataset(n_episodes, batch_size, rng, env_size=5)\n\nmean_rmse_td, mean_rmse_mc = experimentititonification(generated_walks)\n\n# plotting (unchanged)\nplt.plot(mean_rmse_td, label=\"TD(0) batch\")\nplt.plot(mean_rmse_mc, label=\"every-visit MC batch\")\nplt.ylim(0, 0.25)\nplt.legend()\nplt.title(f\"Batch update limits (averaged over {batch_size} replicas)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.6: Performance of TD(0) and every-visit MC under batch training on the random walk task (Sutton and Barto 2018, fig. 6.2).\n\n\n\nWe can see that TD(0) is just better than every-visit MC. Both methods have to generalise somehow from the limited data they are given and TD(0) is better in this example.\n\n\nExample 6.4 You are the Predictor  (Sutton and Barto 2018, Example 6.4)\nTODO\n\n\nExercise 6.7 Design an off-policy version of the TD(0) update that can be used with arbitrary target policy \\(\\pi\\) and covering behavior policy \\(b\\), using at each step \\(t\\) the importance sampling ratio \\(\\rho_{t:t}\\) (5.3).\n\n\nSolution 6.7. TODO read Graves and Ghiassian (2022)\noff-policy version of TD(0):\n\\[\n\\begin{split}\nv_\\pi(s) &= \\mathbb{E}_\\pi[G \\mid S_t = s] \\\\\n&= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] \\\\\n&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s]\n\\end{split}\n\\]\n\\[\n\\begin{split}\nv_\\pi(s) &= \\mathbb{E}_b [R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] \\\\\n&= \\sum_\\tau R_{t+1} + \\gamma\n\\end{split}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Temporal-Difference Learning (Still in Progress üî®)</span>"
    ]
  },
  {
    "objectID": "chapters/06-temporal-difference-learning.html#sarsa-on-policy-td-control",
    "href": "chapters/06-temporal-difference-learning.html#sarsa-on-policy-td-control",
    "title": "6¬† Temporal-Difference Learning (Still in Progress üî®)",
    "section": "6.4 Sarsa: On-policy TD Control",
    "text": "6.4 Sarsa: On-policy TD Control\nThe uptare rule is \\[\nQ(S_t,A_t) \\gets Q(S_t,A_t) + \\alpha \\big[ R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]\n\\]\n\n\n\nListing¬†6.2: First-visit MC prediction, for estimating \\(V \\approx v_\\pi\\)\n\n\nAlgorithm parameters: step size $(0,1], small \\(\\varepsilon &gt; 0\\)\nInitialisation: \\(Q(s,a)\\) arbitrary for all \\(s \\in \\mathcal{S}^+\\), \\(a \\in \\mathcal{A}\\), except \\(Q(\\text{terminal}, \\cdot) = 0\\)\n\nLoop forever:\n¬†¬†¬†¬†Start episode and observe \\(S\\)\n¬†¬†¬†¬†Choose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g.¬†\\(\\varepsilon\\)-greedy)\n¬†¬†¬†¬†While \\(S\\) is not termnial:\n¬†¬†¬†¬†¬†¬†¬†¬†Take action \\(A\\), observe \\(R,S'\\)\n¬†¬†¬†¬†¬†¬†¬†¬†Choose \\(A'\\) from \\(S'\\) using policy derived from \\(Q\\) (e.g.¬†\\(\\varepsilon\\)-greedy)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(Q(S,A) \\gets Q(S,A) + \\alpha [R + \\gamma Q(S',A') - Q(S,A)]\\)\n¬†¬†¬†¬†¬†¬†¬†¬†\\(S \\gets S'\\), \\(A \\gets A'\\)\n\n\n\n\nExample 6.5 Windy Gridwolrd (Sutton and Barto 2018, Example 6.5)\nSo here is the implementation of SARSA. Note that I hardcoded the -1 reward into the sarsa algorithm. Just because I am always afraid that the code will be too slow.\n\n# === implementation Sarsa ===\nimport random\n\n\ndef select_greedy_action(state, Q, action_space):\n    return max(action_space, key=lambda a: Q[(state, a)])\n\n\ndef select_Œµ_greedy_action(state, Q, Œµ, action_space, rng):\n    if rng.random() &lt; Œµ:\n        return rng.choice(action_space)\n    return select_greedy_action(state, Q, action_space)\n\n\ndef run_sarsa_episode(env, Q, Œ±, Œµ, rng):\n    action_space = env.action_space\n\n    state = env.reset()\n    action = select_Œµ_greedy_action(state, Q, Œµ, action_space, rng)\n\n    terminated = False\n    step_count = 0\n    while not terminated:\n        step_count += 1\n\n        next_state, terminated = env.step(action)\n\n        reward = -1\n        if terminated:\n            # terminal state: next-state action-value is zero\n            td_error = reward - Q[(state, action)]\n            Q[(state, action)] += Œ± * td_error\n            break\n\n        next_action = select_Œµ_greedy_action(next_state, Q, Œµ, action_space, rng)\n        td_error = reward + Q[(next_state, next_action)] - Q[(state, action)]\n        Q[(state, action)] += Œ± * td_error\n\n        state = next_state\n        action = next_action\n\n    return step_count\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport math\n\n\nclass WindyGridWorld:\n    def __init__(\n        self,\n        rng,\n        column_wind_strengths=(0, 0, 0, 1, 1, 1, 2, 2, 1, 0),\n        height=7,\n        move_set=\"king\",\n        start_pos=(0, 3),\n        target_pos=(7, 3),\n        wind_randomness=0.0,\n    ):\n        self.rng = rng\n        self.column_wind_strengths = column_wind_strengths\n        self.width = len(column_wind_strengths)\n        self.height = height\n\n        self.state_space = [\n            (x, y) for x in range(self.width) for y in range(self.height)\n        ]\n        self.action_space = self._make_action_space(move_set)\n\n        self.start_pos = np.array(start_pos, dtype=int)\n        self.target_pos = np.array(target_pos, dtype=int)\n\n        self.wind_randomness = wind_randomness\n\n    def _make_action_space(self, move_set: str):\n        ortho = [(0, v) for v in (-1, 1)] + [(h, 0) for h in (-1, 1)]\n        diag = [(h, v) for h in (-1, 1) for v in (-1, 1)]\n        if move_set == \"king\":\n            return ortho\n        if move_set == \"queen\":\n            return ortho + diag\n\n    def reset(self):\n        self.pos = self.start_pos.copy()\n        return (int(self.pos[0]), int(self.pos[1]))\n\n    def step(self, action):\n        # move\n        action = np.array(action, dtype=int)\n\n        base_wind = self.column_wind_strengths[self.pos[0]]\n        effective_wind = base_wind\n\n        if base_wind != 0:\n            sample = self.rng.random()\n            if sample &lt; self.wind_randomness:\n                effective_wind -= 1\n            elif sample &gt; 1.0 - self.wind_randomness:\n                effective_wind += 1\n\n        self.pos += action + (0, effective_wind)\n\n        # clip\n        self.pos = np.clip(\n            self.pos,\n            (0, 0),\n            (self.width - 1, self.height - 1),\n        )\n\n        # output\n        reached_goal = np.array_equal(\n            self.pos,\n            self.target_pos,\n        )\n        return (int(self.pos[0]), int(self.pos[1])), reached_goal\n\n\ndef windy_grid_world_experiment(env, n_episodes, Œ±, Œµ, rng):\n    Q = {\n        (state, action): 0.0 for state in env.state_space for action in env.action_space\n    }\n\n    Q_history = [Q.copy()]\n    steps = []\n\n    for episode in range(n_episodes):\n        steps.append(run_sarsa_episode(env, Q, Œ±, Œµ, rng))\n        Q_history.append(Q.copy())\n\n    return steps, Q_history\n\n\ndef prepare_plot_episodes_over_steps(steps, figsize=(10, 6)):\n    fig, ax = plt.subplots(figsize=figsize)\n    x = np.concatenate(([0], np.cumsum(steps)))\n    y = np.arange(len(steps) + 1)\n    ax.step(x, y, where=\"post\")\n    ax.set_xlabel(\"Time Steps\")\n    ax.set_ylabel(\"Episode\")\n    return fig, ax\n\n\ndef get_greedy_policy(env, Q):\n    return {\n        state: select_greedy_action(state, Q, env.action_space)\n        for state in env.state_space\n    }\n\n\ndef generate_episode(env, œÄ, max_len=20):\n    state = env.reset()\n\n    terminated = False\n    states = [state]\n\n    while not terminated:\n        action = œÄ[state]\n        state, terminated = env.step(action)\n        states.append(state)\n        if len(states) == max_len:\n            return states\n\n    return states\n\n\ndef plot_episode(env, states, title=None, radius=0.3, figsize=(8, 8)):\n    \"\"\"\n    Annotate every visit (timestep) but jitter repeated visits around the cell.\n    radius: radial offset (in cell units) for jitter ring; increase if labels collide.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Create a blank grid\n    grid = np.zeros((env.height, env.width))\n    # Mark start and target positions\n    grid[env.start_pos[1], env.start_pos[0]] = 1  # start position\n    grid[env.target_pos[1], env.target_pos[0]] = 2  # target position\n\n    # Define colormap: background, start, target\n    cmap = ListedColormap(\n        [\"#f2f2f2\", \"#8fd18f\", \"#e68a8a\"]\n    )  # light gray, light green, light red\n    ax.imshow(grid, cmap=cmap, origin=\"lower\", interpolation=\"nearest\")\n\n    # Set ticks and limits\n    ax.set_xticks(np.arange(env.width))\n    ax.set_yticks(np.arange(env.height))\n    ax.set_xlim(-0.5, env.width - 0.5)\n    ax.set_ylim(-0.5, env.height - 0.5)\n    ax.set_aspect(\"equal\")\n\n    # Rotate x-tick labels\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n\n    # grid lines\n    for x in range(env.width):\n        for y in range(env.height):\n            rect = plt.Rectangle(\n                (x - 0.5, y - 0.5),\n                1,\n                1,\n                fill=False,\n                edgecolor=\"#444444\",\n                linewidth=0.1,\n            )\n            ax.add_patch(rect)\n\n    if len(states) &gt; 0:\n        # Check if states are objects with a position attribute or just positions\n        if hasattr(states[0], \"position\"):\n            positions = [s.position for s in states]\n        else:\n            positions = states\n        xs = [p[0] for p in positions]\n        ys = [p[1] for p in positions]\n\n        ax.plot(xs, ys, marker=\"o\")\n\n        # Build map from position -&gt; list of visit indices\n        visits = {}\n        for t, pos in enumerate(positions):\n            visits.setdefault(pos, []).append(t)\n\n        # Annotate each visit with radial offsets to avoid overlap\n        for pos, idx_list in visits.items():\n            x0, y0 = pos\n            n = len(idx_list)\n            for k, t in enumerate(idx_list):\n                if n == 1:\n                    dx, dy = (0.08, 0.18)\n                else:\n                    angle = 2 * math.pi * k / n\n                    dx = math.cos(angle) * radius\n                    dy = math.sin(angle) * radius\n                ax.text(\n                    x0 + dx,\n                    y0 + dy,\n                    str(t),\n                    fontsize=9,\n                    va=\"center\",\n                    ha=\"center\",\n                    bbox=dict(boxstyle=\"round,pad=0.1\", alpha=0.9),\n                )\n\n    if title:\n        ax.set_title(title)\n\n    return fig, ax\n\n\n\nCode\nrng = random.Random(25)\nenv = WindyGridWorld(rng)\n\nŒ± = 0.5\nŒµ = 0.1\nn_episodes = 170\n\nsteps, Q_history = windy_grid_world_experiment(\n    env, n_episodes=n_episodes, Œ±=Œ±, Œµ=Œµ, rng=rng\n)\n\nfig, ax = prepare_plot_episodes_over_steps(steps)\nax.set_title(f\"SARSA on Windy Grid World, Œ±={Œ±}, Œµ={Œµ}\")\n\n# plt.show()\n\n\nœÄ = get_greedy_policy(env, Q_history[-1])\n\nepisode = generate_episode(env, œÄ)\n\nplot_episode(env, episode)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.7\n\n\n\n\n\n\n\n\nGraves, Eric, and Sina Ghiassian. 2022. ‚ÄúImportance Sampling Placement in Off-Policy Temporal-Difference Methods.‚Äù arXiv Preprint arXiv:2203.10172. https://doi.org/10.48550/arXiv.2203.10172.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Temporal-Difference Learning (Still in Progress üî®)</span>"
    ]
  },
  {
    "objectID": "chapters/06-temporal-difference-learning.html#footnotes",
    "href": "chapters/06-temporal-difference-learning.html#footnotes",
    "title": "6¬† Temporal-Difference Learning (Still in Progress üî®)",
    "section": "",
    "text": "In the book, the non-terminal states are labelled \\(A,B,C,D,E\\).‚Ü©Ô∏é\nIn the book this is \\(V(A)\\)‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Temporal-Difference Learning (Still in Progress üî®)</span>"
    ]
  }
]