[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Sutton & Barto",
    "section": "",
    "text": "Preface\nWelcome to my study notes on Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto. This primarily focuses on my solutions and thoughts while working through the exercises in the book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-intro.html",
    "href": "chapters/01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Exercise 1.1 (Self-Play) Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n\n\nSolution 1.1. The algorithm would learn the true game-theoretic values of each board state because, in the long run, both sides would learn the optimal strategy against their opponent.\n\n\nExercise 1.2 (Symmetries) Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n\n\nSolution 1.2. We could alter the learning process by using a canonical representative for each board state instead of the board state itself. This would speed up learning (the algorithm would generalise for symmetric states) and require less memory.\nIf the opponent does not respect the board symmetries, then the environment (board state plus opponent) should be treated as having no symmetries.\n\n\nExercise 1.3 (Greedy Play) Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a non-greedy player? What problems might occur?\n\n\nSolution 1.3. It could potentially learn to play better or worse. The greedy player has the advantage of always exploiting its knowledge. However, it has the significant disadvantage of never exploring. It could end up valuing a position as a draw that is actually a win as it never explores subsequent positions that would lead to win.\n\n\nExercise 1.4 (Learning from Exploration) Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n\n\nSolution 1.4. Without exploratory moves we learn the values of the states according to the optimal policy. With exploratory moves we learn the values of the states according to the \\(\\varepsilon\\)-optimal policy.\nIf we continue playing with \\(\\varepsilon\\)-soft, the latter values are preferable.\n\n\nExercise 1.5 (Other Improvements) Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?\n\n\nSolution 1.5. Surely there are many ways. And I think this book will discuss many of them. Just to give one example we could use \\(n\\)-step temporal difference (update a state’s value only after \\(n\\) moves).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html",
    "href": "chapters/02-multi-armed-bandits.html",
    "title": "2  Multi-armed Bandits",
    "section": "",
    "text": "2.1 A k-armed Bandit Problem\n(Some imports for python, can be ignored)\nThe k-armed bandit problem is a very simple example that already introduces a key structure of reinforcement learning: the reward depends on the action taken. It’s technically not a full Markov chain (more on that in Section 3.1)—since there are no states or transitions—but we still get a sequence of dependent random variables \\(A_1, R_1, A_2, \\dots\\), where each reward \\(R_t\\) depends on the corresponding action \\(A_t\\).\nThe true value of an action is defined as: \\[\nq_*(a) := \\mathbb{E}[ R_t \\mid A_t = a].\n\\]\nThe time index here doesn’t play a special role as the action-reward probabilities in the armed bandit are stationary. You can think of it as “when \\(a\\) is picked”—that is, the expected reward when action \\(a\\) is chosen.\nIf you’re feeling queasy about the conditional expected value, here’s a quick refresher on the relevant notation and concepts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#a-k-armed-bandit-problem",
    "href": "chapters/02-multi-armed-bandits.html#a-k-armed-bandit-problem",
    "title": "2  Multi-armed Bandits",
    "section": "",
    "text": "random variables and probability\nThe foundations of all processes we discuss here are discrete probability spaces \\((\\Omega, \\mathrm{Pr})\\).  \\(\\Omega\\) is the set of all possible trajectories—that is, complete sequences of outcomes for the random variables in a single run of the process—and \\(\\mathrm{Pr}\\) assigns a probability to each trajectory. That is, \\[\n\\mathrm{Pr}\\colon \\Omega \\to [0,1] \\quad \\text{with} \\quad\n\\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) = 1.\n\\]\nThe random variables \\(X\\) are simply functions \\(X\\colon \\Omega \\to \\mathcal{X}\\) from \\(\\Omega\\) to a result space \\(\\mathcal{X}\\).\nWe follow the convention of Sutton and Barto (2018): random variables are written in capital letters, and their possible values are in lowercase.\nIf we want to refer to the concrete outcome of a single trajectory (which we actually don’t often do in theory crafting), we evaluate random variables on a specific \\(\\omega \\in \\Omega\\), which fixes their values.  So an arbitrary trajectory looks like this \\(A_1(\\omega), R_1(\\omega), A_2(\\omega), R_2(\\omega) \\dots\\)\nLet’s bring up two common conventions we can find in \\(\\mathbb{E}[ R_t \\mid A_t = a]\\):\n\nWe usually omit the argument \\(\\omega\\) when referring to the value of a random variable. This is what makes the randomness implicit in a random variable \\(X\\). This mathematically conflates the function \\(X ⁣\\colon\\Omega\\to \\mathcal{X}\\) with the value \\(X(\\omega)\\), but context sorts that out.\nWhen writing functions of sets, we abbreviate expressions like \\(F({\\omega \\in \\Omega : \\text{statement true in }\\omega})\\) to simply \\(F(\\text{statement})\\).\n\nWith both conventions in play, we can see that \\(\\mathrm{Pr}(X = x)\\) is just shorthand for \\(\\mathrm{Pr}(\\omega \\in \\Omega : X(\\omega) = x)\\)\n\n\nconditional probability\nThe reward \\(R_t\\)​ depends on the action \\(A_t\\)​ taken. If we know the value of \\(A_t\\), then the conditional probability that \\(R_t=r\\) given \\(A_t = a\\) is: \\[\n\\mathrm{Pr}(R_t = r \\mid A_t = a) = \\frac{\\mathrm{Pr}(R_t = r, A_t = a)}{\\mathrm{Pr}(A_t = a)},\n\\]\nwhere the comma denotes conjunction of the statements.\nThis is only well-defined if \\(\\mathrm{Pr}(A_t = a) &gt; 0\\) but that’s a technicality we won’t worry too much about—it won’t bite us.\n\n\nexpected value\nReal-valued random variables like \\(R_t\\colon \\Omega \\to \\mathbb{R}\\) have an expected value—also called the mean—\\(\\mathbb{E}[R_t]\\), defined as: \\[\n\\mathbb{E}[ R_t ] := \\sum_{\\omega \\in \\Omega} R_t(\\omega) \\mathrm{Pr}(\\omega)\n\\]\nA more commonly used form—sometimes called the “law of the unconscious statistician” (see appendix Theorem 2.1)—is: \\[\n\\mathbb{E}[R_t] = \\sum_{r \\in \\mathcal{R}} r \\; \\mathrm{Pr}(R_t = r),\n\\]\nTo compute a conditional expectation, we just switch probabilities with conditional probabilities: \\[\n\\mathbb{E}[R_t \\mid A_t = a] = \\sum_{\\omega \\in \\Omega} R_t(\\omega) \\mathrm{Pr}(\\omega \\mid A_t = a).\n\\]\nOr, using the more practical LOTUS form: \\[\n\\mathbb{E}[R_t \\mid A_t = a] = \\sum_{r \\in \\mathcal{R}} r \\; \\mathrm{Pr}(R_t = r \\mid A_t = a).\n\\]\nTo close the loop, a more explicit formulation of the true value of an action is: \\[\nq_*(a) = \\sum_{r \\in \\mathcal{R}} r \\; \\frac{\\mathrm{Pr}(R_t = r, A_t = a)}{\\mathrm{Pr}(A_t = a)}\n\\] (This form is arguably less intuitive, but it’s included here to jog our memory.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#action-value-methods",
    "href": "chapters/02-multi-armed-bandits.html#action-value-methods",
    "title": "2  Multi-armed Bandits",
    "section": "2.2 Action-value Methods",
    "text": "2.2 Action-value Methods\nThis part always trips me up, so let me clarify it for myself: \\(Q_t(a)\\) is the estimated value of action \\(a\\) prior to time \\(t\\), so not included are \\(A_t\\) and it’s corresponding reward \\(R_t\\).\nInstead, \\(A_t\\) is selected based on the current estimates \\(\\{Q_{t}(a):a \\in \\mathcal{A}\\}\\). For example, our algorithm could pick \\(A_t\\)​ greedily as \\(A_t:=\\mathrm{argmax}_{a \\in \\mathcal{A}} Q_t(a)\\), or \\(\\varepsilon\\)-greedily.\n\nExercise 2.1 In \\(\\varepsilon\\)-greedy action selection, for the case of two actions and \\(\\varepsilon = 0.5\\), what is the probability that the greedy action is selected?\n\n\nSolution 2.1. The total probability of selecting the greedy action is: \\[\n\\mathrm{Pr}(\\text{greedy action}) + \\mathrm{Pr}(\\text{exploratory action}) \\cdot \\frac{1}{2} = 0.75\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#the-10-armed-testbed",
    "href": "chapters/02-multi-armed-bandits.html#the-10-armed-testbed",
    "title": "2  Multi-armed Bandits",
    "section": "2.3 The 10-armed Testbed",
    "text": "2.3 The 10-armed Testbed\nThe 10-armed testbed will accompany us through the rest of this chapter (I had to keep it variable in size just for the sake of generalization though).\n\n# === the armed bandit ===\nclass ArmedBandit:\n    \"\"\"k-armed Gaussian bandit.\"\"\"\n    def __init__(self, action_mu, action_sd, seed):\n        self.action_mu = np.asarray(action_mu, dtype=np.float64)\n        self.action_sd = np.asarray(action_sd, dtype=np.float64)\n        self.seed = seed\n        self.rng = np.random.default_rng(self.seed)\n\n    def pull_arm(self, action):\n        return self.rng.normal(loc=self.action_mu[action], scale=self.action_sd)\n\nAnd here’s the code for the sample-average bandit algorithm. For clarity, I’ll refer to this and upcoming algorithms as ‘agents’, given their autonomous implementation. Note that we’re also using the incremental implementation from section 2.4.\n\n# === the simple average bandit agent ===\nclass SampleAverageBanditAgent:\n    def __init__(self, Q1, ε, seed=None):\n        self.rng = np.random.default_rng(seed)\n        self.num_actions = len(Q1) \n        self.Q1 = np.asarray(Q1, dtype=np.float64)  # initial action-value estimates\n        self.ε = ε  \n        self.reset()\n\n    def reset(self):\n        self.Q = self.Q1.copy()\n        self.counts = np.zeros(self.num_actions, dtype=int)\n\n    def act(self, bandit):\n        # ε-greedy action selection\n        if self.rng.random() &lt; self.ε:\n            action = self.rng.integers(self.num_actions)\n        else:\n            action = np.argmax(self.Q)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update count and value estimate\n        self.counts[action] += 1\n        α = 1 / self.counts[action]\n        self.Q[action] += α * (reward - self.Q[action])\n\n        return (action, reward)\n\nNext, I’ll define an experiment function bandit_experiment that we’ll use throughout this chapter. In short, it takes multiple agents, length of each episode, and the number of episodes, then executes these agents repeatedly in a shared bandit environment, which gets reset after each episode. It returns two arrays: the average reward per step and the percentage of optimal actions taken per step—both averaged over all runs. These results can then be visualised using the plotting functions also provided. You don’t have to read the code unless you’re curious…\n\n\nCode\n# === the core bandit experiment ===\n\n# --- config\n@dataclass\nclass Config:\n    bandit_num_arms: int = 10\n    bandit_setup_mu: float = 0.0\n    bandit_setup_sd: float = 1.0\n    bandit_action_sd: float = 1.0\n    bandit_value_drift: bool = False\n    bandit_value_drift_mu: float = 0.0\n    bandit_value_drift_sd: float = 0.0\n    exp_steps: int = 1_000\n    exp_runs: int = 200\n    exp_seed: int = 0\n\n\n# -- core experiment\ndef bandit_experiment(agents, config: Config) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Run `exp_runs` × `exp_steps` episodes and return:\n\n    average_rewards         shape = (len(agents), exp_steps)\n    optimal_action_percent  shape = (len(agents), exp_steps)\n    \"\"\"\n    rng = np.random.default_rng(config.exp_seed)\n    num_agents = len(agents)\n    average_rwds = np.zeros((num_agents, config.exp_steps))\n    optimal_acts = np.zeros((num_agents, config.exp_steps))\n\n    # allocate a single bandit and reuse its object shell\n    bandit = ArmedBandit(\n        action_mu=np.empty(config.bandit_num_arms),  # placeholder\n        action_sd=config.bandit_action_sd,\n        seed=config.exp_seed,\n    )\n\n    for run in range(config.exp_runs):\n        # fresh true values for this run\n        bandit.action_mu[:] = rng.normal(\n            config.bandit_setup_mu, config.bandit_setup_sd, size=config.bandit_num_arms\n        )\n        best_action = np.argmax(bandit.action_mu)\n\n        # reset all agents\n        for agent in agents:\n            agent.reset()\n\n        # vectorised drift noise: shape = (exp_steps, bandit_num_arms)\n        if config.bandit_value_drift:\n            drift_noise = rng.normal(\n                config.bandit_value_drift_mu,\n                config.bandit_value_drift_sd,\n                size=(config.exp_steps, config.bandit_num_arms),\n            )\n\n        # main loop\n        for t in range(config.exp_steps):\n            for i, agent in enumerate(agents):\n                act, rwd = agent.act(bandit)\n                average_rwds[i, t] += rwd\n                optimal_acts[i, t] += act == best_action\n\n            if config.bandit_value_drift:\n                bandit.action_mu += drift_noise[t]\n                best_action = np.argmax(bandit.action_mu)\n\n    # mean over runs\n    average_rwds /= config.exp_runs\n    optimal_acts = 100 * optimal_acts / config.exp_runs\n    return average_rwds, optimal_acts\n\n\n# --- thin plotting helpers\ndef plot_average_reward(\n    average_rewards: np.ndarray,\n    *,\n    labels: Sequence[str] | None = None,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"One line per agent: average reward versus step.\"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(8, 4))\n\n    steps  = np.arange(1, average_rewards.shape[1] + 1)\n    if labels is None:\n        labels = [f\"agent {i}\" for i in range(average_rewards.shape[0])]\n\n    for i, lbl in enumerate(labels):\n        ax.plot(steps, average_rewards[i], label=lbl)\n\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Average reward\")\n    ax.set_title(\"Average reward per step\")\n    ax.grid(alpha=0.3, linestyle=\":\")\n    ax.legend()\n    return ax\n\n\ndef plot_optimal_action_percent(\n    optimal_action_percents: np.ndarray,\n    *,\n    labels: Sequence[str] | None = None,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"One line per agent: % optimal action versus step.\"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(8, 4))\n\n    steps  = np.arange(1, optimal_action_percents.shape[1] + 1)\n    if labels is None:\n        labels = [f\"agent {i}\" for i in range(optimal_action_percents.shape[0])]\n\n    for i, lbl in enumerate(labels):\n        ax.plot(steps, optimal_action_percents[i], label=lbl)\n\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"% optimal action\")\n    ax.set_title(\"Optimal-action frequency\")\n    ax.grid(alpha=0.3, linestyle=\":\")\n    ax.legend()\n    return ax\n\n\n…, but it’s still helpful to see such an experiment in action. We can, for example, recreate Figure 2.2 (Sutton and Barto 2018). It compares the performance of a greedy agent (\\(\\varepsilon = 0\\)), and two \\(\\varepsilon\\)-greedy agents with \\(\\varepsilon =0.1\\) and \\(\\varepsilon=0.01\\). We let them run for a couple of steps and then repeat this process for a couple of runs to get a smoother curve.\n\n# === comparison greediness ===\n# configuration of the experiment\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=2_000)\n\n# initialize agents with different epsilon values\nepsilons = [0.0, 0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), ε=ε, seed=config.exp_seed) for ε in epsilons]\n\n# run bandit experiment\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config    \n)\n\n# plots\nlabels = [f\"ε={e}\" for e in epsilons]\n\nplot_average_reward(avg_rwd, labels=labels)\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n(a) The average reward.\n\n\n\n\n\n\n\n\n\n\n\n(b) The percentage of optimal step selection\n\n\n\n\n\n\nFigure 2.1: This is like Figure 2.2 (Sutton and Barto 2018): the average performance of different ε-greedy sample average methods over 2000 runs. On the 10-armed testbed.\n\n\n\n\nOut of curiosity, let’s see what happens when we have only one run (so it’s not the average anymore but just the reward). It’s a mess, wow. Without averaging over a couple of runs, we can’t make out anything.\n\n\nCode\n# === experiment with only one run ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1)\n\nepsilons = [0.0, 0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), ε=ε, seed=config.exp_seed) for ε in epsilons]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nplot_average_reward(avg_rwd, labels=[f\"ε={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 (Bandit example) Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \\(\\varepsilon\\)-greedy action selection, sample-average action-value estimates, and initial estimates of \\(Q_1(a) = 0\\), for all a. Suppose the initial sequence of actions and rewards is \\(A_1 = 1\\), \\(R_1 = -1\\), \\(A_2 = 2\\), \\(R_2 = 1\\), \\(A_3 = 2\\), \\(R_3 = -2\\), \\(A_4 = 2\\), \\(R_4 = 2\\), \\(A_5 = 3\\), \\(R_5 = 0\\). On some of these time steps the \\(\\varepsilon\\) case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?\n\n\nSolution 2.2. Step 1 could have been exploratory, as all actions have the same estimates. After that, the value function is: \\[\nQ_2(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nAlso, step 2 could have been exploratory. Now the value function is: \\[\nQ_3(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            1,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nIn step 3, the greedy action is taken. But it could also have been an exploratory action that selected 2. At this point, the value function is: \\[\nQ_4(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            -0.5,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nIn step 4, a non-greedy action is taken, so this must have been an exploratory move. The value function is: \\[\nQ_5(a) = \\begin{cases}\n            -1,& \\text{if $a = 1$}\\\\\n            0.33,& \\text{if $a = 2$}\\\\\n            0,& \\text{otherwise}\n         \\end{cases}\n\\]\nIn step 5, again a non-greedy action was taken, so this must have been an exploratory move as well.\n\n\nExercise 2.3 In the comparison shown in Figure 2.1, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.\n\n\nSolution 2.3. Obviously, we can disregard \\(\\varepsilon = 0\\). It’s just rubbish. Before we do the quantitative analysis, let’s see what happens when we just crank up the number of steps (and reduce the runs even though now it’s a bit noisier).\n\n\nCode\n# === battle between ε=0.1 and ε=0.01 ===\nconfig = Config(\n    exp_steps=15_000,\n    exp_runs=200)\n\nepsilons = [0.1, 0.01]\nagents = [SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms), ε=ε, seed=config.exp_seed) for ε in epsilons]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nplot_average_reward(avg_rwd, labels=[f\"ε={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=[f\"ε={e}\" for e in epsilons])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that \\(\\varepsilon=0.01\\) outperforms \\(\\varepsilon=0.1\\) in average reward around step \\(2000\\). However, achieving a higher percentage of optimal actions takes more than \\(10,000\\) steps. It’s actually quite interesting that achieving a higher percentage of optimal actions takes significantly longer.\nNow, let’s consider the long-term behaviour. In the limit, we can assume both methods have near-perfect \\(Q\\)-values and the only reason they select non-optimal actions is due to their \\(\\varepsilon\\)-softness.\nThis makes calculating the optimal action probability quite easy. \\[\n\\mathrm{Pr}(\\text{optimal action}) = (1-\\varepsilon) + \\varepsilon \\frac{1}{10} = 1 - 0.9 \\varepsilon\n\\]\nSo for \\(\\varepsilon=0.1\\) this probability is \\(0.91\\), and for \\(\\varepsilon=0.01\\) this is \\(0.991\\).\nNow the average reward is trickier to compute. It can be done, but it’s quite messy and we’re here to learn reinforcement learning so we don’t need to figure out perfect analytical solutions anymore. Luckily, we get this value directly from the book\n\nIt [greedy algorithm] achieved a reward-per-step of only about \\(1\\), compared with the best possible of about \\(1.55\\) on this testbed (Sutton and Barto 2018, 29).\n\nGreat—they’ve done the work for us. Selecting the optimal action gives an average reward of \\(1.55\\). Selecting a random action has an average reward of \\(0\\) because it’s basically drawing a sample from a normal distribution with mean \\(0\\). That gives:\n\\[\n\\mathbb{E}[R_t] = (1-\\varepsilon) 1.55 + \\varepsilon 0 = 1.55 (1-\\varepsilon)\n\\]\nThis results in \\(1.40\\) for \\(\\varepsilon = 0.1\\) and \\(1.53\\) for \\(\\varepsilon = 0.01\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#sec-incremental-implementation",
    "href": "chapters/02-multi-armed-bandits.html#sec-incremental-implementation",
    "title": "2  Multi-armed Bandits",
    "section": "2.4 Incremental Implementation",
    "text": "2.4 Incremental Implementation\nThe sample average can be updated incrementally using: \\[\nQ_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n].\n\\tag{2.1}\\]\nThis is an instance of a general pattern that is central to reinforcement learning: \\[\n\\text{NewEstimate} \\gets \\text{OldEstimate} + \\text{StepSize}\n\\Big[\\overbrace{\n    \\text{Target} - \\text{OldEstimate}\n    }^\\text{error} \\Big]\n\\]\nI especially like how nice it looks in python. In value-based algorithms, this typically corresponds to the following line of code:\n\nQ[action] += α * (reward - Q[action])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#tracking-a-nonstationary-problem",
    "href": "chapters/02-multi-armed-bandits.html#tracking-a-nonstationary-problem",
    "title": "2  Multi-armed Bandits",
    "section": "2.5 Tracking a Nonstationary Problem",
    "text": "2.5 Tracking a Nonstationary Problem\nTo avoid the learning rate decreasing over time—at the cost of convergence—we can use a constant step size \\(\\alpha \\in (0,1]\\). \\[\nQ_{n+1} := Q_n + \\alpha \\Big[ R_n - Q_n \\Big],\n\\tag{2.2}\\]\nfor \\(n \\geq 1\\) and \\(Q_1\\) is our given initial estimate.\nThis can be phrased as a recurrence relation of the form \\(Q_n = \\sum_{i=0}^n \\gamma^{n-i} r_i\\), as discussed in the appendix Section 2.11.9: We add \\(Q_0\\) and \\(R_0\\). Then, Equation 2.2 is equivalent to: \\[\nQ_{n+1} = \\alpha R_n + (1 - \\alpha) Q_n \\quad \\text{and} \\quad Q_0 = 0,\n\\tag{2.3}\\]\nThis is a recurrence relation of the form Equation 2.18 and thus has the explicit form \\[\nQ_{n+1} = \\sum_{i=0}^n (1 - \\alpha)^{n-i} \\alpha R_{i}\n\\]\nSubstituting \\(R_0 = \\frac{Q_1}{\\alpha}\\)—so that \\(Q_1\\) becomes our arbitrary initial value—yields the form used by Sutton and Barto: \\[\nQ_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} R_i\n\\tag{2.4}\\]\nThis is a weighted average of the random variables involved, as the weights sum to 1. The sum of the weights for the \\(R_i\\) is (using the geometric series identity Equation 2.14): \\[\n\\begin{split}\n\\sum_{i=1}^n \\alpha (1- \\alpha)^{n-i} &= \\alpha \\sum_{i=o}^{n-1}(1-\\alpha)^i \\\\\n&= \\alpha \\frac{1 - (1-\\alpha)^n}{\\alpha}\\\\\n&= 1 - (1 - \\alpha)^n.\n\\end{split}\n\\]\nThus, the total weight sums to 1: \\[\n\\begin{split}\n(1-\\alpha)^n + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i} &= 1\n\\end{split}\n\\]\nThe \\(Q_n\\) are estimators for the true action value \\(q_*\\). And depending on how we determine the \\(Q_n\\), they have different qualities. We note that the \\(R_i\\) are IID with mean \\(q_*\\) and variance \\(\\sigma^2\\). (I refer to the appendix Section 2.11 for more information about all the new terms appearing all of a sudden, as this has gotten quite a bit more technical)\nIf \\(Q_n\\)​ is the sample average, the estimator is unbiased, that is \\[\n\\mathbb{E}[Q_n] = q_* \\quad \\text{for all } n \\in \\mathbb{N}.\n\\] Which is easy to show. Its mean squared error \\(\\mathrm{MSE}(Q_n) := \\mathbb{E}[(Q_n - q_*)^2]\\) is decreasing (Lemma 2.4): \\[\n\\mathrm{MSE}(Q_n) = \\frac{\\sigma^2}{n}.\n\\]\nIf the \\(Q_n\\) are calculated using a constant step size, they are biased (there is always a small dependency on \\(Q_1\\)): \\[\n\\begin{split}\n\\mathbb{E}[Q_{n+1}] &=  (1-\\alpha)^n Q_1 + q\\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i}   \\\\\n&= (1-\\alpha)^n Q_1 + q (1 - (1 - \\alpha)^n)\n\\end{split}\n\\tag{2.5}\\]\nAnd even though they are asymptotically unbiased, i.e., \\(\\lim_{n\\to\\infty} \\mathbb{E}[Q_{n}] = q\\), their mean squared error is bounded away from zero (Lemma 2.5): \\[\n\\mathrm{MSE}(Q_n) &gt; \\sigma^2 \\frac{\\alpha}{2-\\alpha}.\n\\]\nIt’s hard for me to translate these stochastic results in statistic behaviour, but I think that means that constant step size will end up wiggling around the true value.\nLet’s define the constant step size agent to compare it with the sample average method later.\n\n# === the constant step bandit agent ===\nclass ConstantStepBanditAgent:\n    def __init__(self, Q1, α, ε, seed=None):\n        self.rng = np.random.default_rng(seed)\n        self.num_actions = len(Q1)\n        self.Q1 = Q1\n        self.α = α\n        self.ε = ε\n        self.reset()\n\n    def reset(self):\n        self.Q = self.Q1.copy()\n\n    def act(self, bandit):\n        # ε-greedy action selection\n        if self.rng.random() &lt; self.ε:\n            action = self.rng.integers(self.num_actions)\n        else:\n            action = np.argmax(self.Q)\n\n        # take action\n        reward = bandit.pull_arm(action)\n\n        # update value estimate\n        self.Q[action] += self.α * (reward - self.Q[action])\n\n        return (action, reward)\n\n\nExercise 2.4 If the step-size parameters, \\(\\alpha_n\\), are not constant, then the estimate \\(Q_n\\) is a weighted average of previously received rewards with a weighting different from that given by Equation 2.4. What is the weighting on each prior reward for the general case, analogous to Equation 2.4, in terms of the sequence of step-size parameters.\n\n\nSolution 2.4. The update rule for non-constant step size has \\(\\alpha_n\\) depending on the step. \\[\nQ_{n+1} = Q_n + \\alpha_n \\Big[ R_n - Q_n \\Big]\n\\tag{2.6}\\]\nIn this case the weighted average is given by \\[\nQ_{n+1} = \\left( \\prod_{j=1}^n 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^n \\alpha_i \\left( \\prod_{j=i+1}^n 1 - \\alpha_j \\right) R_i\n\\tag{2.7}\\]\nThis explicit form can be verified inductively. For \\(n=0\\), we get \\(Q_1\\) on both sides.\nFor the induction step we have \\[\n\\begin{split}\nQ_{n+1} &= Q_n + \\alpha_n \\Big[ R_n - Q_n \\Big] \\\\\n&= \\alpha_n R_n + (1 - \\alpha_n) Q_n \\\\\n&= \\alpha_n R_n + (1 - \\alpha_n) \\Big[ \\left( \\prod_{j=1}^{n-1} 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^{n-1} \\alpha_i \\left( \\prod_{j=i+1}^{n-1} 1 - \\alpha_j \\right) R_i \\Big] \\\\\n&= \\left( \\prod_{j=1}^n 1-\\alpha_j \\right) Q_1 + \\sum_{i=1}^n \\alpha_i \\left( \\prod_{j=i+1}^n 1 - \\alpha_j \\right) R_i\n\\end{split}\n\\]\nWe also note that in this general setting, Equation 2.7 is still a weighted average. We could prove it by induction or use a little trick. If we set \\(Q_1 = 1\\) and \\(R_n = 1\\) for all \\(n\\) in the recurrence relation Equation 2.6 we see that each \\(Q_n = 1\\). If we do the same in the explicit formula Equation 2.7 we see that each \\(Q_n\\) is equal to the sum of the weights. Therefore, the weights sum up to \\(1\\).\n\n\nExercise 2.5 Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the \\(q_\\star(a)\\) start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the \\(q_\\star(a)\\) on each step). Prepare plots like Figure Figure 2.1 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, \\(\\alpha = 0.1\\). Use \\(\\epsilon = 0.1\\) and longer runs, say of 10,000 steps\n\n\nSolution 2.5. Alright, let’s do a little experiment, just as they told us.\n\n\nCode\n# === battle between sample average and constant step ===\nconfig = Config(\n    bandit_setup_mu=0,\n    bandit_setup_sd=0,\n    bandit_value_drift=True,\n    bandit_value_drift_mu=0,\n    bandit_value_drift_sd=0.01,\n    exp_steps=10_000,\n    exp_runs=100,\n)\n\nagent0 = SampleAverageBanditAgent(Q1=np.zeros(config.bandit_num_arms, dtype=float), ε=0.1, seed=config.exp_seed)\nagent1 = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), α=0.1, ε=0.1, seed=config.exp_seed\n)\nagents = [agent0, agent1]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\"sample averages (ε=0.1)\", \"constant step-size (α=0.1, ε=0.1)\"]\nplot_average_reward(avg_rwd, labels=labels)\nplt.tight_layout()\nplt.show()\n\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot surprisingly, we can see how much the sample average agent struggles to keep up. For longer episode lengths, the problem only gets worse. Eventually, it will be completely out of touch with the world, like an old man unable to keep up with the times.\nHowever, it remains unclear exactly how rapidly the sample average method will deteriorate to an unacceptable level. The results from the final exercise of this chapter (Exercise 2.11) indicate that this deterioration may take a significant number of steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#optimistic-initial-values",
    "href": "chapters/02-multi-armed-bandits.html#optimistic-initial-values",
    "title": "2  Multi-armed Bandits",
    "section": "2.6 Optimistic Initial Values",
    "text": "2.6 Optimistic Initial Values\nWe later need the following figure comparing an optimistic greedy agent and a realistic \\(\\varepsilon\\)-greedy agent.\n\n\nCode\n# === realism vs optimism ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1_000\n)\nagent_optimistic_greedy = ConstantStepBanditAgent(\n    Q1=np.full(config.bandit_num_arms, 5.0, dtype=float), α=0.1, ε=0.0, seed=config.exp_seed\n)\nagent_realistic_ε_greedy = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), α=0.1, ε=0.1, seed=config.exp_seed\n)\n\nagents = [agent_optimistic_greedy, agent_realistic_ε_greedy]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"optimistic,greedy (Q1=5, ε=0, α=0.1)\",\n    \"realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.2: This is like Figure 2.3 (Sutton and Barto 2018): the effect of optimistic initial action-value estimates.\n\n\n\n\n\n\nExercise 2.6 (Mysterious Spikes) The results shown in Figure 2.2 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n\n\nSolution 2.6. We have the hard-earned luxury that we can zoom in on Figure 2.2:\n\n\nCode\n# === realism vs optimism zoomed in ===\nconfig = Config(\n    exp_steps=30,\n    exp_runs=5_000,\n)\nagent_optimistic_greedy = ConstantStepBanditAgent(\n    Q1=np.full(config.bandit_num_arms, 5.0, dtype=float), α=0.1, ε=0.0, seed=config.exp_seed\n)\nagent_realistic_ε_greedy = ConstantStepBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), α=0.1, ε=0.1, seed=config.exp_seed\n)\n\nagents = [agent_optimistic_greedy, agent_realistic_ε_greedy]\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"optimistic,greedy (Q1=5, ε=0, α=0.1)\",\n    \"realistic,ε-greedy (Q1=0, ε=0.1, α=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe spike occurs at step 11. Essentially, the optimistic method samples all actions once (poor performance), and then selects the action with the best result (good performance, with a success rate of over 40%). However, regardless of the outcome (which likely pales in comparison to the current Q-values, which are still likely greater than 4), the method returns to exploring all 10 actions again. This leads to poor performance once more. Around step 22, there is another spike for similar reasons, but this time smaller and more spread out.\n\n\nExercise 2.7 (Unbiased Constant-Step-Size Trick) In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of onstant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of \\[\n\\beta_n := \\alpha / \\bar{o}_n\n\\] to process the \\(n\\)-th reward for a particular action, where \\(\\alpha &gt; 0\\) is a conventional constant step size, and \\(\\bar{o}_n\\) is a trace of one that starts at \\(0\\): \\[\n\\bar{o}_n := \\bar{o}_{n-1} + \\alpha (1 - \\bar{o}_{n-1}), \\text{ for } n \\geq 1, \\text{ with } \\bar{o}_0 := 0\n\\] Carry out an analyises like that in Equation 2.4 to show that \\(Q_n\\) is an exponential recency-weighted average without initial bias.\n\n\nSolution 2.7. My first question when I saw this was, “What’s up with that strange name \\(\\bar{o}_n\\)?”” I guess it could be something like “the weighted average of ones”, maybe? Well, whatever. Let’s crack on.\nWhen we rewrite \\(\\bar{o}_{n+1}\\) as a recurrence relation for \\(\\frac{\\bar{o}_{n+1}}{\\alpha}\\) \\[\n\\frac{\\bar{o}_{n+1}}{\\alpha} = 1 + (1 - \\alpha) \\frac{\\bar{o}_n}{\\alpha}\n\\] we see that it is just the recurrence relation for a geometric series as in Equation 2.13 for \\(\\gamma = 1-\\alpha\\). Thus we get \\[\n\\bar{o}_n = \\alpha\\sum_{i=0}^{n-1} (1 - \\alpha)^{i} = 1 - (1-\\alpha)^n\n\\] and \\(\\beta_n = \\frac{\\alpha}{1-(1-\\alpha)^n}\\)\n(btw. this is such a complicated way to define \\(\\beta_n\\) and I don’t understand why actually.)\nIn particular we have that \\(\\beta_1 = 1\\), which makes the influence of \\(Q_1\\) disappears after the first reward \\(R_1\\) is received: \\(Q_2 = Q_1 + 1 [ R_1 - Q_1] = R_1\\). Great!\nScaling the \\(\\alpha\\) by the \\(\\bar{o}_n\\) has an additional nicer effect. I don’t quite understand how, but we can calculate it.\nFrom Exercise 2.4 we know \\[\nQ_{n+1} = Q_1 \\prod_{j=1}^n (1-\\beta_j)\n+ \\sum_{i=1}^n  R_i \\beta_i \\prod_{j=i+1}^n (1- \\beta_j )\n\\]\nThere is a nice form for these products \\[\n\\prod_{j=i}^n (1 - \\beta_j) = (1-\\alpha)^{n-j+1} \\frac{\\bar{o}_{i-1}}{\\bar{o}_n}\n\\]\nsince they are telescoping using \\[\n\\begin{split}\n1- \\beta_j &= 1 - \\frac{\\alpha}{\\bar{o}_j} = \\frac{\\bar{o}_j - \\alpha}{\\bar{o}_j}\\\\\n&= \\frac{\\alpha + (1-\\alpha) \\bar{o}_{j-1}}{\\bar{o}_j} = (1-\\alpha)\\frac{\\bar{o}_{j-1}}{\\bar{o}_j}.\n\\end{split}\n\\]\nThis gives the following closed form for \\(Q_{n+1}\\) \\[\nQ_{n+1} = \\frac{\\alpha}{1 - (1-\\alpha)^n}\\sum_{i=1}^n R_i (1-\\alpha)^{n-i}\n\\]\nWe can see that the weight given to any reward \\(R_i\\)​ decreases exponentially.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#upper-confidence-bound-action-selection",
    "href": "chapters/02-multi-armed-bandits.html#upper-confidence-bound-action-selection",
    "title": "2  Multi-armed Bandits",
    "section": "2.7 Upper-Confidence-Bound Action Selection",
    "text": "2.7 Upper-Confidence-Bound Action Selection\nWe have the opportunity to introduce a new agent here. The update rule remains the same (I assume sample average), but the action selection is more informed compared to \\(\\varepsilon\\)-greedy algorithms.\n\\[\nA_t := \\mathrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{ \\frac{\\ln t}{N_t(a)} }\\right]\n\\]\nwhere \\(N_t(a)\\) is the number of times that action has been selected, c &gt; 0 controls the exploration (similar to \\(\\varepsilon\\)).\nIf an action has not been selected even once, i.e., \\(N_t(a)=0\\), then \\(a\\) is considered to be a maximizing action. (In our case, this means that in the first few steps, all actions have to be selected once, and only after that does the UCB-based action selection kick in.)\nBy the way, I have no idea where the UCB formulation comes from, but at least it looks fancy (and reasonable), and we can implement it:\n\n# === the ucb bandit agent ===\nclass UcbBanditAgent:\n    def __init__(self, num_actions, c, seed=None):\n        self.num_actions = num_actions\n        self.c = c  # exploration parameter\n        self.reset()\n        self.rng = np.random.default_rng(seed)\n\n    def reset(self):\n        self.t = 0\n        self.Q = np.zeros(self.num_actions, dtype=float)\n        self.counts = np.zeros(self.num_actions, dtype=int)\n\n    def act(self, bandit):\n        self.t += 1\n\n        # upper-Confidence-Bound Action Selection\n        if self.t &lt;= self.num_actions:\n            # if not all actions have been tried yet, select an untried action\n            action = self._choose_untaken_action()\n        else:\n            # calculate UCB values for each action\n            ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / (self.counts))\n            # select the action with the highest UCB value\n            action = np.argmax(ucb_values)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update count and value estimate\n        self.counts[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.counts[action]\n\n        return (action, reward)\n\n    def _choose_untaken_action(self):\n        return self.rng.choice(np.where(self.counts == 0)[0])\n\nLet’s recreate the figure illustrating UCB action selection performance, which we’ll need for the next exercise.\n\n\nCode\n# === ucb agent performance ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=1_000,\n)\nagent_ucb = UcbBanditAgent(num_actions=config.bandit_num_arms, c=2, seed=config.exp_seed)\nagent_ε_greedy = SampleAverageBanditAgent(\n    Q1=np.zeros(config.bandit_num_arms, dtype=float), ε=0.1, seed=config.exp_seed\n)\n\nagents = [agent_ucb, agent_ε_greedy]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [\n    \"ucb (c=2, α=0.1)\",\n    \"ε-greedy (ε=0.1, α=0.1)\",\n]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: This is like Figure 2.4 (Sutton and Barto 2018): average performance of UCB action selection.\n\n\n\n\n\n\nExercise 2.8 (UCB Spikes) In Figure 2.3 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if \\(c = 1\\), then the spike is less prominent.\n\n\nSolution 2.8. I think the answer is similar to the answer to Exercise 2.6. The first \\(10\\) steps the UCB algorithm tries out all actions. Then on step \\(11\\) it will select the one that scored highest, which is quite a decent strategy. But then because of the \\(N_t​(a)\\) in the denominator it is back to exploring.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#gradient-bandit-algorithms",
    "href": "chapters/02-multi-armed-bandits.html#gradient-bandit-algorithms",
    "title": "2  Multi-armed Bandits",
    "section": "2.8 Gradient Bandit Algorithms",
    "text": "2.8 Gradient Bandit Algorithms\nThis introduces a novel method that is not value-based; instead, it directly aims to select the best actions. The agent maintains numerical preferences, denoted by \\(H_t​(a)\\), rather than estimates of the action values.\nFor action selection, gradient bandit uses the softmax distribution: \\[\n\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}.\n\\tag{2.8}\\]\nShifting all preferences by a constant \\(C\\) doesn’t affect \\(\\pi_t(a)\\): \\[\n\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} = \\frac{e^Ce^{H_t(a)}}{\\sum_{b=1}^k e^Ce^{H_t(b)}} = \\frac{e^{H_t(a)+C}}{\\sum_{b=1}^k e^{H_t(b)+C}}\n\\]\nFor learning, gradient bandit uses the following rule: \\[\n\\begin{split}\nH_{t+1}(a) &:= H_t(a) + \\alpha (R_t - \\bar{R}_t) (\\mathbb{I}_{a = A_t} - \\pi_t(a))\n\\end{split}\n\\tag{2.9}\\]\nNow, let’s implement this gradient bandit algorithm:\n\n# === the gradient agent ===\nclass GradientBanditAgent:\n    def __init__(self, H1, α, baseline=True, seed=None):\n        self.num_actions = len(H1) \n        self.α = α \n        self.H1 = np.asarray(H1, dtype=np.float64)  # initial preferences\n        self.baseline = baseline # apply average reward baseline\n        self.reset()\n        self.rng = np.random.default_rng(seed)\n\n    def reset(self):\n        self.H = self.H1.copy()  \n        self.avg_reward = 0 \n        self.t = 0  # step count\n\n    def act(self, bandit):\n        self.t += 1\n\n        # select action using softmax\n        action_probs = GradientBanditAgent.softmax(self.H)\n        action = self.rng.choice(self.num_actions, p=action_probs)\n\n        # take action and observe the reward\n        reward = bandit.pull_arm(action)\n\n        # update average reward\n        if self.baseline:\n            self.avg_reward += (reward - self.avg_reward) / self.t\n\n        # update action preferences\n        advantage = reward - self.avg_reward # avg_reward = 0 if baseline = false\n        one_hot_action = np.eye(self.num_actions)[action]\n        self.H += self.α * advantage * (one_hot_action - action_probs)\n\n        return action, reward\n\n    @staticmethod\n    def softmax(x):\n        # shift vector by max(x) to avoid hughe numbers.\n        # This is basically using the fact that softmax(x) = softmax(x + C)\n        exp_x = np.exp(x - np.max(x))\n        return exp_x / np.sum(exp_x)\n\nSutton and Barto (Sutton and Barto 2018, 37) emphasize the importance of the baseline \\(\\bar{R}_t\\)​ in the update forumla and show that performance drops without it. In the derivation of the update as a form of stochastic gradient ascent, the baseline can be chosen arbitrarily (see Section 2.8.1). Whether or not a baseline is used, the resulting updates are unbiased estimators of the gradient. I assume, the baseline serves to reduce the variance of the estimator, although I have no idea about the maths behind it.\nHere we recreat Figure 2.5 (Sutton and Barto 2018), which shows how drastically the running average baseline can improve performance.\n\n\nCode\n# === gradient bandit performance ===\nconfig = Config(\n    exp_steps=1_000,\n    exp_runs=50,\n    bandit_setup_mu=4,\n)\n\nalphas = [0.1, 0.4]\nagents = [GradientBanditAgent(H1=np.zeros(config.bandit_num_arms), α=α, seed=config.exp_seed) for α in alphas] + [GradientBanditAgent(H1=np.zeros(config.bandit_num_arms), α=α, baseline=False, seed=config.exp_seed) for α in alphas]\n\navg_rwd, opt_pct = bandit_experiment(\n    agents,\n    config\n)\n\nlabels = [f\"α = {α}, with baseline\" for α in alphas] + [f\"α = {α}, without baseline\" for α in alphas]\nplot_optimal_action_percent(opt_pct, labels=labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: This is like Figure 2.5 (Sutton and Barto 2018): average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the \\(q_*(a)\\) are chosen to be near \\(+4\\) rather than near zero. (we averaged over 50 runs because it gives this cool jaggedy looking graph)\n\n\n\n\n\n\nExercise 2.9 Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.\n\n\nSolution 2.9. The logistic function is defined as \\[\n\\sigma(x) := \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}.\n\\] If we map the two preferences \\(H(a_1), H(a_2)\\) to a single value \\(\\Delta = H(a_1) - H(a_2)\\) then \\[\n\\pi(a_1) = \\frac{e^{H(a_1)}}{e^{H(a_1)} + e^{H(a_2)}} = \\frac{e^{H(a_1)-H(a_2)}}{e^{H(a_1)-H(a_2)} + 1} = \\sigma(\\Delta)\n\\] and similarly \\[\n\\pi(a_2) = \\sigma(-\\Delta).\n\\]\n\n\n2.8.1 the bandit gradient algorithm as stochastic gradient ascent\nThis next subsection is devoted to the (I imagine) infamous brown box (Sutton and Barto 2018, 38–40), which marks a significant leap in theoretical complexity. It shows how the update rule arises as a form of stochastic gradient ascent.\nWe’ll retrace their steps as a self-contained argument and flag two subtle points: the role of the baseline and the randomness in the preference vector.\n\n1. quick recap of gradient ascent\nLet \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\) be a differentiable function. We want to produce points \\(\\mathbf{x}_0, \\mathbf{x}_1, \\dots\\) that maximise \\(f\\). Gradient ascent updates the current point \\(\\mathbf{x}^{(t)}\\) in the direction of the gradient: \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\; \\nabla f(\\mathbf{x})\\big|_{\\mathbf{x}^{(t)}}\n\\] where \\(\\alpha &gt; 0\\) is the step size.\nIn one dimension, this becomes: \\[\nx_i^{(t+1)} = x_i^{(t)} + \\alpha \\; \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\bigg|_{x_i^{(t)}}\n\\]\nIn stochastic gradient ascent, we aim to maximise the expected value of a random vector \\(\\mathbf{R}\\) (a vector whose values are random variables), whose distribution depends on a parameter vector \\(\\mathbf{x}\\). That is, the underlying probability space \\((\\Omega, \\mathrm{Pr}_{\\mathbf{x}})\\) is parameterised by \\(\\mathbf{x}\\). Here \\(f\\) is \\(\\mathbb{E}_{\\mathbf{x}}[\\mathbf{R}]\\) where we have explicitly indicated the dependence of the expected value on \\(\\mathbf{x}\\). So this is still a deterministic gradient ascent step—although the true gradient is unknown to the algorithm and must later be estimated via sampling: \\[\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\cdot \\nabla \\mathbb{E}_{\\mathbf{x}}[\\mathbf{R}]\\big|_{\\mathbf{x}^{(t)}}\n\\]\nOur goal is to cast the gradient bandit update in this framework.\n\n\n2. setting up the problem\nIn the gradient bandit algorithm, the parameters we adjust are the action preferences \\(H_t​(a)\\). These determine the policy via the softmax distribution: \\[\n\\pi_H(a) = \\frac{e^{H(a)}}{\\sum_{b \\in \\mathcal{A}} e^{H(b)}}\n\\]\nThis shows how our parameter \\(H\\) determines the probability space: it determines the probability distribution for \\(A_t\\) \\(\\pi_{H_t}(a) := \\mathrm{Pr}_{H_t}(A_t = a)\\) the probabilities for rewards given an action are determined by the system and independent of the parameters \\(q_*(a) := \\mathbb{E}[R_t \\mid A_t = a]\\).\nWith this set-up, it is clear how \\(\\mathbb{E}_{H_t}[R_t]\\) is a function on \\(H_t\\) \\[\n\\begin{split}\n\\mathbb{E}_{H_t}[R_t] &= \\sum_{b} \\mathrm{Pr}_{H_t}(A_t = b) \\cdot \\mathbb{E}[R_t \\mid A_t = b] \\\\\n&= \\sum_{b} \\pi_{H_t}(b) q_*(b)\n\\end{split}\n\\] (if you are unsure about this, check Theorem 2.4).\nIn this context, each action \\(a\\in\\mathcal{A}\\) corresponds to a coordinate in our parameter vector \\(H\\), so the gradient update in one dimension \\(a \\in \\mathcal{A}\\) becomes: \\[\nH_{t+1}(a) = H_t(a) + \\alpha \\frac{\\partial \\mathbf{E}[R_t]}{\\partial H(a)}\\bigg|_{H_t(a)}\n\\tag{2.10}\\]\n\n\n3. calculating the gradient\nNow look at the row of the gradient for \\(a \\in \\mathcal{A}\\): \\[\n\\begin{split}\n\\frac{\\partial \\mathbb{E}_{H}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)} &=\n\\sum_{b} q_*(b) \\cdot \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)} \\\\\n&= \\sum_{b} (q_*(b) - B_t) \\cdot \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)}\n\\end{split}\n\\] We could add here any scalar (called the baseline) as \\(\\sum_b \\pi_H(b) = 1\\) and thus \\(\\sum_{b} \\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Big|_{H'(a)} = 0\\). Note that for this argument to work \\(B_t\\) cannot depend on \\(b\\).\nTo simplify that further we use the softmax derivative, (which is derived in Sutton and Barto): \\[\n\\frac{\\partial \\pi_{H}(b)}{\\partial H(a)}\\Bigg|_{H_t(a)}\n= \\pi_{H_t}(b) (\\mathbb{I}_{a = b} - \\pi_{H_t}(a))\n\\]\nSo we have \\[\n\\begin{split}\n\\frac{\\partial \\mathbb{E}_{H}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)} &=\n\\sum_{b} (q_*(b) - B_t) \\cdot  (\\mathbb{I}_{a = b} - \\pi_{H_t}(a)) \\pi_{H_t}(b) \\\\\n&= \\mathbb{E}_{H_t}[(q_*(A_t)- B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})] \\\\\n&= \\mathbb{E}_{H_t}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})].\n\\end{split}\n\\] We get the second equality by applying the law of the unconscious statistician in reverse (Theorem 2.1), and yes the expression inside the expectation is all just a deterministic function of \\(A_t\\). In the final equality, we substituted \\(R_t\\) for \\(q_*(A_t)\\) using that \\(q_*(A_t) = \\mathbb{E}_{H_t}[R_t]\\), and the law of iterated expectations justifies the substitution under the outer expectation.\nBefore going to the next step, you might want to check the plausibility of \\[\n\\frac{\\partial \\mathbb{E}[R_t]}{\\partial H(a)}\\Bigg|_{H_t(a)}\n= \\mathbb{E}_{H_t}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})].\n\\tag{2.11}\\] On the left we have basically a number, the value of the derivative at some point, and on the right we have an expected value with a parameter \\(B_t\\). So the parameter \\(B_t\\) must cancel out somehow. Which it does indeed, you can check this for yourself.\n\n\n4. one-step update and baseline trade-off\nBy plugging Equation 2.11 into Equation 2.10 we get the deterministic gradient ascent update: \\[\nH_{t+1}(a) = H_t(a) + \\alpha \\mathbb{E}[ (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})]\n\\]\nReplacing the expectation by the single sample \\(A_t, R_t\\) yields the stochastic gradient update: \\[\nH_{t+1}(a) = H_t(a) + \\alpha  (R_t - B_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t(a)})\n\\tag{2.12}\\]\nTo get Equation 2.9 we have to substitute \\(\\bar{R}_t\\) for \\(B_t\\). Which requires some discussion first.\nSutton and Barto make clear that \\(\\bar{R}_t\\) depend on \\(R_t\\):\n\n\\(\\bar{R}_t\\) is the average of all the rewards up through and including time \\(t\\) (Sutton and Barto 2018, 37).\n\nIf we use that in Equation 2.12 for \\(B_t\\) we are not using an unbiased estimator anymore. This is tightly coupled with the fact that when we introduced \\(B_t\\) we required it not to depend on \\(b\\) which does later play the role of \\(A_t\\), and \\(\\bar{R}_t\\) depends on \\(R_t\\) which depends on \\(A_t\\).\nMostl likely there is a good reason for using \\(\\bar{R}_t\\), but I don’t know the mathematical motivation. (As I said, maybe some variance reduction)\nHowever I’m saying the derivation of their update formula is wrong1 as they frame it as an unbiased estimator.\n\n\ncomment on the time parameter\nWe have keept the time parameter in the derivation to stick to the style of Sutton and Barto. We could have equally done the same derivation for \\(H'\\) (new) and \\(H\\) (old). However, conceptually keeping the time parameter is a bit shaky. Where does the \\(H_t\\) come from? If we think this through then \\(H_t\\) actually becomes part of the whole system (enviroment + agent) and thus is a random vector. And then it’s harder for me to think about how to analyse this to obtain the update formula. Once the update rule is fixed, we can then treat the entire system (agent and environment) as stochastic without any problems.\nIf correct or not Sutton-Barto derive this term for the gradient \\[\n\\mathbb{E}\\big[ (R_t - \\bar{R}_t) (\\mathbb{I}_{a = A_t} - \\pi_{H_t}(a)) \\big].\n\\] Which might look a bit fishy because maybe \\(\\mathbb{E}\\big[ R_t - \\bar{R}_t]\\) is always \\(0\\). But it is usually not, because the policy \\(\\pi\\) is not stationary, it get’s updated at every step and thus the policy’s evolution decouples the two expectations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#associative-search-contextual-bandits",
    "href": "chapters/02-multi-armed-bandits.html#associative-search-contextual-bandits",
    "title": "2  Multi-armed Bandits",
    "section": "2.9 Associative Search (Contextual Bandits)",
    "text": "2.9 Associative Search (Contextual Bandits)\n\nExercise 2.10 Suppose you face a \\(2\\)-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions \\(1\\) and \\(2\\) are respectively \\(0.1\\) and \\(0.2\\) with probability \\(0.5\\) (case A), and \\(0.9\\) and \\(0.8\\) with probability \\(0.5\\) (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?\n\n\nSolution 2.10. We are presented with two scenarios and the questions “What is the best strategy?” and “What is its expected reward?” for each scenario.\nIn the first scenario, we don’t know whether we are facing Case A or Case B at any given time step. The true values of the actions are as follows: \\[\n\\begin{split}\n\\mathbb{E}[R_t \\mid A_t = 1] &= \\mathrm{Pr}(\\text{Case A}) \\cdot 0.1 + \\mathrm{Pr}(\\text{Case B}) \\cdot 0.9\\\\\n&= 0.5 (0.1 + 0.9) = 0.5\\\\[3ex]\n\\mathbb{E}[R_t \\mid A_t = 2] &= \\mathrm{Pr}(\\text{Case A}) \\cdot 0.2 + \\mathrm{Pr}(\\text{Case B}) \\cdot 0.8\\\\\n&= 0.5 (0.2 + 0.8) = 0.5\n\\end{split}\n\\]\nSince both actions have the same expected reward of 0.5, it does not matter which action is chosen. Thus, any algorithm is optimal and has an expected of 0.5.\nIn the second scenario, we know whether we are facing Case A or Case B. The expected reward under the optimal strategy, which always chooses the action with the highest expected value, is: \\[\n\\mathbb{E}_{\\pi_*}[ R_t ] = \\overbrace{0.5 \\cdot 0.2}^{\\text{case A}} + \\overbrace{0.5 \\cdot 0.9}^{\\text{case B}} = 0.55\n\\]\nTo achieve this expected reward, we need to keep track of the two bandit problems separately and maximise their rewards. How to do this approximately is the topic of the whole chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#summary",
    "href": "chapters/02-multi-armed-bandits.html#summary",
    "title": "2  Multi-armed Bandits",
    "section": "2.10 Summary",
    "text": "2.10 Summary\n\nExercise 2.11 Make a figure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size \\(\\varepsilon\\)-greedy algorithm with \\(\\alpha\\)= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.\n\n\nSolution 2.11. That last exercise is a banger to finish with. There’s a lot going on here. I’ll explain what was difficult for me at the end, but let’s start with what I actually did and what we can see in the graph.\nI ran a parameter sweep for four different bandit agents over 200 episodes of 300,000 steps each. For each episode, I only looked at the average reward over the last 50,000 steps to measure steady-state performance.\n\n\nCode\n# === Parameter sweep for nonstationary bandit ===\n\"\"\"\nWe compare 4 bandit agents:\n0 - ε-greedy sample-average, parameter = ε\n1 - ε-greedy constant α = 0.1, parameter = ε\n2 - Gradient ascent with baseline, parameter = α\n3 - Gradient ascent no baseline, parameter = α\n\"\"\"\nfrom functools import partial\nfrom matplotlib.ticker import FuncFormatter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# custom import\nfrom scripts.parameter_study.episode_mean import episode_mean\n\n# --- Global config\nCONFIG = dict(\n    seed=1000000,\n    num_arms=10,\n    steps=300_000,\n    keep=50_000,\n    runs=200,\n    q_0 = 10,\n    drift_sd=0.1,\n)\n\nAGENTS = {\n    \"ε in ε-greedy (sample-avg)\": dict(\n        id=0,\n        x_name=\"ε\",\n        x_vals=[2**-k for k in (12, 9, 6, 3, 1, 0)],\n        fixed=dict(),\n    ),\n    \"ε in ε-greedy (α = 0.1)\": dict(\n        id=1,\n        x_name=\"ε\",\n        x_vals=[2**-k for k in (12, 9, 6, 3, 1, 0)],\n        fixed=dict(α=0.1),\n    ),\n    \"α in gradient ascent\": dict(\n        id=2,\n        x_name=\"α\",\n        x_vals=[2**-k for k in (20, 18, 14, 9, 6, 3, 1)],\n        fixed=dict(),\n    ),\n    \"α in gradient ascent (no base)\": dict(\n        id=3,\n        x_name=\"α\",\n        x_vals=[2**-k for k in (20, 18, 14, 9, 6, 3, 1)],\n        fixed=dict(),\n    ),\n}\n\n\n# --- Experiment helper\ndef evaluate(agent_type: int, **kwargs) -&gt; float:\n    \"\"\"Mean reward over the last *keep* steps averaged across *runs* runs.\"\"\"\n    args = {**CONFIG, **kwargs}\n    rng = np.random.default_rng(args[\"seed\"])\n    seeds = rng.integers(0, 2_000_000, size=args[\"runs\"])\n\n    rewards = [\n        episode_mean(\n            agent_type,\n            args[\"num_arms\"],\n            args[\"steps\"],\n            args[\"keep\"],\n            args[\"q_0\"],\n            1,  # bandit_action_sd\n            0,  # drift_mu\n            args[\"drift_sd\"],\n            seed,\n            kwargs.get(\"ε\", 0.1),\n            kwargs.get(\"α\", 0.1),\n        )\n        for seed in seeds\n    ]\n    return float(np.mean(rewards))\n\n\n# --- run the sweeps\nresults = {}\nfor label, spec in AGENTS.items():\n    run = partial(evaluate, spec[\"id\"], **spec[\"fixed\"])\n    results[label] = [run(**{spec[\"x_name\"]: x}) for x in spec[\"x_vals\"]]\n\n# --- plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\nmarkers = [\"^\", \"o\", \"s\", \"*\"]  # one per agent\nfor (label, spec), marker in zip(AGENTS.items(), markers):\n    ax.plot(\n        spec[\"x_vals\"],\n        results[label],\n        marker=marker,\n        label=label,\n    )\n\nax.set_xscale(\"log\", base=2)\nax.set_xlabel(\"Exploration / step-size (log scale)\")\nax.set_ylabel(\n    f\"Average reward for the last {CONFIG['keep']:,} steps ({CONFIG['steps']:,} steps total)\"\n)\nax.set_title(f\"Average reward (mean of {CONFIG['runs']} runs)\")\nax.grid(True, which=\"both\", linewidth=0.5)\n\nax.xaxis.set_major_formatter(\n    FuncFormatter(lambda x, _: f\"1/{int(1/x)}\" if x &lt; 1 else str(int(x)))\n)\n\nax.legend()\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\nParameter sweep for a non-stationary bandit, similar to Figure 2.6 (Sutton and Barto 2018). The plot displays the average reward of the last 50,000 steps of a 300,000-step episode, averaged over 200 runs. The bandit was initialized with action-value means set to 10, which drifted according to a normal distribution with a standard deviation of 0.1 at each step. The standard deviation of the reward noise was 1.\n\n\n\n\nThe agents I tested were:\n\n\\(\\varepsilon\\)-greedy with sample-average action values, sweeping over \\(\\varepsilon\\)\n\\(\\varepsilon\\)-greedy with constant step-size (α = 0.1), also sweeping \\(\\varepsilon\\)\ngradient ascent with a sample-average baseline, sweeping \\(\\alpha\\)\ngradient ascent with no baseline, again sweeping \\(\\alpha\\)\n\nTo encourage adaptability, I changed the drift to 0.1.\nLet’s go through some observations:\n\nconstant step-size ε-greedy outperforms the others. Not that surprising.\ngradient ascent underperforms. That surprised me. I’d assumed the gradient method would be a good fit for a drifting environment. However, it does worse than the sample-average ε-greedy.\nsmall α performs best for gradient ascent. This one I don’t have a great explanation for, as very small step sizes typically result in slow learning.\nalso, the baseline doesn’t matter for verly low α. Again no clue.\nsample-average ε-greedy agent is better than I expected. I thought that, with that many steps, it wouldn’t be able to keep up with the changing environment.\nrandom Behaviour for ε = 1. The ε-greedy agents with ε = 1 behaves randomly, as expected. Both agents achieve a reward of 10, which is consistent with random action selection.\n\nSo, this figure raises quite a few questions I can’t yet answer. I think that makes this execrise really good. And maybe in the future I will be able to provide more insights about these findings.\nThis was a lot of work. Running 200 episodes at 300,000 steps each is way too slow in pure Python. I had to offload the inner loop into a separate file and used Numba to JIT-compile it, basically rewriting all the algorithms used in this experiment. That’s also why I don’t want to spend more time trying out gradient ascent with a constant-step baseline. Additionally, I can’t guarantee that there isn’t some subtle bug in the code for the main loop, as I didn’t really test all the algorithms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#sec-appendix-multi-arm-bandits",
    "href": "chapters/02-multi-armed-bandits.html#sec-appendix-multi-arm-bandits",
    "title": "2  Multi-armed Bandits",
    "section": "2.11 appendix",
    "text": "2.11 appendix\nHere are some more details on concepts that came up during this chapter.\n\n2.11.1 distribution\nEvery random variable \\(X\\) has a distribution, denoted \\(p_X\\), which maps each possible value to its probability: \\[\np_X(x) := \\mathrm{Pr}(X = x).\n\\]\nOften, we say \\(X\\) is distributed according to \\(f\\) for a function \\(f \\colon \\mathcal{X} \\to [0,1]\\), which means that \\(f(x) = \\mathrm{Pr}(X = x)\\). We write this as: \\[\nX \\sim f.\n\\]\nTwo random variables \\(X\\) and \\(Y\\) have the same distribution if \\(p_X = p_Y\\).\nThe distribution of \\(X\\) turns \\((\\mathcal{X}, p_X)\\) into a probability space, where \\(p_X\\) is called the pushforward measure.\n\n\n2.11.2 independent and identically distributed random variables\nA very important concept: IID. A collection of random variables \\(X_1, \\dots, X_n\\) is independent and identically distributed (IID) if all random variables have the same probability distribution, and all are mutually independent.\nFormally, this means:\n\n\\(\\mathrm{Pr}(X_i = x) = \\mathrm{Pr}(X_j = x)\\)\n\\(\\mathrm{Pr}(X_i = x,\\, X_j = x') = \\mathrm{Pr}(X_i = x) \\cdot \\mathrm{Pr}(X_j = x')\\)\n\nfor all distinct indices \\(i \\neq j\\).\n\n\n2.11.3 lotus\nThe following theorem is widely known as the “law of the unconscious statistician”. It is fundamental in many calculations as it allows us to compute the expected value of functions of random variables by only knowing the distributions of the random variables.\n\nTheorem 2.1 Let \\(X \\colon \\Omega \\to \\mathcal{X}\\) be a random variable, and let \\(g \\colon \\mathcal{X} \\to \\mathbb{R}\\) be a real-valued function on the result space.\nThen the expected value of \\(g\\) with respect to the pushforward distribution \\(p_X\\) is the same as the expected value of the random variable \\(g(X) := g \\circ X\\) on \\(\\Omega\\): \\[\n\\mathbb{E}[g(X)] = \\sum_{x \\in \\mathcal{X}} g(x)\\, p_X(x)\n\\]\n\n\nProof. Pretty sure this proof could be beautifully visualised: summing over columns is the same as summing over rows. But indicator functions \\(\\mathbb{I}\\) do the trick too.\n\\[\n\\begin{split}\n\\sum_{x \\in \\mathcal{X}} g(x) p_X(x)\n&= \\sum_{x \\in \\mathcal{X}} g(x) \\mathrm{Pr}(X = x) \\\\\n&= \\sum_{x \\in \\mathcal{X}} g(x) \\left( \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) \\mathbb{I}_{X = x} \\right) \\\\\n&= \\sum_{x \\in \\mathcal{X}, \\omega \\in \\Omega} g(x) \\mathrm{Pr}(\\omega) \\mathbb{I}_{X = x} \\\\\n&= \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) \\left( \\sum_{x \\in \\mathcal{X}} g(x) \\mathbb{I}_{X = x} \\right) \\\\\n&= \\sum_{\\omega \\in \\Omega} \\mathrm{Pr}(\\omega) g(X) \\\\\n&= \\mathbb{E}[g(X)]\n\\end{split}\n\\]\n\n\n\n2.11.4 multiplication rule of conditional probabilities\nThe multiplication rule of conditional probabilities is great for manipulating unknown distributions into known distributions.\n\nTheorem 2.2 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R}\\) be random variables. Then \\[\n\\mathrm{Pr}[R = r, A = a] =  \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\] for \\(a \\in \\mathcal{A}\\) and \\(r \\in \\mathcal{R}\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}[R = r, A = a]\n&= \\mathrm{Pr}(A = a) \\frac{\\mathrm{Pr}[R = r, A = a]}{\\mathrm{Pr}(A = a)} \\\\\n&= \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\end{split}\n\\]\n\n\nTheorem 2.3 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R}\\) be random variables. Then \\[\n\\mathrm{Pr}[R =r] = \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\] for \\(r \\in \\mathcal{R}\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}[R =r]\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}[R = r, A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a]\n\\end{split}\n\\]\n\n\nTheorem 2.4 Let \\(A \\colon \\Omega \\to \\mathcal{A}\\) and \\(R \\colon \\Omega \\to \\mathcal{R} \\subseteq \\mathbb{R}\\) be random variables. Then \\[\n\\mathbb{E}[R] = \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathbb{E}[R \\mid A = a]\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathbb{E}[R]\n&= \\sum_{r \\in \\mathcal{R}} r \\mathrm{Pr}(R = r) \\\\\n&= \\sum_{r \\in \\mathcal{R}} r \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathrm{Pr}[R = r \\mid A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\sum_{r \\in \\mathcal{R}} r \\mathrm{Pr}[R = r \\mid A = a] \\\\\n&= \\sum_{a \\in \\mathcal{A}} \\mathrm{Pr}(A = a) \\mathbb{E}[R \\mid A = a]\n\\end{split}\n\\]\n\n\n\n2.11.5 variance\nThe variance \\(\\mathrm{Var}(X)\\) of a random variable is defined as: \\[\n\\mathrm{Var}(X) := \\mathbb{E}[(X-\\mu)^2]\n\\]\nwhere \\(\\mu = \\mathbb{E}[X]\\) is the mean of \\(X\\).\nIt can be easily shown that \\[\n\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2.\n\\]\n\n\n2.11.6 independent variables\nTwo random variables \\(X,Y\\) are independent if \\[\n\\mathrm{Pr}(X = x, Y = y) = \\mathrm{Pr}(X = x) \\cdot \\mathrm{Pr}(Y = y).\n\\]\nIn this case, the conditioned probabilities are equal to the ordinary probabilities\n\nLemma 2.1 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathrm{Pr}(X = x \\mid Y = y) = \\mathrm{Pr}(X = x).\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Pr}(X = x \\mid Y = y) &= \\frac{\\mathrm{Pr}(X = x, Y = y)}{\\mathrm{Pr}(Y = y)} \\\\\n&= \\frac{\\mathrm{Pr}(X = x) \\mathrm{Pr}(Y = y)}{\\mathrm{Pr}(Y = y)} \\\\\n&= \\mathrm{Pr}(X = x)\n\\end{split}\n\\]\n\n\nLemma 2.2 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathbb{E}(XY) = \\mathbb{E}(X) \\cdot \\mathbb{E}(Y)\n\\]\n\n\nProof. We are cheating a bit here (but not doing anything wrong) and apply LOTUS on two random variables at once. \\[\n\\begin{split}\n\\mathbb{E}[XY] &= \\sum_{x,y} x\\cdot y \\; \\mathrm{Pr}(X = x, Y = y) \\\\\n&= \\left(\\sum_{x} x \\mathrm{Pr}(X = x)\\right) \\cdot \\left(\\sum_{y} y \\mathrm{Pr}(Y = y)\\right) \\\\\n&= \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]\n\\end{split}\n\\]\n\nWe can use this lemma to prove “linearity” for independent variables.\n\nLemma 2.3 If \\(X\\) and \\(Y\\) are independent random variables, then \\[\n\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n\\]\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Var}(X + Y) &= \\mathbb{E}[(X+Y)^2] - \\mathbb{E}[X+Y]^2 \\\\\n&= (\\mathbb{E}[X^2] + 2 \\mathbb{E}[XY] + \\mathbb{E}[Y^2]) - (\\mathbb{E}[X]^2 + 2 \\mathbb{E}[X]\\mathbb{E}[Y] + \\mathbb{E}[Y]^2) \\\\\n&= (\\mathbb{E}[X^2] - \\mathbb{E}[X]^2) + (\\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2) \\\\\n&= \\mathrm{Var}(X) + \\mathrm{Var}(Y)\n\\end{split}\n\\]\n\n\nTheorem 2.5 The population mean \\(\\bar{X}_n\\) of IID real-valued random variables \\(X_1, \\dots, X_n\\) has variance \\[\n\\mathrm{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of the \\(X_i\\).\n\n\nProof. \\[\n\\begin{split}\n\\mathrm{Var}(\\bar{X}_n) &= \\mathrm{Var}(\\frac{1}{n}\\sum_{i=1}^n X_i) \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n \\mathrm{Var}(X_i)\\\\\n&= \\frac{1}{n^2} n \\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\n\n\n\n2.11.7 estimators\nEstimators are functions used to infer the value of a hidden parameter from observed data.\nI don’t want to create too much theory for estimators. Let’s look at the \\(Q_n\\) and \\(R_i\\) from Section 2.4.\nThe \\(Q_n\\) are somehow based on the \\(R_1, \\dots, R_{n-1}\\) and called estimators for \\(q_*\\).\nThere are some common metrics for determining the quality of an estimator.\n\nbias\nThe bias of \\(Q_n\\) is \\[\n\\mathrm{Bias}(Q_n) = \\mathbb{E}[Q_n] - q_*.\n\\]\nIf this is \\(0\\) then \\(Q_n\\) is unbiased. If the bias disappears asymptotically, then \\(Q_n\\)​ is asymptotically unbiased.\n\n\nmean squared error\nIt is used to indicate how far, on average, the collection of estimates are from the single parameter being estimated. \\[\n\\mathrm{MSE}(Q_n) = \\mathbb{E}[ (Q_n - q_*)^2]\n\\]\nThe mean squared error can be expressed in terms of bias and variance. \\[\n\\mathrm{MSE}(Q_n) = \\mathrm{Var}(Q_n) + \\mathrm{Bias}(Q_n)^2\n\\]\nIn particular, for unbiased estimators, the mean squared error is just the variance.\n\nLemma 2.4 Let \\(Q_n\\) be the sample average of \\(R_1, \\dots, R_n\\). Then \\[\n\\mathrm{MSE}(Q_n) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of the \\(X_i\\).\n\n\nProof. The sample average is unbiased. Thus, its mean squared error is its variance given in Theorem 2.5.\n\n\nLemma 2.5 Let \\(Q_n\\) be the constant step size weighted average of the \\(R_1, \\dots, R_n\\). Then, the Mean Squared Error of \\(Q_{n+1}\\) is given by: \\[\n\\mathrm{MSE}(Q_{n+1}) = \\sigma^2 \\frac{\\alpha}{2-\\alpha} + (1 - \\alpha)^{2n} [(Q_n - q_*)^2 - \\sigma^2\\frac{\\alpha}{2-\\alpha}],\n\\] where \\(\\sigma^2\\) is the variance of the \\(R_i\\).\nIn particular, the MSE is bounded from below by \\[\n\\lim_{n\\to\\infty}\\mathrm{MSE}(Q_n) = \\sigma^2 \\frac{\\alpha}{2-\\alpha}.\n\\]\n\n\nProof. The weighted average \\(Q_{n+1}\\)​ is defined as: \\[\nQ_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i\n\\]\n\nstep 1: compute the expected value\nThis has been done in Equation 2.5 \\[\n\\mathbb{E}(Q_{n+1}) = (1-\\alpha)Q_1 + (1 - (1-\\alpha)^n)q_*\n\\]\n\n\nstep 2: compute the bias of\nUsing \\(\\mathrm{Bias}(Q_{n+1}) = \\mathbb{E}[Q_{n+1}] - q_*\\) we get \\[\n\\begin{split}\n\\mathrm{Bias}(Q_{n+1}) &= (1-\\alpha)Q_1 + (1 - (1-\\alpha)^n)q_* - q_* \\\\\n&= (1-\\alpha)^n [Q_n - q_*].\n\\end{split}\n\\]\n\n\nstep 3: compute the variance\n\\[\n\\begin{split}\n\\mathrm{Var}(Q_{n+1}) &= \\mathrm{Var}\\big((1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i \\big) \\\\\n&= \\sum_{i=1}^n \\alpha^2 \\big((1-\\alpha)^{2}\\big)^{n-i} \\sigma^2 \\\\\n&= \\sigma^2\\alpha^2 \\sum_{i=0}^{n-1} \\big((1-\\alpha)^{2}\\big)^{i} \\\\\n&= \\sigma^2\\alpha^2 \\frac{1 - (1-\\alpha)^{2n}}{1 - (1-\\alpha)^2} \\\\\n&= \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\big(1 - (1-\\alpha)^{2n}\\big).\n\\end{split}\n\\] Here it’s crucial that the \\(R_i\\) are independent.\n\n\n2.11.7.0.1 step 4: compute the mean squared error\nNow we can use \\(\\mathrm{MSE}(Q_{n+1}) = \\mathrm{Var}(Q_{n+1}) + \\mathrm{Bias}(Q_{n+1})^2\\) \\[\n\\begin{split}\n\\mathrm{MSE}(Q_{n+1}) &= \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\big(1 - (1-\\alpha)^{2n}\\big) + \\Big( (1-\\alpha)^n [Q_n - q_*] \\Big)^2 \\\\\n&= \\sigma^2 \\frac{\\alpha}{2-\\alpha} + (1 - \\alpha)^{2n} [(Q_n - q_*)^2 - \\sigma^2\\frac{\\alpha}{2-\\alpha}]\n\\end{split}\n\\]\n\n\n\n\n\n2.11.8 geometric series\nIn the context of reinforcement learning, the concept of discounting naturally requires the notion of geometric series. This series is defined as, \\[\nS(n+1) := \\sum_{i=0}^n \\gamma^i,\n\\]\nwhere \\(\\gamma \\in \\mathbb{R}\\). By convention, an empty sum is considered to be 0, thus \\(S(0)=0\\).\nIf \\(\\gamma = 1\\), then the geometric series simplifies to \\(S(n+1) = n+1\\). So let’s assume \\(\\gamma \\neq 1\\) from now on.\nBy pulling out the term for \\(i=0\\) and factoring out a \\(\\gamma\\), we can derive a recurrence relation for the geometric series \\[\nS(n+1) = 1 + \\gamma S(n) \\quad \\text{and} \\quad S(0) = 0\n\\tag{2.13}\\]\nWhen we even add a clever \\(0 = \\gamma^{n+1} - \\gamma^{n+1}\\), we get this equation for \\(S(n)\\) \\[\nS(n) = (1 - \\gamma^{n+1}) + \\gamma S(n).\n\\]\nFrom this, we can deduce the closed-form expression for the geometric series: \\[\n\\sum_{i=0}^n \\gamma^i  = \\frac{1 - \\gamma^{n+1}}{1 - \\gamma}\n\\tag{2.14}\\]\nBy omitting the first term (starting from \\(i = 1\\)), we obtain: \\[\n\\sum_{i=1}^n \\gamma^i =  \\frac{\\gamma - \\gamma^{n+1}}{1 - \\gamma}\n\\tag{2.15}\\]\nThe infinite geometric series converges, if and only if, \\(|\\gamma| &lt; 1\\). Using the previous formulas, we can derive their limits: \\[\n\\sum_{i=0}^\\infty \\gamma^i = \\frac{1}{1-\\gamma}\n\\tag{2.16}\\] \\[\n\\sum_{i=1}^\\infty \\gamma^i = \\frac{\\gamma}{1-\\gamma}\n\\tag{2.17}\\]\n\n\n2.11.9 recurrence relations\nAs it turns out, the basic geometric series we’ve explored isn’t quite enough to handle discounting and cumulative discounted returns in reinforcement learning. While the geometric series solves the homogeneous linear recurrence relation given by Equation 2.13, dealing with cumulative discounted returns introduces a non-homogeneous variation, where the constants 1s are replaced by a some \\(r_i\\)​, leading to the recurrence relation: \\[\nQ(n+1) := r_n + \\gamma Q(n) \\quad \\text{and} \\quad Q(0) := 0\n\\tag{2.18}\\]\nWe can also give an explicit formula for \\(Q(n+1)\\): \\[\nQ(n+1) = \\sum_{i=0}^n \\gamma^{n-i} r_i.\n\\tag{2.19}\\]\nIt’s easy to verify that this fulfils the recursive definition: \\[\n\\begin{split}\nQ(n+1) &= r_n + \\sum_{i=0}^{n-1} \\gamma^{n-i} r_i \\\\\n&= r_n + \\gamma\\sum_{i=0}^{n-1} \\gamma^{n-1-i} r_{i} \\\\\n&= r_n + \\gamma Q(n).\n\\end{split}\n\\]\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/02-multi-armed-bandits.html#footnotes",
    "href": "chapters/02-multi-armed-bandits.html#footnotes",
    "title": "2  Multi-armed Bandits",
    "section": "",
    "text": "I’m saying this not very loudly. Maybe I’m somewhere wrong.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html",
    "href": "chapters/03-finite-markov-decision-processes.html",
    "title": "3  Finite Markov Decision Processes",
    "section": "",
    "text": "3.1 The Agent–Environment Interface\nWe are already anticipating Exercise 3.5 and will give the formulations for a Markov Decision Process (MDP) for continuing and episodic tasks.\nA continuing trajectory looks like this: \\[\nS_0, A_0, R_1, S_1, A_1, R_2, S_2 A_2, R_3, \\dots,\n\\] and an episodic trajectory looks like this: \\[\nS_0, A_0, R_1, S_1, A_1, \\dots R_{T-1}, S_{T-1}, A_{T-1}, R_T, S_T.\n\\] Note that the sequencing of actions and rewards has changed from the previous chapter. Now, the reward for an action \\(A_t\\) is \\(R_{t+1}\\), not \\(R_t\\) as before.\nAn MDP is completely described by its dynamics: \\[\np(s', r |s,a) := \\mathrm{Pr}(S_t = s', R_t = r \\mid S_{t-1} = s, A_{t-1} = a)\n\\tag{3.1}\\] giving the probability that, from state \\(s \\in \\mathcal{S}\\) under action \\(a \\in \\mathcal{A}(s)\\), the environment transitions to state \\(s' \\in \\mathcal{S}^+\\), where \\(\\mathcal{S}^+\\) denotes the state space with any possible terminal states, and gives reward \\(r \\in \\mathcal{R}\\).\nIn particular when \\(s\\) and \\(a\\) are fixed \\(p(s', r | s,a)\\) is a discrete probability density, i.e., \\[\np(\\cdot, \\cdot | s,a)\\colon \\mathcal{S}^+ \\times \\mathcal{R} \\to [0,1]\n\\] and \\[\n\\sum_{s' \\in \\mathcal{S}^+, r \\in \\mathcal{R}} p(s',r | s,a) = 1.\n\\tag{3.2}\\]\nI want to add some more words about MDP and other Markov Chains that will be important for us.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#sec-the-agent-environment-interface",
    "href": "chapters/03-finite-markov-decision-processes.html#sec-the-agent-environment-interface",
    "title": "3  Finite Markov Decision Processes",
    "section": "",
    "text": "3.1.1 markov chains\nVery generally, Markov chains are processes modelled with sequences of random variables \\(X_1, X_2, \\dots\\), where the conditional probabilities have a finite horizon. We will describe Markov Chains with increasing complexity until we end up at MDPs.\n\nmarkov process (MP)\nMPs model systems that evolve randomly over discrete time steps. They are a sequence of random variables \\(S_0, S_1, \\dots\\), where \\(S_t\\) is the state of the system at time \\(t\\). In the past the system was in the states \\(S_0, \\dots, S_{t-1}\\) and its future is \\(S_{t+1}\\).\nThe defining property of a Markov chain is that the future is independent of the past given the present state of the process. This is expressed as: \\[\n\\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s, (S_{t'} = s_{t'})_{t' &lt; t}) = \\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s)\n\\]\nUsually we require the environment to be stationary, i.e., the transition probabilities are independent of \\(t\\): \\[\n\\mathrm{Pr}(S_{t+1} = s' \\mid S_t = s) = \\mathrm{Pr}(S_{t'+1} = s' \\mid S_t' = s)\n\\]\nSo, in our case a Markov Process1 is completely described by\n\nstate space \\(\\mathcal{S}\\) and\ntransition probabilities: \\(p(s' | s) := P(S_{t+1}=s′∣ S_t=s)\\).\n\n\n\nmarkov reward process (MRP)\nA Markov Reward Process adds a reward structure to a Markov Process. What are we rewarded for? Simply for observing the process diligently and keeping our feet still as there is no interaction with the environment yet.\nHere, we have a sequence of random variables \\(R_0, S_0, R_1, S_1, R_2, \\dots\\). Basically it’s a sequence of random vectors \\((R_i, S_i)\\), where \\(S_i\\) tracks the state and the \\(R_i\\) give us some numerical information about the system. (Sutton and Barto usually omit the 0-th reward, which occurs before anything really happens—essentially a reward for starting the environment. It doesn’t change much, of course, but I like the symmetry it brings.)\nA Markov reward process (MRP) is therefore specified by:\n\nfinite state space \\(\\mathcal{S}\\)\nfinite reward space \\(\\mathcal{R} \\subseteq \\mathbb{R}\\)\n\\(p(s', r | s) := \\mathrm{Pr}(S_{t+1}=s', R_{t+1} = r∣ S_t = s)\\).\n\nHere \\(p(\\cdot, \\cdot | s)\\) is a probability measure on the product space \\(\\mathcal{S} \\times \\mathcal{R}\\), in particular \\(\\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} p(s',r|s) = 1\\)\n\n\nmarkov decision process (MDP)\nNow we add interaction to the environment.\nThe trajectory looks like this: \\[\nR_0, S_0, A_0, R_1, S_1, A_1, \\dots ,\n\\] where \\(R_i\\) take values in the reward space \\(\\mathcal{R}\\), \\(S_i\\) values in the state space \\(\\mathcal{S}\\), and \\(A_i\\) in the action space \\(\\mathcal{A}\\).\nThe full dynamic of this process is an interwoven interaction between environment and agent. It looks a bit like this: \\[\n(R_0, S_0) \\overset{\\text{agent}}{\\to} A_0  \\overset{\\text{env}}{\\to}(R_1, S_1) \\overset{\\text{agent}}{\\to}  A_1 \\overset{\\text{env}}{\\to} \\dots\n\\] So, going from \\(S_t, A_t\\) to the next state-reward pair is given by the environment \\[\np(s', r | s, a) := \\mathrm{Pr}(S_{t+1}=s', R_{t+1} = r∣ S_t = s, A_t = a).\n\\] and going from a state reward pair to an action is is given by the agent \\[\n\\pi_t(a|s) = \\mathrm{Pr}(A_t = a | S_t = s).\n\\] If the agent is stationary, we can drop the \\(t\\). \\[\n\\pi(a|s) = \\mathrm{Pr}(A_t = a | S_t = s)\n\\]\n\nExercise 3.1 Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n\n\nSolution 3.1. TBD\n\n\nExercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n\n\nSolution 3.2. No, I can’t think of any clear exceptions. There’s only the challenge of how to model MDP for goals that we don’t know how to specify properly in the reward signal, e.g., human happiness. I can’t come up with a reward signal that wouldn’t be vulnerable to reward hacking, like “number pressed by user on screen”, “time smiling”, “endorphins level in brain”.\n\n\nExercise 3.3 Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n\n\nSolution 3.3. TBD\n\n\nExercise 3.4 Give a table analogous to that in Example 3.3, but for \\(p(s' , r |s, a)\\). It should have columns for \\(s, a, s' , r\\) and \\(p(s' , r |s, a)\\), and a row for every 4-tuple for which \\(p(s', r |s, a) &gt; 0\\).\n\n\nSolution 3.4. \n\n\n\ns\na\ns’\nr\np(s’,r | s,a)\n\n\n\n\nhigh\nwait\nhigh\nr_wait\n1\n\n\nhigh\nsearch\nhigh\nr_search\nα\n\n\nhigh\nsearch\nlow\nr_search\n1 - α\n\n\nlow\nwait\nlow\nr_wait\n1\n\n\nlow\nsearch\nlow\nr_search\nβ\n\n\nlow\nsearch\nhigh\n-3\n1 - β\n\n\nlow\nrecharge\nhigh\n0\n1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#goals-and-rewards",
    "href": "chapters/03-finite-markov-decision-processes.html#goals-and-rewards",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.2 Goals and Rewards",
    "text": "3.2 Goals and Rewards",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#returns-and-episodes",
    "href": "chapters/03-finite-markov-decision-processes.html#returns-and-episodes",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.3 Returns and Episodes",
    "text": "3.3 Returns and Episodes\nThe expected (discounted) return is defined as: \\[\nG_t := \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}.\n\\] For episodic tasks we have the convention that \\(R_{t} = 0\\) when \\(t &gt; T\\), and thus, in particular, \\(G_T = 0\\).\nIn an undiscounted, episodic task, this becomes \\[\nG_t := \\sum_{k=0}^{T-t-1}  R_{t+k+1}\n\\tag{3.3}\\]\nAnother important recursive identity is \\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\tag{3.4}\\]\n\nExercise 3.5 The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of Equation 3.2\n\n\nSolution 3.5. We already described Section 3.1 for continuing and episodic tasks. So, Equation 3.2 is already in the right form.\n\n\nExercise 3.6 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n\n\nSolution 3.6. The reward at time \\(t\\) would be \\[\nG_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t + k+1} =  -\\gamma^{T - t - 1},\n\\] where \\(T\\) is the length of that episode.\nIn the continuing formulation there can be multiple failures in the future so the return is of the form \\(-\\gamma^{K_1} - \\gamma^{K_2} - \\dots\\). Here there can always just be one failure.\n\n\nExercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward Equation 3.3. After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n\n\nSolution 3.7. In this setup the reward basically says: “Finish the maze eventually.”. So when the robot has learned to finish a maze somehow, it can’t perform better regarding this reward.\n\n\nExercise 3.8 Suppose \\(\\gamma = 0.5\\) and the following sequence of rewards is received \\(R_1 = 1, R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2\\), with \\(T = 5\\). What are \\(G_0, G_1 \\dots, G_5\\)? Hint: Work backwards.\n\n\nSolution 3.8. We can use the recursive formula (Equation 3.4) for the reward: \\(G_t = R_{t+1} + \\gamma G_{t+1}\\)\n\n\n\nt\n\\(R_{t+1}\\)\n\\(\\gamma G_{t+1}\\)\n\\(G_t\\)\n\n\n\n\n5\n0\n0\n0\n\n\n4\n2\n0\n2\n\n\n3\n3\n1\n4\n\n\n2\n6\n2\n8\n\n\n1\n2\n8\n10\n\n\n0\n1\n5\n6\n\n\n\nWe recall that, by convention, \\(R_t := 0\\) for \\(t &gt; T\\)\n\n\nExercise 3.9 Suppose \\(\\gamma = 0.9\\) and the reward sequence is \\(R_1 = 2\\) followed by an infinite sequence of 7s. What are \\(G_1\\) and \\(G_0\\)?\n\n\nSolution 3.9. \\[\nG_1 = \\sum_{k=0}^\\infty 0.9^k R_{2+k} = 7 \\sum_{k=0}^\\infty 0.9^k = 7 / 0.1 = 70\n\\] and \\[\nG_0 = R_1 + \\gamma G_1 =  2 + 0.9 \\cdot G_1 = 2 + 0.9 \\cdot 70 = 65\n\\]\n\n\nExercise 3.10 Prove the second equality in (3.10).\n\n\nSolution 3.10. See Section 2.11.8 for a proof.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#unified-notation-for-episodic-and-continuing-tasks",
    "href": "chapters/03-finite-markov-decision-processes.html#unified-notation-for-episodic-and-continuing-tasks",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.4 Unified Notation for Episodic and Continuing Tasks",
    "text": "3.4 Unified Notation for Episodic and Continuing Tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#policies-and-value-functions",
    "href": "chapters/03-finite-markov-decision-processes.html#policies-and-value-functions",
    "title": "3  Finite Markov Decision Processes",
    "section": "3.5 Policies and Value Functions",
    "text": "3.5 Policies and Value Functions\nTo generate trajectories from an MDP, we must specify the agent’s policy. We indicate this notationally by adding a subscript \\(\\pi\\) to probabilities, so that \\(\\mathrm{Pr}_\\pi(A_t = a \\mid S_t = s) = \\pi(a \\mid s)\\).\n\nExercise 3.11 If the current state is \\(S_t\\), and actions are selected according to stochastic policy \\(\\pi\\), then what is the expectation of \\(R_{t+1}\\) in terms of \\(\\pi\\) and the four-argument function \\(p\\) (Equation 3.1)?\n\n\nSolution 3.11. It’s clearer to rephrase the exercise as “given that the current state \\(S_t\\) is \\(s\\)”, so we proceed with that.\nWe solve this in two ways. First the intuitive way and second with our theory machine.\nIntuitively when \\(S_t = s\\) then we know that \\(A_t\\) is distributed according to \\(\\pi(\\cdot | s)\\) and then from \\(S_t\\) and \\(A_t\\) we can get the next \\(S_{t+1}, R_{t+1}\\) via the MDP dynamics measure. So let’s put this together. The agent selects \\(a\\) with probability \\(\\pi(a \\mid s)\\), and then the environment transitions to \\((s', r)\\) with probability \\(p(s', r | s,a)\\). We don’t care about the \\(s'\\) right now. So we get reward \\(r\\) with probability \\(\\sum_{s'} p(s',r | s,a)\\). Thus we have \\[\n\\mathbb{E}_{\\pi}[R_{t+1} \\mid S_t = s]\n= \\sum_{a} \\pi(a|s) \\sum_{r} r \\left(\\sum_{s'} p(s',r | s,a)\\right)\n\\] Or in a nicer format \\[\n\\mathbb{E}_{\\pi}[R_{t+1} \\mid S_t = s]\n= \\sum_{a} \\pi(a|s) \\sum_{r,s'} r \\; p(s',r | s,a)\n\\]\nNow let us derive this using LOTUS (Theorem 2.1) the law of total expectation (Theorem 2.3) \\[\n\\begin{split}\n\\mathbb{E}_{\\pi}&[R_{t+1} \\mid S_t = s] =\n\\sum_{r} r \\; \\mathrm{Pr}_{\\pi}[R_{t+1} = r \\mid S_t = s] \\\\\n&= \\sum_r r \\sum_{a, s'} \\mathrm{Pr}_{\\pi}[R_{t+1} = r, S_{t+1} = s' \\mid A_t = a, S_t = s] \\mathrm{Pr}_{\\pi}[A_t = a \\mid S_t = s] \\\\\n&=  \\sum_{r,a,s'} p(s',r | a,s) \\pi(a|s)\n\\end{split}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "chapters/03-finite-markov-decision-processes.html#footnotes",
    "href": "chapters/03-finite-markov-decision-processes.html#footnotes",
    "title": "3  Finite Markov Decision Processes",
    "section": "",
    "text": "“Our” Markov processes could be called more precisely stationary discrete-time Markov process.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes</span>"
    ]
  }
]